{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efff6d01-f895-417d-8bed-8442f28ee9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec5e32ba-aa0b-42b4-b51c-2f46fad02513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f0a012c-2132-486c-88ee-49a038e783a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42d93e50-f93b-4a18-814f-085af39924b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Lenovo\\Downloads\\Exam.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eadd5857-b4a7-4a61-966c-5c0580cba4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db974398-bff9-499c-aae7-c755dadefe35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 226\n"
     ]
    }
   ],
   "source": [
    "# Number of pages\n",
    "print(\"Total pages:\", len(reader.pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f91e4114-0669-44c9-a3c4-2733194a26b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_one = reader.pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a26ce813-5a7e-409e-a739-7aaab8a02efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_one_text = page_one.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b89e82a-033b-4f3e-8949-3370533b073a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Contents\\nI Supervised learning 5\\n1 Linear regression 8\\n1.1 LMS algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.2 The normal equations . . . . . . . . . . . . . . . . . . . . . . . 13\\n1.2.1 Matrix derivatives . . . . . . . . . . . . . . . . . . . . . 13\\n1.2.2 Least squares revisited . . . . . . . . . . . . . . . . . . 14\\n1.3 Probabilistic interpretation . . . . . . . . . . . . . . . . . . . . 15\\n1.4 Locally weighted linear regression (optional reading) . . . . . . 17\\n2 Classi\\x0ccation and logistic regression 20\\n2.1 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.2 Digression: the perceptron learning algorithm . . . . . . . . . 23\\n2.3 Multi-class classi\\x0ccation . . . . . . . . . . . . . . . . . . . . . 24\\n2.4 Another algorithm for maximizing `(\\x12) . . . . . . . . . . . . . 27\\n3 Generalized linear models 29\\n3.1 The exponential family . . . . . . . . . . . . . . . . . . . . . . 29\\n3.2 Constructing GLMs . . . . . . . . . . . . . . . . . . . . . . . . 31\\n3.2.1 Ordinary least squares . . . . . . . . . . . . . . . . . . 32\\n3.2.2 Logistic regression . . . . . . . . . . . . . . . . . . . . 33\\n4 Generative learning algorithms 34\\n4.1 Gaussian discriminant analysis . . . . . . . . . . . . . . . . . . 35\\n4.1.1 The multivariate normal distribution . . . . . . . . . . 35\\n4.1.2 The Gaussian discriminant analysis model . . . . . . . 38\\n4.1.3 Discussion: GDA and logistic regression . . . . . . . . 40\\n4.2 Naive bayes (Option Reading) . . . . . . . . . . . . . . . . . . 41\\n4.2.1 Laplace smoothing . . . . . . . . . . . . . . . . . . . . 44\\n4.2.2 Event models for text classi\\x0ccation . . . . . . . . . . . 46\\n1'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_one_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebb59090-cd67-4aac-a4b9-48fce44a5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PyPDF2.PdfReader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e22a25a-0510-479a-b995-8450db5ddbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_page = pdf_reader.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ff5272f-9af0-4f0c-b729-0298d850d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_writer = PyPDF2.PdfWriter(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "df7d9364-2a5b-408e-bc48-000907d19ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Type': '/Page',\n",
       " '/Contents': {'/Filter': '/FlateDecode'},\n",
       " '/Resources': {'/Font': {'/F26': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/BYZMGY+CMR17',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/BYZMGY+CMR17',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-33, -250, 945, 749],\n",
       "     '/Ascent': 694,\n",
       "     '/CapHeight': 683,\n",
       "     '/Descent': -195,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 53,\n",
       "     '/XHeight': 430,\n",
       "     '/CharSet': '/C/L/N/S/c/e/nine/o/parenleft/parenright/r/s/t/two/u',\n",
       "     '/FontFile': {'/Length1': 1586,\n",
       "      '/Length2': 8056,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 40,\n",
       "    '/LastChar': 117,\n",
       "    '/Widths': [354.1,\n",
       "     354.1,\n",
       "     458.6,\n",
       "     719.8,\n",
       "     249.6,\n",
       "     301.9,\n",
       "     249.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     458.6,\n",
       "     249.6,\n",
       "     249.6,\n",
       "     249.6,\n",
       "     719.8,\n",
       "     432.5,\n",
       "     432.5,\n",
       "     719.8,\n",
       "     693.3,\n",
       "     654.3,\n",
       "     667.6,\n",
       "     706.6,\n",
       "     628.2,\n",
       "     602.1,\n",
       "     726.3,\n",
       "     693.3,\n",
       "     327.6,\n",
       "     471.5,\n",
       "     719.4,\n",
       "     576,\n",
       "     850,\n",
       "     693.3,\n",
       "     719.8,\n",
       "     628.2,\n",
       "     719.8,\n",
       "     680.5,\n",
       "     510.9,\n",
       "     667.6,\n",
       "     693.3,\n",
       "     693.3,\n",
       "     954.5,\n",
       "     693.3,\n",
       "     693.3,\n",
       "     563.1,\n",
       "     249.6,\n",
       "     458.6,\n",
       "     249.6,\n",
       "     458.6,\n",
       "     249.6,\n",
       "     249.6,\n",
       "     458.6,\n",
       "     510.9,\n",
       "     406.4,\n",
       "     510.9,\n",
       "     406.4,\n",
       "     275.8,\n",
       "     458.6,\n",
       "     510.9,\n",
       "     249.6,\n",
       "     275.8,\n",
       "     484.7,\n",
       "     249.6,\n",
       "     772.1,\n",
       "     510.9,\n",
       "     458.6,\n",
       "     510.9,\n",
       "     484.7,\n",
       "     354.1,\n",
       "     359.4,\n",
       "     354.1,\n",
       "     510.9]},\n",
       "   '/F27': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/ISUIIJ+CMR12',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/ISUIIJ+CMR12',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-34, -251, 988, 750],\n",
       "     '/Ascent': 694,\n",
       "     '/CapHeight': 683,\n",
       "     '/Descent': -194,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 65,\n",
       "     '/XHeight': 431,\n",
       "     '/CharSet': '/A/B/C/D/E/F/G/H/I/J/K/L/M/N/O/P/Phi/Psi/Q/R/S/Sigma/T/U/V/W/X/Y/Z/a/b/bracketleft/bracketright/c/circumflex/colon/comma/d/dotaccent/e/eight/emdash/endash/equal/exclam/f/ff/ffi/ffl/fi/five/fl/four/g/h/hyphen/i/j/k/l/m/macron/n/nine/numbersign/o/one/p/parenleft/parenright/percent/period/plus/q/question/quotedblleft/quotedblright/quoteright/r/s/semicolon/seven/six/slash/t/three/tilde/two/u/v/w/x/y/z/zero',\n",
       "     '/FontFile': {'/Length1': 2916,\n",
       "      '/Length2': 22575,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 6,\n",
       "    '/LastChar': 126,\n",
       "    '/Widths': [707.2,\n",
       "     761.6,\n",
       "     707.2,\n",
       "     761.6,\n",
       "     707.2,\n",
       "     571.2,\n",
       "     544,\n",
       "     544,\n",
       "     816,\n",
       "     816,\n",
       "     272,\n",
       "     299.2,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     734,\n",
       "     435.2,\n",
       "     489.6,\n",
       "     707.2,\n",
       "     761.6,\n",
       "     489.6,\n",
       "     883.8,\n",
       "     992.6,\n",
       "     761.6,\n",
       "     272,\n",
       "     272,\n",
       "     489.6,\n",
       "     816,\n",
       "     489.6,\n",
       "     816,\n",
       "     761.6,\n",
       "     272,\n",
       "     380.8,\n",
       "     380.8,\n",
       "     489.6,\n",
       "     761.6,\n",
       "     272,\n",
       "     326.4,\n",
       "     272,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     272,\n",
       "     272,\n",
       "     272,\n",
       "     761.6,\n",
       "     462.4,\n",
       "     462.4,\n",
       "     761.6,\n",
       "     734,\n",
       "     693.4,\n",
       "     707.2,\n",
       "     747.8,\n",
       "     666.2,\n",
       "     639,\n",
       "     768.3,\n",
       "     734,\n",
       "     353.2,\n",
       "     503,\n",
       "     761.2,\n",
       "     611.8,\n",
       "     897.2,\n",
       "     734,\n",
       "     761.6,\n",
       "     666.2,\n",
       "     761.6,\n",
       "     720.6,\n",
       "     544,\n",
       "     707.2,\n",
       "     734,\n",
       "     734,\n",
       "     1006,\n",
       "     734,\n",
       "     734,\n",
       "     598.4,\n",
       "     272,\n",
       "     489.6,\n",
       "     272,\n",
       "     489.6,\n",
       "     272,\n",
       "     272,\n",
       "     489.6,\n",
       "     544,\n",
       "     435.2,\n",
       "     544,\n",
       "     435.2,\n",
       "     299.2,\n",
       "     489.6,\n",
       "     544,\n",
       "     272,\n",
       "     299.2,\n",
       "     516.8,\n",
       "     272,\n",
       "     816,\n",
       "     544,\n",
       "     489.6,\n",
       "     544,\n",
       "     516.8,\n",
       "     380.8,\n",
       "     386.2,\n",
       "     380.8,\n",
       "     544,\n",
       "     516.8,\n",
       "     707.2,\n",
       "     516.8,\n",
       "     516.8,\n",
       "     435.2,\n",
       "     489.6,\n",
       "     979.2,\n",
       "     489.6,\n",
       "     489.6]}},\n",
       "  '/ProcSet': ['/PDF', '/Text']},\n",
       " '/MediaBox': [0, 0, 612, 792],\n",
       " '/Annots': [],\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Count': 1,\n",
       "  '/Kids': [IndirectObject(4, 0, 1949834863936)]}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_writer.add_page(first_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "72c194e6-f862-4dc7-89da-df21ff70187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_output = open(\"new_pdf\",\"wb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa4a87df-b9f0-4f7a-b79b-c65abe9a1f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, <_io.BufferedWriter name='new_pdf'>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_writer.write(pdf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "006a02dd-eb1c-4948-a99a-078faf2ea310",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = []\n",
    "pdf_reader = PyPDF2.PdfReader(file_path)\n",
    "for p in range(len(pdf_reader.pages)):\n",
    "    page = pdf_reader.pages[p]\n",
    "    pdf_text.append(page.extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e32e154-8f3f-4ec3-8a7d-20c6bc108b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CS229 Lecture Notes\\nAndrew Ng and Tengyu Ma\\nApril 30, 2023',\n",
       " 'Contents\\nI Supervised learning 5\\n1 Linear regression 8\\n1.1 LMS algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.2 The normal equations . . . . . . . . . . . . . . . . . . . . . . . 13\\n1.2.1 Matrix derivatives . . . . . . . . . . . . . . . . . . . . . 13\\n1.2.2 Least squares revisited . . . . . . . . . . . . . . . . . . 14\\n1.3 Probabilistic interpretation . . . . . . . . . . . . . . . . . . . . 15\\n1.4 Locally weighted linear regression (optional reading) . . . . . . 17\\n2 Classi\\x0ccation and logistic regression 20\\n2.1 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.2 Digression: the perceptron learning algorithm . . . . . . . . . 23\\n2.3 Multi-class classi\\x0ccation . . . . . . . . . . . . . . . . . . . . . 24\\n2.4 Another algorithm for maximizing `(\\x12) . . . . . . . . . . . . . 27\\n3 Generalized linear models 29\\n3.1 The exponential family . . . . . . . . . . . . . . . . . . . . . . 29\\n3.2 Constructing GLMs . . . . . . . . . . . . . . . . . . . . . . . . 31\\n3.2.1 Ordinary least squares . . . . . . . . . . . . . . . . . . 32\\n3.2.2 Logistic regression . . . . . . . . . . . . . . . . . . . . 33\\n4 Generative learning algorithms 34\\n4.1 Gaussian discriminant analysis . . . . . . . . . . . . . . . . . . 35\\n4.1.1 The multivariate normal distribution . . . . . . . . . . 35\\n4.1.2 The Gaussian discriminant analysis model . . . . . . . 38\\n4.1.3 Discussion: GDA and logistic regression . . . . . . . . 40\\n4.2 Naive bayes (Option Reading) . . . . . . . . . . . . . . . . . . 41\\n4.2.1 Laplace smoothing . . . . . . . . . . . . . . . . . . . . 44\\n4.2.2 Event models for text classi\\x0ccation . . . . . . . . . . . 46\\n1',\n",
       " 'CS229 Spring 20223 2\\n5 Kernel methods 48\\n5.1 Feature maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\\n5.2 LMS (least mean squares) with features . . . . . . . . . . . . . 49\\n5.3 LMS with the kernel trick . . . . . . . . . . . . . . . . . . . . 49\\n5.4 Properties of kernels . . . . . . . . . . . . . . . . . . . . . . . 53\\n6 Support vector machines 59\\n6.1 Margins: intuition . . . . . . . . . . . . . . . . . . . . . . . . . 59\\n6.2 Notation (option reading) . . . . . . . . . . . . . . . . . . . . 61\\n6.3 Functional and geometric margins (option reading) . . . . . . 61\\n6.4 The optimal margin classi\\x0cer (option reading) . . . . . . . . . 63\\n6.5 Lagrange duality (optional reading) . . . . . . . . . . . . . . . 65\\n6.6 Optimal margin classi\\x0cers: the dual form (option reading) . . 68\\n6.7 Regularization and the non-separable case (optional reading) . 72\\n6.8 The SMO algorithm (optional reading) . . . . . . . . . . . . . 73\\n6.8.1 Coordinate ascent . . . . . . . . . . . . . . . . . . . . . 74\\n6.8.2 SMO . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\nII Deep learning 79\\n7 Deep learning 80\\n7.1 Supervised learning with non-linear models . . . . . . . . . . . 80\\n7.2 Neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . 84\\n7.3 Modules in Modern Neural Networks . . . . . . . . . . . . . . 92\\n7.4 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . 98\\n7.4.1 Preliminaries on partial derivatives . . . . . . . . . . . 99\\n7.4.2 General strategy of backpropagation . . . . . . . . . . 102\\n7.4.3 Backward functions for basic modules . . . . . . . . . . 105\\n7.4.4 Back-propagation for MLPs . . . . . . . . . . . . . . . 107\\n7.5 Vectorization over training examples . . . . . . . . . . . . . . 109\\nIII Generalization and regularization 112\\n8 Generalization 113\\n8.1 Bias-variance tradeo\\x0b . . . . . . . . . . . . . . . . . . . . . . . 115\\n8.1.1 A mathematical decomposition (for regression) . . . . . 120\\n8.2 The double descent phenomenon . . . . . . . . . . . . . . . . . 121\\n8.3 Sample complexity bounds (optional readings) . . . . . . . . . 126',\n",
       " \"CS229 Spring 20223 3\\n8.3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . 126\\n8.3.2 The case of \\x0cnite H. . . . . . . . . . . . . . . . . . . . 128\\n8.3.3 The case of in\\x0cnite H. . . . . . . . . . . . . . . . . . 131\\n9 Regularization and model selection 135\\n9.1 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n9.2 Implicit regularization e\\x0bect . . . . . . . . . . . . . . . . . . . 137\\n9.3 Model selection via cross validation . . . . . . . . . . . . . . . 139\\n9.4 Bayesian statistics and regularization . . . . . . . . . . . . . . 142\\nIV Unsupervised learning 144\\n10 Clustering and the k-means algorithm 145\\n11 EM algorithms 148\\n11.1 EM for mixture of Gaussians . . . . . . . . . . . . . . . . . . . 148\\n11.2 Jensen's inequality . . . . . . . . . . . . . . . . . . . . . . . . 151\\n11.3 General EM algorithms . . . . . . . . . . . . . . . . . . . . . . 152\\n11.3.1 Other interpretation of ELBO . . . . . . . . . . . . . . 158\\n11.4 Mixture of Gaussians revisited . . . . . . . . . . . . . . . . . . 158\\n11.5 Variational inference and variational auto-encoder (optional\\nreading) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\\n12 Principal components analysis 165\\n13 Independent components analysis 171\\n13.1 ICA ambiguities . . . . . . . . . . . . . . . . . . . . . . . . . . 172\\n13.2 Densities and linear transformations . . . . . . . . . . . . . . . 173\\n13.3 ICA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\\n14 Self-supervised learning and foundation models 177\\n14.1 Pretraining and adaptation . . . . . . . . . . . . . . . . . . . . 177\\n14.2 Pretraining methods in computer vision . . . . . . . . . . . . . 179\\n14.3 Pretrained large language models . . . . . . . . . . . . . . . . 181\\n14.3.1 Zero-shot learning and in-context learning . . . . . . . 183\\nV Reinforcement Learning and Control 185\\n15 Reinforcement learning 186\",\n",
       " 'CS229 Spring 20223 4\\n15.1 Markov decision processes . . . . . . . . . . . . . . . . . . . . 187\\n15.2 Value iteration and policy iteration . . . . . . . . . . . . . . . 189\\n15.3 Learning a model for an MDP . . . . . . . . . . . . . . . . . . 191\\n15.4 Continuous state MDPs . . . . . . . . . . . . . . . . . . . . . 193\\n15.4.1 Discretization . . . . . . . . . . . . . . . . . . . . . . . 193\\n15.4.2 Value function approximation . . . . . . . . . . . . . . 196\\n15.5 Connections between Policy and Value Iteration (Optional) . . 200\\n16 LQR, DDP and LQG 203\\n16.1 Finite-horizon MDPs . . . . . . . . . . . . . . . . . . . . . . . 203\\n16.2 Linear Quadratic Regulation (LQR) . . . . . . . . . . . . . . . 207\\n16.3 From non-linear dynamics to LQR . . . . . . . . . . . . . . . 210\\n16.3.1 Linearization of dynamics . . . . . . . . . . . . . . . . 211\\n16.3.2 Di\\x0berential Dynamic Programming (DDP) . . . . . . . 211\\n16.4 Linear Quadratic Gaussian (LQG) . . . . . . . . . . . . . . . . 213\\n17 Policy Gradient (REINFORCE) 217',\n",
       " 'Part I\\nSupervised learning\\n5',\n",
       " '6\\nLet\\'s start by talking about a few examples of supervised learning prob-\\nlems. Suppose we have a dataset giving the living areas and prices of 47\\nhouses from Portland, Oregon:\\nLiving area (feet2)Price (1000 $s)\\n2104 400\\n1600 330\\n2400 369\\n1416 232\\n3000 540\\n......\\nWe can plot this data:\\n500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices\\nsquare feetprice (in $1000)\\nGiven data like this, how can we learn to predict the prices of other houses\\nin Portland, as a function of the size of their living areas?\\nTo establish notation for future use, we\\'ll use x(i)to denote the \\\\input\"\\nvariables (living area in this example), also called input features , andy(i)\\nto denote the \\\\output\" or target variable that we are trying to predict\\n(price). A pair ( x(i);y(i)) is called a training example , and the dataset\\nthat we\\'ll be using to learn|a list of ntraining examples f(x(i);y(i));i=\\n1;:::;ng|is called a training set . Note that the superscript \\\\( i)\" in the\\nnotation is simply an index into the training set, and has nothing to do with\\nexponentiation. We will also use Xdenote the space of input values, and Y\\nthe space of output values. In this example, X=Y=R.\\nTo describe the supervised learning problem slightly more formally, our\\ngoal is, given a training set, to learn a function h:X7!Y so thath(x) is a\\n\\\\good\" predictor for the corresponding value of y. For historical reasons, this',\n",
       " \"7\\nfunctionhis called a hypothesis . Seen pictorially, the process is therefore\\nlike this:\\nTraining \\n    set\\n house.)(living area ofLearning \\nalgorithm\\nh predicted y x\\n(predicted price)\\nof house)\\nWhen the target variable that we're trying to predict is continuous, such\\nas in our housing example, we call the learning problem a regression prob-\\nlem. When ycan take on only a small number of discrete values (such as\\nif, given the living area, we wanted to predict if a dwelling is a house or an\\napartment, say), we call it a classi\\x0ccation problem.\",\n",
       " \"Chapter 1\\nLinear regression\\nTo make our housing example more interesting, let's consider a slightly richer\\ndataset in which we also know the number of bedrooms in each house:\\nLiving area (feet2)#bedrooms Price (1000 $s)\\n2104 3 400\\n1600 3 330\\n2400 3 369\\n1416 2 232\\n3000 4 540\\n.........\\nHere, thex's are two-dimensional vectors in R2. For instance, x(i)\\n1is the\\nliving area of the i-th house in the training set, and x(i)\\n2is its number of\\nbedrooms. (In general, when designing a learning problem, it will be up to\\nyou to decide what features to choose, so if you are out in Portland gathering\\nhousing data, you might also decide to include other features such as whether\\neach house has a \\x0creplace, the number of bathrooms, and so on. We'll say\\nmore about feature selection later, but for now let's take the features as\\ngiven.)\\nTo perform supervised learning, we must decide how we're going to rep-\\nresent functions/hypotheses hin a computer. As an initial choice, let's say\\nwe decide to approximate yas a linear function of x:\\nh\\x12(x) =\\x120+\\x121x1+\\x122x2\\nHere, the\\x12i's are the parameters (also called weights ) parameterizing the\\nspace of linear functions mapping from XtoY. When there is no risk of\\n8\",\n",
       " '9\\nconfusion, we will drop the \\x12subscript in h\\x12(x), and write it more simply as\\nh(x). To simplify our notation, we also introduce the convention of letting\\nx0= 1 (this is the intercept term ), so that\\nh(x) =dX\\ni=0\\x12ixi=\\x12Tx;\\nwhere on the right-hand side above we are viewing \\x12andxboth as vectors,\\nand heredis the number of input variables (not counting x0).\\nNow, given a training set, how do we pick, or learn, the parameters \\x12?\\nOne reasonable method seems to be to make h(x) close toy, at least for\\nthe training examples we have. To formalize this, we will de\\x0cne a function\\nthat measures, for each value of the \\x12\\'s, how close the h(x(i))\\'s are to the\\ncorresponding y(i)\\'s. We de\\x0cne the cost function :\\nJ(\\x12) =1\\n2nX\\ni=1(h\\x12(x(i))\\x00y(i))2:\\nIf you\\'ve seen linear regression before, you may recognize this as the familiar\\nleast-squares cost function that gives rise to the ordinary least squares\\nregression model. Whether or not you have seen it previously, let\\'s keep\\ngoing, and we\\'ll eventually show this to be a special case of a much broader\\nfamily of algorithms.\\n1.1 LMS algorithm\\nWe want to choose \\x12so as to minimize J(\\x12). To do so, let\\'s use a search\\nalgorithm that starts with some \\\\initial guess\" for \\x12, and that repeatedly\\nchanges\\x12to makeJ(\\x12) smaller, until hopefully we converge to a value of\\n\\x12that minimizes J(\\x12). Speci\\x0ccally, let\\'s consider the gradient descent\\nalgorithm, which starts with some initial \\x12, and repeatedly performs the\\nupdate:\\n\\x12j:=\\x12j\\x00\\x0b@\\n@\\x12jJ(\\x12):\\n(This update is simultaneously performed for all values of j= 0;:::;d .)\\nHere,\\x0bis called the learning rate . This is a very natural algorithm that\\nrepeatedly takes a step in the direction of steepest decrease of J.\\nIn order to implement this algorithm, we have to work out what is the\\npartial derivative term on the right hand side. Let\\'s \\x0crst work it out for the',\n",
       " '10\\ncase of if we have only one training example ( x;y), so that we can neglect\\nthe sum in the de\\x0cnition of J. We have:\\n@\\n@\\x12jJ(\\x12) =@\\n@\\x12j1\\n2(h\\x12(x)\\x00y)2\\n= 2\\x011\\n2(h\\x12(x)\\x00y)\\x01@\\n@\\x12j(h\\x12(x)\\x00y)\\n= (h\\x12(x)\\x00y)\\x01@\\n@\\x12j dX\\ni=0\\x12ixi\\x00y!\\n= (h\\x12(x)\\x00y)xj\\nFor a single training example, this gives the update rule:1\\n\\x12j:=\\x12j+\\x0b\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\nj:\\nThe rule is called the LMS update rule (LMS stands for \\\\least mean squares\"),\\nand is also known as the Widrow-Ho\\x0b learning rule. This rule has several\\nproperties that seem natural and intuitive. For instance, the magnitude of\\nthe update is proportional to the error term (y(i)\\x00h\\x12(x(i))); thus, for in-\\nstance, if we are encountering a training example on which our prediction\\nnearly matches the actual value of y(i), then we \\x0cnd that there is little need\\nto change the parameters; in contrast, a larger change to the parameters will\\nbe made if our prediction h\\x12(x(i)) has a large error (i.e., if it is very far from\\ny(i)).\\nWe\\'d derived the LMS rule for when there was only a single training\\nexample. There are two ways to modify this method for a training set of\\nmore than one example. The \\x0crst is replace it with the following algorithm:\\nRepeat until convergence f\\n\\x12j:=\\x12j+\\x0bnX\\ni=1\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\nj;(for everyj) (1.1)\\ng\\n1We use the notation \\\\ a:=b\" to denote an operation (in a computer program) in\\nwhich we setthe value of a variable ato be equal to the value of b. In other words, this\\noperation overwrites awith the value of b. In contrast, we will write \\\\ a=b\" when we are\\nasserting a statement of fact, that the value of ais equal to the value of b.',\n",
       " \"11\\nBy grouping the updates of the coordinates into an update of the vector\\n\\x12, we can rewrite update (1.1) in a slightly more succinct way:\\n\\x12:=\\x12+\\x0bnX\\ni=1\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\nThe reader can easily verify that the quantity in the summation in the\\nupdate rule above is just @J(\\x12)=@\\x12j(for the original de\\x0cnition of J). So, this\\nis simply gradient descent on the original cost function J. This method looks\\nat every example in the entire training set on every step, and is called batch\\ngradient descent . Note that, while gradient descent can be susceptible\\nto local minima in general, the optimization problem we have posed here\\nfor linear regression has only one global, and no other local, optima; thus\\ngradient descent always converges (assuming the learning rate \\x0bis not too\\nlarge) to the global minimum. Indeed, Jis a convex quadratic function.\\nHere is an example of gradient descent as it is run to minimize a quadratic\\nfunction.\\n5 10 15 20 25 30 35 40 45 505101520253035404550\\nThe ellipses shown above are the contours of a quadratic function. Also\\nshown is the trajectory taken by gradient descent, which was initialized at\\n(48,30). The x's in the \\x0cgure (joined by straight lines) mark the successive\\nvalues of\\x12that gradient descent went through.\\nWhen we run batch gradient descent to \\x0ct \\x12on our previous dataset,\\nto learn to predict housing price as a function of living area, we obtain\\n\\x120= 71:27,\\x121= 0:1345. If we plot h\\x12(x) as a function of x(area), along\\nwith the training data, we obtain the following \\x0cgure:\",\n",
       " '12\\n500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices\\nsquare feetprice (in $1000)\\nIf the number of bedrooms were included as one of the input features as well,\\nwe get\\x120= 89:60;\\x121= 0:1392,\\x122=\\x008:738.\\nThe above results were obtained with batch gradient descent. There is\\nan alternative to batch gradient descent that also works very well. Consider\\nthe following algorithm:\\nLoopf\\nfori= 1 ton,f\\n\\x12j:=\\x12j+\\x0b\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\nj;(for everyj) (1.2)\\ng\\ng\\nBy grouping the updates of the coordinates into an update of the vector\\n\\x12, we can rewrite update (1.2) in a slightly more succinct way:\\n\\x12:=\\x12+\\x0b\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\nIn this algorithm, we repeatedly run through the training set, and each\\ntime we encounter a training example, we update the parameters according\\nto the gradient of the error with respect to that single training example only.\\nThis algorithm is called stochastic gradient descent (also incremental\\ngradient descent ). Whereas batch gradient descent has to scan through\\nthe entire training set before taking a single step|a costly operation if nis\\nlarge|stochastic gradient descent can start making progress right away, and',\n",
       " '13\\ncontinues to make progress with each example it looks at. Often, stochastic\\ngradient descent gets \\x12\\\\close\" to the minimum much faster than batch gra-\\ndient descent. (Note however that it may never \\\\converge\" to the minimum,\\nand the parameters \\x12will keep oscillating around the minimum of J(\\x12); but\\nin practice most of the values near the minimum will be reasonably good\\napproximations to the true minimum.2) For these reasons, particularly when\\nthe training set is large, stochastic gradient descent is often preferred over\\nbatch gradient descent.\\n1.2 The normal equations\\nGradient descent gives one way of minimizing J. Let\\'s discuss a second way\\nof doing so, this time performing the minimization explicitly and without\\nresorting to an iterative algorithm. In this method, we will minimize Jby\\nexplicitly taking its derivatives with respect to the \\x12j\\'s, and setting them to\\nzero. To enable us to do this without having to write reams of algebra and\\npages full of matrices of derivatives, let\\'s introduce some notation for doing\\ncalculus with matrices.\\n1.2.1 Matrix derivatives\\nFor a function f:Rn\\x02d7!Rmapping from n-by-dmatrices to the real\\nnumbers, we de\\x0cne the derivative of fwith respect to Ato be:\\nrAf(A) =2\\n64@f\\n@A11\\x01\\x01\\x01@f\\n@A1d.........\\n@f\\n@An1\\x01\\x01\\x01@f\\n@And3\\n75\\nThus, the gradient rAf(A) is itself an n-by-dmatrix, whose ( i;j)-element is\\n@f=@Aij. For example, suppose A=\\x14\\nA11A12\\nA21A22\\x15\\nis a 2-by-2 matrix, and\\nthe function f:R2\\x0227!Ris given by\\nf(A) =3\\n2A11+ 5A2\\n12+A21A22:\\n2By slowly letting the learning rate \\x0bdecrease to zero as the algorithm runs, it is also\\npossible to ensure that the parameters will converge to the global minimum rather than\\nmerely oscillate around the minimum.',\n",
       " \"14\\nHere,Aijdenotes the ( i;j) entry of the matrix A. We then have\\nrAf(A) =\\x143\\n210A12\\nA22A21\\x15\\n:\\n1.2.2 Least squares revisited\\nArmed with the tools of matrix derivatives, let us now proceed to \\x0cnd in\\nclosed-form the value of \\x12that minimizes J(\\x12). We begin by re-writing Jin\\nmatrix-vectorial notation.\\nGiven a training set, de\\x0cne the design matrix Xto be then-by-dmatrix\\n(actuallyn-by-d+ 1, if we include the intercept term) that contains the\\ntraining examples' input values in its rows:\\nX=2\\n6664| (x(1))T|\\n| (x(2))T|\\n...\\n| (x(n))T|3\\n7775:\\nAlso, let~ ybe then-dimensional vector containing all the target values from\\nthe training set:\\n~ y=2\\n6664y(1)\\ny(2)\\n...\\ny(n)3\\n7775:\\nNow, since h\\x12(x(i)) = (x(i))T\\x12, we can easily verify that\\nX\\x12\\x00~ y=2\\n64(x(1))T\\x12\\n...\\n(x(n))T\\x123\\n75\\x002\\n64y(1)\\n...\\ny(n)3\\n75\\n=2\\n64h\\x12(x(1))\\x00y(1)\\n...\\nh\\x12(x(n))\\x00y(n)3\\n75:\\nThus, using the fact that for a vector z, we have that zTz=P\\niz2\\ni:\\n1\\n2(X\\x12\\x00~ y)T(X\\x12\\x00~ y) =1\\n2nX\\ni=1(h\\x12(x(i))\\x00y(i))2\\n=J(\\x12)\",\n",
       " '15\\nFinally, to minimize J, let\\'s \\x0cnd its derivatives with respect to \\x12. Hence,\\nr\\x12J(\\x12) =r\\x121\\n2(X\\x12\\x00~ y)T(X\\x12\\x00~ y)\\n=1\\n2r\\x12\\x00\\n(X\\x12)TX\\x12\\x00(X\\x12)T~ y\\x00~ yT(X\\x12) +~ yT~ y\\x01\\n=1\\n2r\\x12\\x00\\n\\x12T(XTX)\\x12\\x00~ yT(X\\x12)\\x00~ yT(X\\x12)\\x01\\n=1\\n2r\\x12\\x00\\n\\x12T(XTX)\\x12\\x002(XT~ y)T\\x12\\x01\\n=1\\n2\\x00\\n2XTX\\x12\\x002XT~ y\\x01\\n=XTX\\x12\\x00XT~ y\\nIn the third step, we used the fact that aTb=bTa, and in the \\x0cfth step\\nused the factsrxbTx=bandrxxTAx= 2Axfor symmetric matrix A(for\\nmore details, see Section 4.3 of \\\\Linear Algebra Review and Reference\"). To\\nminimizeJ, we set its derivatives to zero, and obtain the normal equations :\\nXTX\\x12=XT~ y\\nThus, the value of \\x12that minimizes J(\\x12) is given in closed form by the\\nequation\\n\\x12= (XTX)\\x001XT~ y:3\\n1.3 Probabilistic interpretation\\nWhen faced with a regression problem, why might linear regression, and\\nspeci\\x0ccally why might the least-squares cost function J, be a reasonable\\nchoice? In this section, we will give a set of probabilistic assumptions, under\\nwhich least-squares regression is derived as a very natural algorithm.\\nLet us assume that the target variables and the inputs are related via the\\nequation\\ny(i)=\\x12Tx(i)+\\x0f(i);\\n3Note that in the above step, we are implicitly assuming that XTXis an invertible\\nmatrix. This can be checked before calculating the inverse. If either the number of\\nlinearly independent examples is fewer than the number of features, or if the features\\nare not linearly independent, then XTXwill not be invertible. Even in such cases, it is\\npossible to \\\\\\x0cx\" the situation with additional techniques, which we skip here for the sake\\nof simplicty.',\n",
       " '16\\nwhere\\x0f(i)is an error term that captures either unmodeled e\\x0bects (such as\\nif there are some features very pertinent to predicting housing price, but\\nthat we\\'d left out of the regression), or random noise. Let us further assume\\nthat the\\x0f(i)are distributed IID (independently and identically distributed)\\naccording to a Gaussian distribution (also called a Normal distribution) with\\nmean zero and some variance \\x1b2. We can write this assumption as \\\\ \\x0f(i)\\x18\\nN(0;\\x1b2).\" I.e., the density of \\x0f(i)is given by\\np(\\x0f(i)) =1p\\n2\\x19\\x1bexp\\x12\\n\\x00(\\x0f(i))2\\n2\\x1b2\\x13\\n:\\nThis implies that\\np(y(i)jx(i);\\x12) =1p\\n2\\x19\\x1bexp\\x12\\n\\x00(y(i)\\x00\\x12Tx(i))2\\n2\\x1b2\\x13\\n:\\nThe notation \\\\ p(y(i)jx(i);\\x12)\" indicates that this is the distribution of y(i)\\ngivenx(i)and parameterized by \\x12. Note that we should not condition on \\x12\\n(\\\\p(y(i)jx(i);\\x12)\"), since\\x12is not a random variable. We can also write the\\ndistribution of y(i)asy(i)jx(i);\\x12\\x18N(\\x12Tx(i);\\x1b2).\\nGivenX(the design matrix, which contains all the x(i)\\'s) and\\x12, what\\nis the distribution of the y(i)\\'s? The probability of the data is given by\\np(~ yjX;\\x12). This quantity is typically viewed a function of ~ y(and perhaps X),\\nfor a \\x0cxed value of \\x12. When we wish to explicitly view this as a function of\\n\\x12, we will instead call it the likelihood function:\\nL(\\x12) =L(\\x12;X;~ y) =p(~ yjX;\\x12):\\nNote that by the independence assumption on the \\x0f(i)\\'s (and hence also the\\ny(i)\\'s given the x(i)\\'s), this can also be written\\nL(\\x12) =nY\\ni=1p(y(i)jx(i);\\x12)\\n=nY\\ni=11p\\n2\\x19\\x1bexp\\x12\\n\\x00(y(i)\\x00\\x12Tx(i))2\\n2\\x1b2\\x13\\n:\\nNow, given this probabilistic model relating the y(i)\\'s and thex(i)\\'s, what\\nis a reasonable way of choosing our best guess of the parameters \\x12? The\\nprincipal of maximum likelihood says that we should choose \\x12so as to\\nmake the data as high probability as possible. I.e., we should choose \\x12to\\nmaximizeL(\\x12).',\n",
       " \"17\\nInstead of maximizing L(\\x12), we can also maximize any strictly increasing\\nfunction of L(\\x12). In particular, the derivations will be a bit simpler if we\\ninstead maximize the log likelihood `(\\x12):\\n`(\\x12) = logL(\\x12)\\n= lognY\\ni=11p\\n2\\x19\\x1bexp\\x12\\n\\x00(y(i)\\x00\\x12Tx(i))2\\n2\\x1b2\\x13\\n=nX\\ni=1log1p\\n2\\x19\\x1bexp\\x12\\n\\x00(y(i)\\x00\\x12Tx(i))2\\n2\\x1b2\\x13\\n=nlog1p\\n2\\x19\\x1b\\x001\\n\\x1b2\\x011\\n2nX\\ni=1(y(i)\\x00\\x12Tx(i))2:\\nHence, maximizing `(\\x12) gives the same answer as minimizing\\n1\\n2nX\\ni=1(y(i)\\x00\\x12Tx(i))2;\\nwhich we recognize to be J(\\x12), our original least-squares cost function.\\nTo summarize: Under the previous probabilistic assumptions on the data,\\nleast-squares regression corresponds to \\x0cnding the maximum likelihood esti-\\nmate of\\x12. This is thus one set of assumptions under which least-squares re-\\ngression can be justi\\x0ced as a very natural method that's just doing maximum\\nlikelihood estimation. (Note however that the probabilistic assumptions are\\nby no means necessary for least-squares to be a perfectly good and rational\\nprocedure, and there may|and indeed there are|other natural assumptions\\nthat can also be used to justify it.)\\nNote also that, in our previous discussion, our \\x0cnal choice of \\x12did not\\ndepend on what was \\x1b2, and indeed we'd have arrived at the same result\\neven if\\x1b2were unknown. We will use this fact again later, when we talk\\nabout the exponential family and generalized linear models.\\n1.4 Locally weighted linear regression (optional\\nreading)\\nConsider the problem of predicting yfromx2R. The leftmost \\x0cgure below\\nshows the result of \\x0ctting a y=\\x120+\\x121xto a dataset. We see that the data\\ndoesn't really lie on straight line, and so the \\x0ct is not very good.\",\n",
       " \"18\\n0 1 2 3 4 5 6 700.511.522.533.544.5\\nxy\\n0 1 2 3 4 5 6 700.511.522.533.544.5\\nxy\\n0 1 2 3 4 5 6 700.511.522.533.544.5\\nxy\\nInstead, if we had added an extra feature x2, and \\x0cty=\\x120+\\x121x+\\x122x2,\\nthen we obtain a slightly better \\x0ct to the data. (See middle \\x0cgure) Naively, it\\nmight seem that the more features we add, the better. However, there is also\\na danger in adding too many features: The rightmost \\x0cgure is the result of\\n\\x0ctting a 5-th order polynomial y=P5\\nj=0\\x12jxj. We see that even though the\\n\\x0ctted curve passes through the data perfectly, we would not expect this to\\nbe a very good predictor of, say, housing prices ( y) for di\\x0berent living areas\\n(x). Without formally de\\x0cning what these terms mean, we'll say the \\x0cgure\\non the left shows an instance of under\\x0ctting |in which the data clearly\\nshows structure not captured by the model|and the \\x0cgure on the right is\\nan example of over\\x0ctting . (Later in this class, when we talk about learning\\ntheory we'll formalize some of these notions, and also de\\x0cne more carefully\\njust what it means for a hypothesis to be good or bad.)\\nAs discussed previously, and as shown in the example above, the choice of\\nfeatures is important to ensuring good performance of a learning algorithm.\\n(When we talk about model selection, we'll also see algorithms for automat-\\nically choosing a good set of features.) In this section, let us brie\\ry talk\\nabout the locally weighted linear regression (LWR) algorithm which, assum-\\ning there is su\\x0ecient training data, makes the choice of features less critical.\\nThis treatment will be brief, since you'll get a chance to explore some of the\\nproperties of the LWR algorithm yourself in the homework.\\nIn the original linear regression algorithm, to make a prediction at a query\\npointx(i.e., to evaluate h(x)), we would:\\n1. Fit\\x12to minimizeP\\ni(y(i)\\x00\\x12Tx(i))2.\\n2. Output\\x12Tx.\\nIn contrast, the locally weighted linear regression algorithm does the fol-\\nlowing:\\n1. Fit\\x12to minimizeP\\niw(i)(y(i)\\x00\\x12Tx(i))2.\\n2. Output\\x12Tx.\",\n",
       " '19\\nHere, thew(i)\\'s are non-negative valued weights . Intuitively, if w(i)is large\\nfor a particular value of i, then in picking \\x12, we\\'ll try hard to make ( y(i)\\x00\\n\\x12Tx(i))2small. Ifw(i)is small, then the ( y(i)\\x00\\x12Tx(i))2error term will be\\npretty much ignored in the \\x0ct.\\nA fairly standard choice for the weights is4\\nw(i)= exp\\x12\\n\\x00(x(i)\\x00x)2\\n2\\x1c2\\x13\\nNote that the weights depend on the particular point xat which we\\'re trying\\nto evaluate x. Moreover, ifjx(i)\\x00xjis small, then w(i)is close to 1; and\\nifjx(i)\\x00xjis large, then w(i)is small. Hence, \\x12is chosen giving a much\\nhigher \\\\weight\" to the (errors on) training examples close to the query point\\nx. (Note also that while the formula for the weights takes a form that is\\ncosmetically similar to the density of a Gaussian distribution, the w(i)\\'s do\\nnot directly have anything to do with Gaussians, and in particular the w(i)\\nare not random variables, normally distributed or otherwise.) The parameter\\n\\x1ccontrols how quickly the weight of a training example falls o\\x0b with distance\\nof itsx(i)from the query point x;\\x1cis called the bandwidth parameter, and\\nis also something that you\\'ll get to experiment with in your homework.\\nLocally weighted linear regression is the \\x0crst example we\\'re seeing of a\\nnon-parametric algorithm. The (unweighted) linear regression algorithm\\nthat we saw earlier is known as a parametric learning algorithm, because\\nit has a \\x0cxed, \\x0cnite number of parameters (the \\x12i\\'s), which are \\x0ct to the\\ndata. Once we\\'ve \\x0ct the \\x12i\\'s and stored them away, we no longer need to\\nkeep the training data around to make future predictions. In contrast, to\\nmake predictions using locally weighted linear regression, we need to keep\\nthe entire training set around. The term \\\\non-parametric\" (roughly) refers\\nto the fact that the amount of stu\\x0b we need to keep in order to represent the\\nhypothesis hgrows linearly with the size of the training set.\\n4Ifxis vector-valued, this is generalized to be w(i)= exp(\\x00(x(i)\\x00x)T(x(i)\\x00x)=(2\\x1c2)),\\norw(i)= exp(\\x00(x(i)\\x00x)T\\x06\\x001(x(i)\\x00x)=(2\\x1c2)), for an appropriate choice of \\x1cor \\x06.',\n",
       " 'Chapter 2\\nClassi\\x0ccation and logistic\\nregression\\nLet\\'s now talk about the classi\\x0ccation problem. This is just like the regression\\nproblem, except that the values ywe now want to predict take on only\\na small number of discrete values. For now, we will focus on the binary\\nclassi\\x0ccation problem in which ycan take on only two values, 0 and 1.\\n(Most of what we say here will also generalize to the multiple-class case.)\\nFor instance, if we are trying to build a spam classi\\x0cer for email, then x(i)\\nmay be some features of a piece of email, and ymay be 1 if it is a piece\\nof spam mail, and 0 otherwise. 0 is also called the negative class , and 1\\nthepositive class , and they are sometimes also denoted by the symbols \\\\-\"\\nand \\\\+.\" Given x(i), the corresponding y(i)is also called the label for the\\ntraining example.\\n2.1 Logistic regression\\nWe could approach the classi\\x0ccation problem ignoring the fact that yis\\ndiscrete-valued, and use our old linear regression algorithm to try to predict\\nygivenx. However, it is easy to construct examples where this method\\nperforms very poorly. Intuitively, it also doesn\\'t make sense for h\\x12(x) to take\\nvalues larger than 1 or smaller than 0 when we know that y2f0;1g.\\nTo \\x0cx this, let\\'s change the form for our hypotheses h\\x12(x). We will choose\\nh\\x12(x) =g(\\x12Tx) =1\\n1 +e\\x00\\x12Tx;\\nwhere\\ng(z) =1\\n1 +e\\x00z\\n20',\n",
       " \"21\\nis called the logistic function or the sigmoid function . Here is a plot\\nshowingg(z):\\n−5 −4 −3 −2 −1 0 1 2 3 4 500.10.20.30.40.50.60.70.80.91\\nzg(z)\\nNotice that g(z) tends towards 1 as z!1 , andg(z) tends towards 0 as\\nz!\\x001 . Moreover, g(z), and hence also h(x), is always bounded between\\n0 and 1. As before, we are keeping the convention of letting x0= 1, so that\\n\\x12Tx=\\x120+Pd\\nj=1\\x12jxj.\\nFor now, let's take the choice of gas given. Other functions that smoothly\\nincrease from 0 to 1 can also be used, but for a couple of reasons that we'll see\\nlater (when we talk about GLMs, and when we talk about generative learning\\nalgorithms), the choice of the logistic function is a fairly natural one. Before\\nmoving on, here's a useful property of the derivative of the sigmoid function,\\nwhich we write as g0:\\ng0(z) =d\\ndz1\\n1 +e\\x00z\\n=1\\n(1 +e\\x00z)2\\x00\\ne\\x00z\\x01\\n=1\\n(1 +e\\x00z)\\x01\\x12\\n1\\x001\\n(1 +e\\x00z)\\x13\\n=g(z)(1\\x00g(z)):\\nSo, given the logistic regression model, how do we \\x0ct \\x12for it? Following\\nhow we saw least squares regression could be derived as the maximum like-\\nlihood estimator under a set of assumptions, let's endow our classi\\x0ccation\\nmodel with a set of probabilistic assumptions, and then \\x0ct the parameters\\nvia maximum likelihood.\",\n",
       " \"22\\nLet us assume that\\nP(y= 1jx;\\x12) =h\\x12(x)\\nP(y= 0jx;\\x12) = 1\\x00h\\x12(x)\\nNote that this can be written more compactly as\\np(yjx;\\x12) = (h\\x12(x))y(1\\x00h\\x12(x))1\\x00y\\nAssuming that the ntraining examples were generated independently, we\\ncan then write down the likelihood of the parameters as\\nL(\\x12) =p(~ yjX;\\x12)\\n=nY\\ni=1p(y(i)jx(i);\\x12)\\n=nY\\ni=1\\x00\\nh\\x12(x(i))\\x01y(i)\\x00\\n1\\x00h\\x12(x(i))\\x011\\x00y(i)\\nAs before, it will be easier to maximize the log likelihood:\\n`(\\x12) = logL(\\x12) =nX\\ni=1y(i)logh(x(i)) + (1\\x00y(i)) log(1\\x00h(x(i))) (2.1)\\nHow do we maximize the likelihood? Similar to our derivation in the case\\nof linear regression, we can use gradient ascent. Written in vectorial notation,\\nour updates will therefore be given by \\x12:=\\x12+\\x0br\\x12`(\\x12). (Note the positive\\nrather than negative sign in the update formula, since we're maximizing,\\nrather than minimizing, a function now.) Let's start by working with just\\none training example ( x;y), and take derivatives to derive the stochastic\\ngradient ascent rule:\\n@\\n@\\x12j`(\\x12) =\\x12\\ny1\\ng(\\x12Tx)\\x00(1\\x00y)1\\n1\\x00g(\\x12Tx)\\x13@\\n@\\x12jg(\\x12Tx)\\n=\\x12\\ny1\\ng(\\x12Tx)\\x00(1\\x00y)1\\n1\\x00g(\\x12Tx)\\x13\\ng(\\x12Tx)(1\\x00g(\\x12Tx))@\\n@\\x12j\\x12Tx\\n=\\x00\\ny(1\\x00g(\\x12Tx))\\x00(1\\x00y)g(\\x12Tx)\\x01\\nxj\\n= (y\\x00h\\x12(x))xj (2.2)\",\n",
       " \"23\\nAbove, we used the fact that g0(z) =g(z)(1\\x00g(z)). This therefore gives us\\nthe stochastic gradient ascent rule\\n\\x12j:=\\x12j+\\x0b\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\nj\\nIf we compare this to the LMS update rule, we see that it looks identical; but\\nthis is notthe same algorithm, because h\\x12(x(i)) is now de\\x0cned as a non-linear\\nfunction of \\x12Tx(i). Nonetheless, it's a little surprising that we end up with\\nthe same update rule for a rather di\\x0berent algorithm and learning problem.\\nIs this coincidence, or is there a deeper reason behind this? We'll answer this\\nwhen we get to GLM models.\\nRemark 2.1.1: An alternative notational viewpoint of the same loss func-\\ntion is also useful, especially for Section 7.1 where we study nonlinear models.\\nLet`logistic :R\\x02f0;1g!R\\x150be the logistic loss de\\x0cned as\\n`logistic (t;y),ylog(1 + exp(\\x00t)) + (1\\x00y) log(1 + exp( t)): (2.3)\\nOne can verify by plugging in h\\x12(x) = 1=(1 +e\\x00\\x12>x) that the negative log-\\nlikelihood (the negation of `(\\x12) in equation (2.1)) can be re-written as\\n\\x00`(\\x12) =`logistic (\\x12>x;y): (2.4)\\nOftentimes \\x12>xortis called the logit. Basic calculus gives us that\\n@`logistic (t;y)\\n@t=y\\x00exp(\\x00t)\\n1 + exp(\\x00t)+ (1\\x00y)1\\n1 + exp(\\x00t)(2.5)\\n= 1=(1 + exp(\\x00t))\\x00y: (2.6)\\nThen, using the chain rule, we have that\\n@\\n@\\x12j`(\\x12) =\\x00@`logistic (t;y)\\n@t\\x01@t\\n@\\x12j(2.7)\\n= (y\\x001=(1 + exp(\\x00t)))\\x01xj= (y\\x00h\\x12(x))xj; (2.8)\\nwhich is consistent with the derivation in equation (2.2). We will see this\\nviewpoint can be extended nonlinear models in Section 7.1.\\n2.2 Digression: the perceptron learning algo-\\nrithm\\nWe now digress to talk brie\\ry about an algorithm that's of some historical\\ninterest, and that we will also return to later when we talk about learning\",\n",
       " '24\\ntheory. Consider modifying the logistic regression method to \\\\force\" it to\\noutput values that are either 0 or 1 or exactly. To do so, it seems natural to\\nchange the de\\x0cnition of gto be the threshold function:\\ng(z) =\\x1a1 ifz\\x150\\n0 ifz <0\\nIf we then let h\\x12(x) =g(\\x12Tx) as before but using this modi\\x0ced de\\x0cnition of\\ng, and if we use the update rule\\n\\x12j:=\\x12j+\\x0b\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\nj:\\nthen we have the perceptron learning algorithn .\\nIn the 1960s, this \\\\perceptron\" was argued to be a rough model for how\\nindividual neurons in the brain work. Given how simple the algorithm is, it\\nwill also provide a starting point for our analysis when we talk about learning\\ntheory later in this class. Note however that even though the perceptron may\\nbe cosmetically similar to the other algorithms we talked about, it is actually\\na very di\\x0berent type of algorithm than logistic regression and least squares\\nlinear regression; in particular, it is di\\x0ecult to endow the perceptron\\'s predic-\\ntions with meaningful probabilistic interpretations, or derive the perceptron\\nas a maximum likelihood estimation algorithm.\\n2.3 Multi-class classi\\x0ccation\\nConsider a classi\\x0ccation problem in which the response variable ycan take on\\nany one ofkvalues, soy2f1;2;:::;kg. For example, rather than classifying\\nemails into the two classes spam or not-spam|which would have been a\\nbinary classi\\x0ccation problem|we might want to classify them into three\\nclasses, such as spam, personal mails, and work-related mails. The label /\\nresponse variable is still discrete, but can now take on more than two values.\\nWe will thus model it as distributed according to a multinomial distribution.\\nIn this case, p(yjx;\\x12) is a distribution over kpossible discrete outcomes\\nand is thus a multinomial distribution. Recall that a multinomial distribu-\\ntion involves knumbers\\x1e1;:::;\\x1ekspecifying the probability of each of the\\noutcomes. Note that these numbers must satisfyPk\\ni=1\\x1ei= 1. We will de-\\nsign a parameterized model that outputs \\x1e1;:::;\\x1eksatisfying this constraint\\ngiven the input x.\\nWe introduce kgroups of parameters \\x121;:::;\\x12k, each of them being a\\nvector in Rd. Intuitively, we would like to use \\x12>\\n1x;:::;\\x12>\\nkxto represent',\n",
       " \"25\\n\\x1e1;:::;\\x1ek, the probabilities P(y= 1jx;\\x12);:::;P (y=kjx;\\x12). However,\\nthere are two issues with such a direct approach. First, \\x12>\\njxis not neces-\\nsarily within [0 ;1]. Second, the summation of \\x12>\\njx's is not necessarily 1.\\nThus, instead, we will use the softmax function to turn ( \\x12>\\n1x;\\x01\\x01\\x01;\\x12>\\nkx) into\\na probability vector with nonnegative entries that sum up to 1.\\nDe\\x0cne the softmax function softmax : Rk!Rkas\\nsoftmax(t1;:::;tk) =2\\n664exp(t1)Pk\\nj=1exp(tj)\\n...\\nexp(tk)Pk\\nj=1exp(tj)3\\n775: (2.9)\\nThe inputs to the softmax function, the vector there, are often called log-\\nits. Note that by de\\x0cnition, the output of the softmax function is always a\\nprobability vector whose entries are nonnegative and sum up to 1.\\nLet (t1;:::;tk) = (\\x12>\\n1x;\\x01\\x01\\x01;\\x12>\\nkx). We apply the softmax function to\\n(t1;:::;tk), and use the output as the probabilities P(y= 1jx;\\x12);:::;P (y=\\nkjx;\\x12). We obtain the following probabilistic model:\\n2\\n64P(y= 1jx;\\x12)\\n...\\nP(y=kjx;\\x12)3\\n75= softmax( t1;\\x01\\x01\\x01;tk) =2\\n6664exp(\\x12>\\n1x)Pk\\nj=1exp(\\x12>\\njx)\\n...\\nexp(\\x12>\\nkx)Pk\\nj=1exp(\\x12>\\njx)3\\n7775: (2.10)\\nFor notational convenience, we will let \\x1ei=exp(ti)Pk\\nj=1exp(tj). More succinctly, the\\nequation above can be written as:\\nP(y=ijx;\\x12) =\\x1ei=exp(ti)Pk\\nj=1exp(tj)=exp(\\x12>\\nix)Pk\\nj=1exp(\\x12>\\njx): (2.11)\\nNext, we compute the negative log-likelihood of a single example ( x;y).\\n\\x00logp(yjx;\\x12) =\\x00log \\nexp(ty)Pk\\nj=1exp(tj)!\\n=\\x00log \\nexp(\\x12>\\nyx)\\nPk\\nj=1exp(\\x12>\\njx)!\\n(2.12)\\nThus, the loss function, the negative log-likelihood of the training data, is\\ngiven as\\n`(\\x12) =nX\\ni=1\\x00log \\nexp(\\x12>\\ny(i)x(i))\\nPk\\nj=1exp(\\x12>\\njx(i))!\\n: (2.13)\",\n",
       " \"26\\nIt's convenient to de\\x0cne the cross-entropy loss `ce:Rk\\x02f1;:::;kg!R\\x150,\\nwhich modularizes in the complex equation above:1\\n`ce((t1;:::;tk);y) =\\x00log \\nexp(ty)Pk\\nj=1exp(tj)!\\n: (2.14)\\nWith this notation, we can simply rewrite equation (2.13) as\\n`(\\x12) =nX\\ni=1`ce((\\x12>\\n1x(i);:::;\\x12>\\nkx(i));y(i)): (2.15)\\nMoreover, conveniently, the cross-entropy loss also has a simple gradient. Let\\nt= (t1;:::;tk), and recall \\x1ei=exp(ti)Pk\\nj=1exp(tj). By basic calculus, we can derive\\n@`ce(t;y)\\n@ti=\\x1ei\\x001fy=ig; (2.16)\\nwhere 1f\\x01gis the indicator function, that is, 1 fy=ig= 1 ify=i, and\\n1fy=ig= 0 ify6=i. Alternatively, in vectorized notations, we have the\\nfollowing form which will be useful for Chapter 7:\\n@`ce(t;y)\\n@t=\\x1e\\x00ey; (2.17)\\nwherees2Rkis thes-th natural basis vector (where the s-th entry is 1 and\\nall other entries are zeros.) Using Chain rule, we have that\\n@`ce((\\x12>\\n1x;:::;\\x12>\\nkx);y)\\n@\\x12i=@`(t;y)\\n@ti\\x01@ti\\n@\\x12i= (\\x1ei\\x001fy=ig)\\x01x: (2.18)\\nTherefore, the gradient of the loss with respect to the part of parameter \\x12iis\\n@`(\\x12)\\n@\\x12i=nX\\nj=1(\\x1e(j)\\ni\\x001fy(j)=ig)\\x01x(j); (2.19)\\nwhere\\x1e(j)\\ni=exp(\\x12>\\nix(j))Pk\\ns=1exp(\\x12>sx(j))is the probability that the model predicts item i\\nfor example x(j). With the gradients above, one can implement (stochastic)\\ngradient descent to minimize the loss function `(\\x12).\\n1There are some ambiguity in the naming here. Some people call the cross-entropy loss\\nthe function that maps the probability vector (the \\x1ein our language) and label yto the\\n\\x0cnal real number, and call our version of cross-entropy loss softmax-cross-entropy loss.\\nWe choose our current naming convention because it's consistent with the naming of most\\nmodern deep learning library such as PyTorch and Jax.\",\n",
       " \"27\\n1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060\\nxf(x)\\n1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060\\nxf(x)\\n1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060\\nxf(x)\\n2.4 Another algorithm for maximizing `(\\x12)\\nReturning to logistic regression with g(z) being the sigmoid function, let's\\nnow talk about a di\\x0berent algorithm for maximizing `(\\x12).\\nTo get us started, let's consider Newton's method for \\x0cnding a zero of a\\nfunction. Speci\\x0ccally, suppose we have some function f:R7!R, and we\\nwish to \\x0cnd a value of \\x12so thatf(\\x12) = 0. Here, \\x122Ris a real number.\\nNewton's method performs the following update:\\n\\x12:=\\x12\\x00f(\\x12)\\nf0(\\x12):\\nThis method has a natural interpretation in which we can think of it as\\napproximating the function fvia a linear function that is tangent to fat\\nthe current guess \\x12, solving for where that linear function equals to zero, and\\nletting the next guess for \\x12be where that linear function is zero.\\nHere's a picture of the Newton's method in action:\\nIn the leftmost \\x0cgure, we see the function fplotted along with the line\\ny= 0. We're trying to \\x0cnd \\x12so thatf(\\x12) = 0; the value of \\x12that achieves this\\nis about 1.3. Suppose we initialized the algorithm with \\x12= 4:5. Newton's\\nmethod then \\x0cts a straight line tangent to fat\\x12= 4:5, and solves for the\\nwhere that line evaluates to 0. (Middle \\x0cgure.) This give us the next guess\\nfor\\x12, which is about 2.8. The rightmost \\x0cgure shows the result of running\\none more iteration, which the updates \\x12to about 1.8. After a few more\\niterations, we rapidly approach \\x12= 1:3.\\nNewton's method gives a way of getting to f(\\x12) = 0. What if we want to\\nuse it to maximize some function `? The maxima of `correspond to points\\nwhere its \\x0crst derivative `0(\\x12) is zero. So, by letting f(\\x12) =`0(\\x12), we can use\\nthe same algorithm to maximize `, and we obtain update rule:\\n\\x12:=\\x12\\x00`0(\\x12)\\n`00(\\x12):\\n(Something to think about: How would this change if we wanted to use\\nNewton's method to minimize rather than maximize a function?)\",\n",
       " \"28\\nLastly, in our logistic regression setting, \\x12is vector-valued, so we need to\\ngeneralize Newton's method to this setting. The generalization of Newton's\\nmethod to this multidimensional setting (also called the Newton-Raphson\\nmethod) is given by\\n\\x12:=\\x12\\x00H\\x001r\\x12`(\\x12):\\nHere,r\\x12`(\\x12) is, as usual, the vector of partial derivatives of `(\\x12) with respect\\nto the\\x12i's; andHis and-by-dmatrix (actually, d+1\\x00by\\x00d+1, assuming that\\nwe include the intercept term) called the Hessian , whose entries are given\\nby\\nHij=@2`(\\x12)\\n@\\x12i@\\x12j:\\nNewton's method typically enjoys faster convergence than (batch) gra-\\ndient descent, and requires many fewer iterations to get very close to the\\nminimum. One iteration of Newton's can, however, be more expensive than\\none iteration of gradient descent, since it requires \\x0cnding and inverting an\\nd-by-dHessian; but so long as dis not too large, it is usually much faster\\noverall. When Newton's method is applied to maximize the logistic regres-\\nsion log likelihood function `(\\x12), the resulting method is also called Fisher\\nscoring .\",\n",
       " \"Chapter 3\\nGeneralized linear models\\nSo far, we've seen a regression example, and a classi\\x0ccation example. In the\\nregression example, we had yjx;\\x12\\x18N (\\x16;\\x1b2), and in the classi\\x0ccation one,\\nyjx;\\x12\\x18Bernoulli(\\x1e), for some appropriate de\\x0cnitions of \\x16and\\x1eas functions\\nofxand\\x12. In this section, we will show that both of these methods are\\nspecial cases of a broader family of models, called Generalized Linear Models\\n(GLMs).1We will also show how other models in the GLM family can be\\nderived and applied to other classi\\x0ccation and regression problems.\\n3.1 The exponential family\\nTo work our way up to GLMs, we will begin by de\\x0cning exponential family\\ndistributions. We say that a class of distributions is in the exponential family\\nif it can be written in the form\\np(y;\\x11) =b(y) exp(\\x11TT(y)\\x00a(\\x11)) (3.1)\\nHere,\\x11is called the natural parameter (also called the canonical param-\\neter) of the distribution; T(y) is the su\\x0ecient statistic (for the distribu-\\ntions we consider, it will often be the case that T(y) =y); anda(\\x11) is the log\\npartition function . The quantity e\\x00a(\\x11)essentially plays the role of a nor-\\nmalization constant, that makes sure the distribution p(y;\\x11) sums/integrates\\noveryto 1.\\nA \\x0cxed choice of T,aandbde\\x0cnes a family (or set) of distributions that\\nis parameterized by \\x11; as we vary \\x11, we then get di\\x0berent distributions within\\nthis family.\\n1The presentation of the material in this section takes inspiration from Michael I.\\nJordan, Learning in graphical models (unpublished book draft), and also McCullagh and\\nNelder, Generalized Linear Models (2nd ed.) .\\n29\",\n",
       " \"30\\nWe now show that the Bernoulli and the Gaussian distributions are ex-\\namples of exponential family distributions. The Bernoulli distribution with\\nmean\\x1e, written Bernoulli( \\x1e), speci\\x0ces a distribution over y2f0;1g, so that\\np(y= 1;\\x1e) =\\x1e;p(y= 0;\\x1e) = 1\\x00\\x1e. As we vary \\x1e, we obtain Bernoulli\\ndistributions with di\\x0berent means. We now show that this class of Bernoulli\\ndistributions, ones obtained by varying \\x1e, is in the exponential family; i.e.,\\nthat there is a choice of T,aandbso that Equation (3.1) becomes exactly\\nthe class of Bernoulli distributions.\\nWe write the Bernoulli distribution as:\\np(y;\\x1e) =\\x1ey(1\\x00\\x1e)1\\x00y\\n= exp(ylog\\x1e+ (1\\x00y) log(1\\x00\\x1e))\\n= exp\\x12\\x12\\nlog\\x12\\x1e\\n1\\x00\\x1e\\x13\\x13\\ny+ log(1\\x00\\x1e)\\x13\\n:\\nThus, the natural parameter is given by \\x11= log(\\x1e=(1\\x00\\x1e)). Interestingly, if\\nwe invert this de\\x0cnition for \\x11by solving for \\x1ein terms of \\x11, we obtain \\x1e=\\n1=(1 +e\\x00\\x11). This is the familiar sigmoid function! This will come up again\\nwhen we derive logistic regression as a GLM. To complete the formulation\\nof the Bernoulli distribution as an exponential family distribution, we also\\nhave\\nT(y) =y\\na(\\x11) =\\x00log(1\\x00\\x1e)\\n= log(1 + e\\x11)\\nb(y) = 1\\nThis shows that the Bernoulli distribution can be written in the form of\\nEquation (3.1), using an appropriate choice of T,aandb.\\nLet's now move on to consider the Gaussian distribution. Recall that,\\nwhen deriving linear regression, the value of \\x1b2had no e\\x0bect on our \\x0cnal\\nchoice of\\x12andh\\x12(x). Thus, we can choose an arbitrary value for \\x1b2without\\nchanging anything. To simplify the derivation below, let's set \\x1b2= 1.2We\\n2If we leave \\x1b2as a variable, the Gaussian distribution can also be shown to be in the\\nexponential family, where \\x112R2is now a 2-dimension vector that depends on both \\x16and\\n\\x1b. For the purposes of GLMs, however, the \\x1b2parameter can also be treated by considering\\na more general de\\x0cnition of the exponential family: p(y;\\x11;\\x1c) =b(a;\\x1c) exp((\\x11TT(y)\\x00\\na(\\x11))=c(\\x1c)). Here,\\x1cis called the dispersion parameter , and for the Gaussian, c(\\x1c) =\\x1b2;\\nbut given our simpli\\x0ccation above, we won't need the more general de\\x0cnition for the\\nexamples we will consider here.\",\n",
       " '31\\nthen have:\\np(y;\\x16) =1p\\n2\\x19exp\\x12\\n\\x001\\n2(y\\x00\\x16)2\\x13\\n=1p\\n2\\x19exp\\x12\\n\\x001\\n2y2\\x13\\n\\x01exp\\x12\\n\\x16y\\x001\\n2\\x162\\x13\\nThus, we see that the Gaussian is in the exponential family, with\\n\\x11=\\x16\\nT(y) =y\\na(\\x11) =\\x162=2\\n=\\x112=2\\nb(y) = (1=p\\n2\\x19) exp(\\x00y2=2):\\nThere\\'re many other distributions that are members of the exponen-\\ntial family: The multinomial (which we\\'ll see later), the Poisson (for mod-\\nelling count-data; also see the problem set); the gamma and the exponen-\\ntial (for modelling continuous, non-negative random variables, such as time-\\nintervals); the beta and the Dirichlet (for distributions over probabilities);\\nand many more. In the next section, we will describe a general \\\\recipe\"\\nfor constructing models in which y(givenxand\\x12) comes from any of these\\ndistributions.\\n3.2 Constructing GLMs\\nSuppose you would like to build a model to estimate the number yof cus-\\ntomers arriving in your store (or number of page-views on your website) in\\nany given hour, based on certain features xsuch as store promotions, recent\\nadvertising, weather, day-of-week, etc. We know that the Poisson distribu-\\ntion usually gives a good model for numbers of visitors. Knowing this, how\\ncan we come up with a model for our problem? Fortunately, the Poisson is an\\nexponential family distribution, so we can apply a Generalized Linear Model\\n(GLM). In this section, we will we will describe a method for constructing\\nGLM models for problems such as these.\\nMore generally, consider a classi\\x0ccation or regression problem where we\\nwould like to predict the value of some random variable yas a function of\\nx. To derive a GLM for this problem, we will make the following three\\nassumptions about the conditional distribution of ygivenxand about our\\nmodel:',\n",
       " '32\\n1.yjx;\\x12\\x18ExponentialFamily( \\x11). I.e., given xand\\x12, the distribution of\\nyfollows some exponential family distribution, with parameter \\x11.\\n2. Givenx, our goal is to predict the expected value of T(y) givenx.\\nIn most of our examples, we will have T(y) =y, so this means we\\nwould like the prediction h(x) output by our learned hypothesis hto\\nsatisfyh(x) = E[yjx]. (Note that this assumption is satis\\x0ced in the\\nchoices for h\\x12(x) for both logistic regression and linear regression. For\\ninstance, in logistic regression, we had h\\x12(x) =p(y= 1jx;\\x12) = 0\\x01p(y=\\n0jx;\\x12) + 1\\x01p(y= 1jx;\\x12) = E[yjx;\\x12].)\\n3. The natural parameter \\x11and the inputs xare related linearly: \\x11=\\x12Tx.\\n(Or, if\\x11is vector-valued, then \\x11i=\\x12T\\nix.)\\nThe third of these assumptions might seem the least well justi\\x0ced of\\nthe above, and it might be better thought of as a \\\\design choice\" in our\\nrecipe for designing GLMs, rather than as an assumption per se. These\\nthree assumptions/design choices will allow us to derive a very elegant class\\nof learning algorithms, namely GLMs, that have many desirable properties\\nsuch as ease of learning. Furthermore, the resulting models are often very\\ne\\x0bective for modelling di\\x0berent types of distributions over y; for example, we\\nwill shortly show that both logistic regression and ordinary least squares can\\nboth be derived as GLMs.\\n3.2.1 Ordinary least squares\\nTo show that ordinary least squares is a special case of the GLM family\\nof models, consider the setting where the target variable y(also called the\\nresponse variable in GLM terminology) is continuous, and we model the\\nconditional distribution of ygivenxas a GaussianN(\\x16;\\x1b2). (Here,\\x16may\\ndependx.) So, we let the ExponentialFamily (\\x11) distribution above be\\nthe Gaussian distribution. As we saw previously, in the formulation of the\\nGaussian as an exponential family distribution, we had \\x16=\\x11. So, we have\\nh\\x12(x) =E[yjx;\\x12]\\n=\\x16\\n=\\x11\\n=\\x12Tx:\\nThe \\x0crst equality follows from Assumption 2, above; the second equality\\nfollows from the fact that yjx;\\x12\\x18N(\\x16;\\x1b2), and so its expected value is given',\n",
       " \"33\\nby\\x16; the third equality follows from Assumption 1 (and our earlier derivation\\nshowing that \\x16=\\x11in the formulation of the Gaussian as an exponential\\nfamily distribution); and the last equality follows from Assumption 3.\\n3.2.2 Logistic regression\\nWe now consider logistic regression. Here we are interested in binary classi\\x0c-\\ncation, soy2f0;1g. Given that yis binary-valued, it therefore seems natural\\nto choose the Bernoulli family of distributions to model the conditional dis-\\ntribution of ygivenx. In our formulation of the Bernoulli distribution as\\nan exponential family distribution, we had \\x1e= 1=(1 +e\\x00\\x11). Furthermore,\\nnote that if yjx;\\x12\\x18Bernoulli(\\x1e), then E[yjx;\\x12] =\\x1e. So, following a similar\\nderivation as the one for ordinary least squares, we get:\\nh\\x12(x) =E[yjx;\\x12]\\n=\\x1e\\n= 1=(1 +e\\x00\\x11)\\n= 1=(1 +e\\x00\\x12Tx)\\nSo, this gives us hypothesis functions of the form h\\x12(x) = 1=(1 +e\\x00\\x12Tx). If\\nyou are previously wondering how we came up with the form of the logistic\\nfunction 1=(1 +e\\x00z), this gives one answer: Once we assume that ycondi-\\ntioned onxis Bernoulli, it arises as a consequence of the de\\x0cnition of GLMs\\nand exponential family distributions.\\nTo introduce a little more terminology, the function ggiving the distri-\\nbution's mean as a function of the natural parameter ( g(\\x11) = E[T(y);\\x11])\\nis called the canonical response function . Its inverse, g\\x001, is called the\\ncanonical link function . Thus, the canonical response function for the\\nGaussian family is just the identify function; and the canonical response\\nfunction for the Bernoulli is the logistic function.3\\n3Many texts use gto denote the link function, and g\\x001to denote the response function;\\nbut the notation we're using here, inherited from the early machine learning literature,\\nwill be more consistent with the notation used in the rest of the class.\",\n",
       " \"Chapter 4\\nGenerative learning algorithms\\nSo far, we've mainly been talking about learning algorithms that model\\np(yjx;\\x12), the conditional distribution of ygivenx. For instance, logistic\\nregression modeled p(yjx;\\x12) ash\\x12(x) =g(\\x12Tx) wheregis the sigmoid func-\\ntion. In these notes, we'll talk about a di\\x0berent type of learning algorithm.\\nConsider a classi\\x0ccation problem in which we want to learn to distinguish\\nbetween elephants ( y= 1) and dogs ( y= 0), based on some features of\\nan animal. Given a training set, an algorithm like logistic regression or\\nthe perceptron algorithm (basically) tries to \\x0cnd a straight line|that is, a\\ndecision boundary|that separates the elephants and dogs. Then, to classify\\na new animal as either an elephant or a dog, it checks on which side of the\\ndecision boundary it falls, and makes its prediction accordingly.\\nHere's a di\\x0berent approach. First, looking at elephants, we can build a\\nmodel of what elephants look like. Then, looking at dogs, we can build a\\nseparate model of what dogs look like. Finally, to classify a new animal, we\\ncan match the new animal against the elephant model, and match it against\\nthe dog model, to see whether the new animal looks more like the elephants\\nor more like the dogs we had seen in the training set.\\nAlgorithms that try to learn p(yjx) directly (such as logistic regression),\\nor algorithms that try to learn mappings directly from the space of inputs X\\nto the labelsf0;1g, (such as the perceptron algorithm) are called discrim-\\ninative learning algorithms. Here, we'll talk about algorithms that instead\\ntry to model p(xjy) (andp(y)). These algorithms are called generative\\nlearning algorithms. For instance, if yindicates whether an example is a\\ndog (0) or an elephant (1), then p(xjy= 0) models the distribution of dogs'\\nfeatures, and p(xjy= 1) models the distribution of elephants' features.\\nAfter modeling p(y) (called the class priors ) andp(xjy), our algorithm\\n34\",\n",
       " '35\\ncan then use Bayes rule to derive the posterior distribution on ygivenx:\\np(yjx) =p(xjy)p(y)\\np(x):\\nHere, the denominator is given by p(x) =p(xjy= 1)p(y= 1) +p(xjy=\\n0)p(y= 0) (you should be able to verify that this is true from the standard\\nproperties of probabilities), and thus can also be expressed in terms of the\\nquantitiesp(xjy) andp(y) that we\\'ve learned. Actually, if were calculating\\np(yjx) in order to make a prediction, then we don\\'t actually need to calculate\\nthe denominator, since\\narg max\\nyp(yjx) = arg max\\nyp(xjy)p(y)\\np(x)\\n= arg max\\nyp(xjy)p(y):\\n4.1 Gaussian discriminant analysis\\nThe \\x0crst generative learning algorithm that we\\'ll look at is Gaussian discrim-\\ninant analysis (GDA). In this model, we\\'ll assume that p(xjy) is distributed\\naccording to a multivariate normal distribution. Let\\'s talk brie\\ry about the\\nproperties of multivariate normal distributions before moving on to the GDA\\nmodel itself.\\n4.1.1 The multivariate normal distribution\\nThe multivariate normal distribution in d-dimensions, also called the multi-\\nvariate Gaussian distribution, is parameterized by a mean vector \\x162Rd\\nand a covariance matrix \\x062Rd\\x02d, where \\x06\\x150 is symmetric and positive\\nsemi-de\\x0cnite. Also written \\\\ N(\\x16;\\x06)\", its density is given by:\\np(x;\\x16;\\x06) =1\\n(2\\x19)d=2j\\x06j1=2exp\\x12\\n\\x001\\n2(x\\x00\\x16)T\\x06\\x001(x\\x00\\x16)\\x13\\n:\\nIn the equation above, \\\\ j\\x06j\" denotes the determinant of the matrix \\x06.\\nFor a random variable XdistributedN(\\x16;\\x06), the mean is (unsurpris-\\ningly) given by \\x16:\\nE[X] =Z\\nxxp(x;\\x16;\\x06)dx=\\x16\\nThecovariance of a vector-valued random variable Zis de\\x0cned as Cov( Z) =\\nE[(Z\\x00E[Z])(Z\\x00E[Z])T]. This generalizes the notion of the variance of a',\n",
       " '36\\nreal-valued random variable. The covariance can also be de\\x0cned as Cov( Z) =\\nE[ZZT]\\x00(E[Z])(E[Z])T. (You should be able to prove to yourself that these\\ntwo de\\x0cnitions are equivalent.) If X\\x18N(\\x16;\\x06), then\\nCov(X) = \\x06:\\nHere are some examples of what the density of a Gaussian distribution\\nlooks like:\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\nThe left-most \\x0cgure shows a Gaussian with mean zero (that is, the 2x1\\nzero-vector) and covariance matrix \\x06 = I(the 2x2 identity matrix). A Gaus-\\nsian with zero mean and identity covariance is also called the standard nor-\\nmal distribution . The middle \\x0cgure shows the density of a Gaussian with\\nzero mean and \\x06 = 0 :6I; and in the rightmost \\x0cgure shows one with , \\x06 = 2 I.\\nWe see that as \\x06 becomes larger, the Gaussian becomes more \\\\spread-out,\"\\nand as it becomes smaller, the distribution becomes more \\\\compressed.\"\\nLet\\'s look at some more examples.\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\nThe \\x0cgures above show Gaussians with mean 0, and with covariance\\nmatrices respectively\\n\\x06 =\\x141 0\\n0 1\\x15\\n; \\x06 =\\x141 0.5\\n0.5 1\\x15\\n; \\x06 =\\x141 0.8\\n0.8 1\\x15\\n:\\nThe leftmost \\x0cgure shows the familiar standard normal distribution, and we\\nsee that as we increase the o\\x0b-diagonal entry in \\x06, the density becomes more',\n",
       " '37\\n\\\\compressed\" towards the 45\\x0eline (given by x1=x2). We can see this more\\nclearly when we look at the contours of the same three densities:\\n−3 −2 −1 0 1 2 3−3−2−10123\\n−3 −2 −1 0 1 2 3−3−2−10123\\n−3 −2 −1 0 1 2 3−3−2−10123\\nHere\\'s one last set of examples generated by varying \\x06:\\n−3 −2 −1 0 1 2 3−3−2−10123\\n−3 −2 −1 0 1 2 3−3−2−10123\\n−3 −2 −1 0 1 2 3−3−2−10123\\nThe plots above used, respectively,\\n\\x06 =\\x141 -0.5\\n-0.5 1\\x15\\n; \\x06 =\\x141 -0.8\\n-0.8 1\\x15\\n; \\x06 =\\x143 0.8\\n0.8 1\\x15\\n:\\nFrom the leftmost and middle \\x0cgures, we see that by decreasing the o\\x0b-\\ndiagonal elements of the covariance matrix, the density now becomes \\\\com-\\npressed\" again, but in the opposite direction. Lastly, as we vary the pa-\\nrameters, more generally the contours will form ellipses (the rightmost \\x0cgure\\nshowing an example).\\nAs our last set of examples, \\x0cxing \\x06 = I, by varying \\x16, we can also move\\nthe mean of the density around.\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25\\n−3−2−10123\\n−3−2−101230.050.10.150.20.25',\n",
       " \"38\\nThe \\x0cgures above were generated using \\x06 = I, and respectively\\n\\x16=\\x141\\n0\\x15\\n;\\x16=\\x14-0.5\\n0\\x15\\n;\\x16=\\x14-1\\n-1.5\\x15\\n:\\n4.1.2 The Gaussian discriminant analysis model\\nWhen we have a classi\\x0ccation problem in which the input features xare\\ncontinuous-valued random variables, we can then use the Gaussian Discrim-\\ninant Analysis (GDA) model, which models p(xjy) using a multivariate nor-\\nmal distribution. The model is:\\ny\\x18Bernoulli(\\x1e)\\nxjy= 0\\x18 N (\\x160;\\x06)\\nxjy= 1\\x18 N (\\x161;\\x06)\\nWriting out the distributions, this is:\\np(y) =\\x1ey(1\\x00\\x1e)1\\x00y\\np(xjy= 0) =1\\n(2\\x19)d=2j\\x06j1=2exp\\x12\\n\\x001\\n2(x\\x00\\x160)T\\x06\\x001(x\\x00\\x160)\\x13\\np(xjy= 1) =1\\n(2\\x19)d=2j\\x06j1=2exp\\x12\\n\\x001\\n2(x\\x00\\x161)T\\x06\\x001(x\\x00\\x161)\\x13\\nHere, the parameters of our model are \\x1e, \\x06,\\x160and\\x161. (Note that while\\nthere're two di\\x0berent mean vectors \\x160and\\x161, this model is usually applied\\nusing only one covariance matrix \\x06.) The log-likelihood of the data is given\\nby\\n`(\\x1e;\\x16 0;\\x161;\\x06) = lognY\\ni=1p(x(i);y(i);\\x1e;\\x16 0;\\x161;\\x06)\\n= lognY\\ni=1p(x(i)jy(i);\\x160;\\x161;\\x06)p(y(i);\\x1e):\",\n",
       " \"39\\nBy maximizing `with respect to the parameters, we \\x0cnd the maximum like-\\nlihood estimate of the parameters (see problem set 1) to be:\\n\\x1e=1\\nnnX\\ni=11fy(i)= 1g\\n\\x160=Pn\\ni=11fy(i)= 0gx(i)\\nPn\\ni=11fy(i)= 0g\\n\\x161=Pn\\ni=11fy(i)= 1gx(i)\\nPn\\ni=11fy(i)= 1g\\n\\x06 =1\\nnnX\\ni=1(x(i)\\x00\\x16y(i))(x(i)\\x00\\x16y(i))T:\\nPictorially, what the algorithm is doing can be seen in as follows:\\n−2 −1 0 1 2 3 4 5 6 7−7−6−5−4−3−2−101\\nShown in the \\x0cgure are the training set, as well as the contours of the\\ntwo Gaussian distributions that have been \\x0ct to the data in each of the\\ntwo classes. Note that the two Gaussians have contours that are the same\\nshape and orientation, since they share a covariance matrix \\x06, but they have\\ndi\\x0berent means \\x160and\\x161. Also shown in the \\x0cgure is the straight line\\ngiving the decision boundary at which p(y= 1jx) = 0:5. On one side of\\nthe boundary, we'll predict y= 1 to be the most likely outcome, and on the\\nother side, we'll predict y= 0.\",\n",
       " '40\\n4.1.3 Discussion: GDA and logistic regression\\nThe GDA model has an interesting relationship to logistic regression. If we\\nview the quantity p(y= 1jx;\\x1e;\\x16 0;\\x161;\\x06) as a function of x, we\\'ll \\x0cnd that it\\ncan be expressed in the form\\np(y= 1jx;\\x1e;\\x06;\\x160;\\x161) =1\\n1 + exp(\\x00\\x12Tx);\\nwhere\\x12is some appropriate function of \\x1e;\\x06;\\x160;\\x161.1This is exactly the form\\nthat logistic regression|a discriminative algorithm|used to model p(y=\\n1jx).\\nWhen would we prefer one model over another? GDA and logistic regres-\\nsion will, in general, give di\\x0berent decision boundaries when trained on the\\nsame dataset. Which is better?\\nWe just argued that if p(xjy) is multivariate gaussian (with shared \\x06),\\nthenp(yjx) necessarily follows a logistic function. The converse, however,\\nis not true; i.e., p(yjx) being a logistic function does not imply p(xjy) is\\nmultivariate gaussian. This shows that GDA makes stronger modeling as-\\nsumptions about the data than does logistic regression. It turns out that\\nwhen these modeling assumptions are correct, then GDA will \\x0cnd better \\x0cts\\nto the data, and is a better model. Speci\\x0ccally, when p(xjy) is indeed gaus-\\nsian (with shared \\x06), then GDA is asymptotically e\\x0ecient . Informally,\\nthis means that in the limit of very large training sets (large n), there is no\\nalgorithm that is strictly better than GDA (in terms of, say, how accurately\\nthey estimate p(yjx)). In particular, it can be shown that in this setting,\\nGDA will be a better algorithm than logistic regression; and more generally,\\neven for small training set sizes, we would generally expect GDA to better.\\nIn contrast, by making signi\\x0ccantly weaker assumptions, logistic regres-\\nsion is also more robust and less sensitive to incorrect modeling assumptions.\\nThere are many di\\x0berent sets of assumptions that would lead to p(yjx) taking\\nthe form of a logistic function. For example, if xjy= 0\\x18Poisson(\\x150), and\\nxjy= 1\\x18Poisson(\\x151), thenp(yjx) will be logistic. Logistic regression will\\nalso work well on Poisson data like this. But if we were to use GDA on such\\ndata|and \\x0ct Gaussian distributions to such non-Gaussian data|then the\\nresults will be less predictable, and GDA may (or may not) do well.\\nTo summarize: GDA makes stronger modeling assumptions, and is more\\ndata e\\x0ecient (i.e., requires less training data to learn \\\\well\") when the mod-\\neling assumptions are correct or at least approximately correct. Logistic\\n1This uses the convention of rede\\x0cning the x(i)\\'s on the right-hand-side to be ( d+ 1)-\\ndimensional vectors by adding the extra coordinate x(i)\\n0= 1; see problem set 1.',\n",
       " '41\\nregression makes weaker assumptions, and is signi\\x0ccantly more robust to\\ndeviations from modeling assumptions. Speci\\x0ccally, when the data is in-\\ndeed non-Gaussian, then in the limit of large datasets, logistic regression will\\nalmost always do better than GDA. For this reason, in practice logistic re-\\ngression is used more often than GDA. (Some related considerations about\\ndiscriminative vs. generative models also apply for the Naive Bayes algo-\\nrithm that we discuss next, but the Naive Bayes algorithm is still considered\\na very good, and is certainly also a very popular, classi\\x0ccation algorithm.)\\n4.2 Naive bayes (Option Reading)\\nIn GDA, the feature vectors xwere continuous, real-valued vectors. Let\\'s\\nnow talk about a di\\x0berent learning algorithm in which the xj\\'s are discrete-\\nvalued.\\nFor our motivating example, consider building an email spam \\x0clter using\\nmachine learning. Here, we wish to classify messages according to whether\\nthey are unsolicited commercial (spam) email, or non-spam email. After\\nlearning to do this, we can then have our mail reader automatically \\x0clter\\nout the spam messages and perhaps place them in a separate mail folder.\\nClassifying emails is one example of a broader set of problems called text\\nclassi\\x0ccation .\\nLet\\'s say we have a training set (a set of emails labeled as spam or non-\\nspam). We\\'ll begin our construction of our spam \\x0clter by specifying the\\nfeaturesxjused to represent an email.\\nWe will represent an email via a feature vector whose length is equal to\\nthe number of words in the dictionary. Speci\\x0ccally, if an email contains the\\nj-th word of the dictionary, then we will set xj= 1; otherwise, we let xj= 0.\\nFor instance, the vector\\nx=2\\n66666666641\\n0\\n0\\n...\\n1\\n...\\n03\\n7777777775a\\naardvark\\naardwolf\\n...\\nbuy\\n...\\nzygmurgy\\nis used to represent an email that contains the words \\\\a\" and \\\\buy,\" but not',\n",
       " '42\\n\\\\aardvark,\" \\\\aardwolf\" or \\\\zygmurgy.\"2The set of words encoded into the\\nfeature vector is called the vocabulary , so the dimension of xis equal to\\nthe size of the vocabulary.\\nHaving chosen our feature vector, we now want to build a generative\\nmodel. So, we have to model p(xjy). But if we have, say, a vocabulary of\\n50000 words, then x2f0;1g50000(xis a 50000-dimensional vector of 0\\'s and\\n1\\'s), and if we were to model xexplicitly with a multinomial distribution over\\nthe 250000possible outcomes, then we\\'d end up with a (250000\\x001)-dimensional\\nparameter vector. This is clearly too many parameters.\\nTo modelp(xjy), we will therefore make a very strong assumption. We will\\nassume that the xi\\'s are conditionally independent given y. This assumption\\nis called the Naive Bayes (NB) assumption , and the resulting algorithm is\\ncalled the Naive Bayes classi\\x0cer . For instance, if y= 1 means spam email;\\n\\\\buy\" is word 2087 and \\\\price\" is word 39831; then we are assuming that if\\nI tell youy= 1 (that a particular piece of email is spam), then knowledge\\nofx2087(knowledge of whether \\\\buy\" appears in the message) will have no\\ne\\x0bect on your beliefs about the value of x39831 (whether \\\\price\" appears).\\nMore formally, this can be written p(x2087jy) =p(x2087jy;x 39831). (Note that\\nthis is notthe same as saying that x2087andx39831 are independent, which\\nwould have been written \\\\ p(x2087) =p(x2087jx39831)\"; rather, we are only\\nassuming that x2087andx39831 are conditionally independent giveny.)\\nWe now have:\\np(x1;:::;x 50000jy)\\n=p(x1jy)p(x2jy;x 1)p(x3jy;x 1;x2)\\x01\\x01\\x01p(x50000jy;x 1;:::;x 49999)\\n=p(x1jy)p(x2jy)p(x3jy)\\x01\\x01\\x01p(x50000jy)\\n=dY\\nj=1p(xjjy)\\nThe \\x0crst equality simply follows from the usual properties of probabilities,\\nand the second equality used the NB assumption. We note that even though\\n2Actually, rather than looking through an English dictionary for the list of all English\\nwords, in practice it is more common to look through our training set and encode in our\\nfeature vector only the words that occur at least once there. Apart from reducing the\\nnumber of words modeled and hence reducing our computational and space requirements,\\nthis also has the advantage of allowing us to model/include as a feature many words\\nthat may appear in your email (such as \\\\cs229\") but that you won\\'t \\x0cnd in a dictionary.\\nSometimes (as in the homework), we also exclude the very high frequency words (which\\nwill be words like \\\\the,\" \\\\of,\" \\\\and\"; these high frequency, \\\\content free\" words are called\\nstop words ) since they occur in so many documents and do little to indicate whether an\\nemail is spam or non-spam.',\n",
       " '43\\nthe Naive Bayes assumption is an extremely strong assumptions, the resulting\\nalgorithm works well on many problems.\\nOur model is parameterized by \\x1ejjy=1=p(xj= 1jy= 1),\\x1ejjy=0=p(xj=\\n1jy= 0), and\\x1ey=p(y= 1). As usual, given a training set f(x(i);y(i));i=\\n1;:::;ng, we can write down the joint likelihood of the data:\\nL(\\x1ey;\\x1ejjy=0;\\x1ejjy=1) =nY\\ni=1p(x(i);y(i)):\\nMaximizing this with respect to \\x1ey;\\x1ejjy=0and\\x1ejjy=1gives the maximum\\nlikelihood estimates:\\n\\x1ejjy=1=Pn\\ni=11fx(i)\\nj= 1^y(i)= 1gPn\\ni=11fy(i)= 1g\\n\\x1ejjy=0=Pn\\ni=11fx(i)\\nj= 1^y(i)= 0gPn\\ni=11fy(i)= 0g\\n\\x1ey=Pn\\ni=11fy(i)= 1g\\nn\\nIn the equations above, the \\\\ ^\" symbol means \\\\and.\" The parameters have\\na very natural interpretation. For instance, \\x1ejjy=1is just the fraction of the\\nspam (y= 1) emails in which word jdoes appear.\\nHaving \\x0ct all these parameters, to make a prediction on a new example\\nwith features x, we then simply calculate\\np(y= 1jx) =p(xjy= 1)p(y= 1)\\np(x)\\n=\\x10Qd\\nj=1p(xjjy= 1)\\x11\\np(y= 1)\\n\\x10Qd\\nj=1p(xjjy= 1)\\x11\\np(y= 1) +\\x10Qd\\nj=1p(xjjy= 0)\\x11\\np(y= 0);\\nand pick whichever class has the higher posterior probability.\\nLastly, we note that while we have developed the Naive Bayes algorithm\\nmainly for the case of problems where the features xjare binary-valued, the\\ngeneralization to where xjcan take values in f1;2;:::;kjgis straightforward.\\nHere, we would simply model p(xjjy) as multinomial rather than as Bernoulli.\\nIndeed, even if some original input attribute (say, the living area of a house,\\nas in our earlier example) were continuous valued, it is quite common to\\ndiscretize it|that is, turn it into a small set of discrete values|and apply\\nNaive Bayes. For instance, if we use some feature xjto represent living area,\\nwe might discretize the continuous values as follows:',\n",
       " '44\\nLiving area (sq. feet) <400 400-800 800-1200 1200-1600 >1600\\nxi 1 2 3 4 5\\nThus, for a house with living area 890 square feet, we would set the value\\nof the corresponding feature xjto 3. We can then apply the Naive Bayes\\nalgorithm, and model p(xjjy) with a multinomial distribution, as described\\npreviously. When the original, continuous-valued attributes are not well-\\nmodeled by a multivariate normal distribution, discretizing the features and\\nusing Naive Bayes (instead of GDA) will often result in a better classi\\x0cer.\\n4.2.1 Laplace smoothing\\nThe Naive Bayes algorithm as we have described it will work fairly well\\nfor many problems, but there is a simple change that makes it work much\\nbetter, especially for text classi\\x0ccation. Let\\'s brie\\ry discuss a problem with\\nthe algorithm in its current form, and then talk about how we can \\x0cx it.\\nConsider spam/email classi\\x0ccation, and let\\'s suppose that, we are in the\\nyear of 20xx, after completing CS229 and having done excellent work on the\\nproject, you decide around May 20xx to submit work you did to the NeurIPS\\nconference for publication.3Because you end up discussing the conference\\nin your emails, you also start getting messages with the word \\\\neurips\"\\nin it. But this is your \\x0crst NeurIPS paper, and until this time, you had\\nnot previously seen any emails containing the word \\\\neurips\"; in particular\\n\\\\neurips\" did not ever appear in your training set of spam/non-spam emails.\\nAssuming that \\\\neurips\" was the 35000th word in the dictionary, your Naive\\nBayes spam \\x0clter therefore had picked its maximum likelihood estimates of\\nthe parameters \\x1e35000jyto be\\n\\x1e35000jy=1=Pn\\ni=11fx(i)\\n35000 = 1^y(i)= 1gPn\\ni=11fy(i)= 1g= 0\\n\\x1e35000jy=0=Pn\\ni=11fx(i)\\n35000 = 1^y(i)= 0gPn\\ni=11fy(i)= 0g= 0\\nI.e., because it has never seen \\\\neurips\" before in either spam or non-spam\\ntraining examples, it thinks the probability of seeing it in either type of email\\nis zero. Hence, when trying to decide if one of these messages containing\\n3NeurIPS is one of the top machine learning conferences. The deadline for submitting\\na paper is typically in May-June.',\n",
       " '45\\n\\\\neurips\" is spam, it calculates the class posterior probabilities, and obtains\\np(y= 1jx) =Qd\\nj=1p(xjjy= 1)p(y= 1)\\nQd\\nj=1p(xjjy= 1)p(y= 1) +Qd\\nj=1p(xjjy= 0)p(y= 0)\\n=0\\n0:\\nThis is because each of the terms \\\\Qd\\nj=1p(xjjy)\" includes a term p(x35000jy) =\\n0 that is multiplied into it. Hence, our algorithm obtains 0 =0, and doesn\\'t\\nknow how to make a prediction.\\nStating the problem more broadly, it is statistically a bad idea to esti-\\nmate the probability of some event to be zero just because you haven\\'t seen\\nit before in your \\x0cnite training set. Take the problem of estimating the mean\\nof a multinomial random variable ztaking values inf1;:::;kg. We can pa-\\nrameterize our multinomial with \\x1ej=p(z=j). Given a set of nindependent\\nobservationsfz(1);:::;z(n)g, the maximum likelihood estimates are given by\\n\\x1ej=Pn\\ni=11fz(i)=jg\\nn:\\nAs we saw previously, if we were to use these maximum likelihood estimates,\\nthen some of the \\x1ej\\'s might end up as zero, which was a problem. To avoid\\nthis, we can use Laplace smoothing , which replaces the above estimate\\nwith\\n\\x1ej=1 +Pn\\ni=11fz(i)=jg\\nk+n:\\nHere, we\\'ve added 1 to the numerator, and kto the denominator. Note thatPk\\nj=1\\x1ej= 1 still holds (check this yourself!), which is a desirable property\\nsince the\\x1ej\\'s are estimates for probabilities that we know must sum to 1.\\nAlso,\\x1ej6= 0 for all values of j, solving our problem of probabilities being\\nestimated as zero. Under certain (arguably quite strong) conditions, it can\\nbe shown that the Laplace smoothing actually gives the optimal estimator\\nof the\\x1ej\\'s.\\nReturning to our Naive Bayes classi\\x0cer, with Laplace smoothing, we\\ntherefore obtain the following estimates of the parameters:\\n\\x1ejjy=1=1 +Pn\\ni=11fx(i)\\nj= 1^y(i)= 1g\\n2 +Pn\\ni=11fy(i)= 1g\\n\\x1ejjy=0=1 +Pn\\ni=11fx(i)\\nj= 1^y(i)= 0g\\n2 +Pn\\ni=11fy(i)= 0g',\n",
       " '46\\n(In practice, it usually doesn\\'t matter much whether we apply Laplace smooth-\\ning to\\x1eyor not, since we will typically have a fair fraction each of spam and\\nnon-spam messages, so \\x1eywill be a reasonable estimate of p(y= 1) and will\\nbe quite far from 0 anyway.)\\n4.2.2 Event models for text classi\\x0ccation\\nTo close o\\x0b our discussion of generative learning algorithms, let\\'s talk about\\none more model that is speci\\x0ccally for text classi\\x0ccation. While Naive Bayes\\nas we\\'ve presented it will work well for many classi\\x0ccation problems, for text\\nclassi\\x0ccation, there is a related model that does even better.\\nIn the speci\\x0cc context of text classi\\x0ccation, Naive Bayes as presented uses\\nthe what\\'s called the Bernoulli event model (or sometimes multi-variate\\nBernoulli event model ). In this model, we assumed that the way an email\\nis generated is that \\x0crst it is randomly determined (according to the class\\npriorsp(y)) whether a spammer or non-spammer will send you your next\\nmessage. Then, the person sending the email runs through the dictionary,\\ndeciding whether to include each word jin that email independently and\\naccording to the probabilities p(xj= 1jy) =\\x1ejjy. Thus, the probability of a\\nmessage was given by p(y)Qd\\nj=1p(xjjy).\\nHere\\'s a di\\x0berent model, called the Multinomial event model . To\\ndescribe this model, we will use a di\\x0berent notation and set of features for\\nrepresenting emails. We let xjdenote the identity of the j-th word in the\\nemail. Thus, xjis now an integer taking values in f1;:::;jVjg, wherejVj\\nis the size of our vocabulary (dictionary). An email of dwords is now rep-\\nresented by a vector ( x1;x2;:::;xd) of length d; note that dcan vary for\\ndi\\x0berent documents. For instance, if an email starts with \\\\A NeurIPS . . . ,\"\\nthenx1= 1 (\\\\a\" is the \\x0crst word in the dictionary), and x2= 35000 (if\\n\\\\neurips\" is the 35000th word in the dictionary).\\nIn the multinomial event model, we assume that the way an email is\\ngenerated is via a random process in which spam/non-spam is \\x0crst deter-\\nmined (according to p(y)) as before. Then, the sender of the email writes the\\nemail by \\x0crst generating x1from some multinomial distribution over words\\n(p(x1jy)). Next, the second word x2is chosen independently of x1but from\\nthe same multinomial distribution, and similarly for x3,x4, and so on, until\\nalldwords of the email have been generated. Thus, the overall probability of\\na message is given by p(y)Qd\\nj=1p(xjjy). Note that this formula looks like the\\none we had earlier for the probability of a message under the Bernoulli event\\nmodel, but that the terms in the formula now mean very di\\x0berent things. In\\nparticularxjjyis now a multinomial, rather than a Bernoulli distribution.',\n",
       " '47\\nThe parameters for our new model are \\x1ey=p(y) as before, \\x1ekjy=1=\\np(xj=kjy= 1) (for any j) and\\x1ekjy=0=p(xj=kjy= 0). Note that we have\\nassumed that p(xjjy) is the same for all values of j(i.e., that the distribution\\naccording to which a word is generated does not depend on its position j\\nwithin the email).\\nIf we are given a training set f(x(i);y(i));i= 1;:::;ngwherex(i)=\\n(x(i)\\n1;x(i)\\n2;:::;x(i)\\ndi) (here,diis the number of words in the i-training example),\\nthe likelihood of the data is given by\\nL(\\x1ey;\\x1ekjy=0;\\x1ekjy=1) =nY\\ni=1p(x(i);y(i))\\n=nY\\ni=1 diY\\nj=1p(x(i)\\njjy;\\x1ekjy=0;\\x1ekjy=1)!\\np(y(i);\\x1ey):\\nMaximizing this yields the maximum likelihood estimates of the parameters:\\n\\x1ekjy=1=Pn\\ni=1Pdi\\nj=11fx(i)\\nj=k^y(i)= 1gPn\\ni=11fy(i)= 1gdi\\n\\x1ekjy=0=Pn\\ni=1Pdi\\nj=11fx(i)\\nj=k^y(i)= 0gPn\\ni=11fy(i)= 0gdi\\n\\x1ey=Pn\\ni=11fy(i)= 1g\\nn:\\nIf we were to apply Laplace smoothing (which is needed in practice for good\\nperformance) when estimating \\x1ekjy=0and\\x1ekjy=1, we add 1 to the numerators\\nandjVjto the denominators, and obtain:\\n\\x1ekjy=1=1 +Pn\\ni=1Pdi\\nj=11fx(i)\\nj=k^y(i)= 1g\\njVj+Pn\\ni=11fy(i)= 1gdi\\n\\x1ekjy=0=1 +Pn\\ni=1Pdi\\nj=11fx(i)\\nj=k^y(i)= 0g\\njVj+Pn\\ni=11fy(i)= 0gdi:\\nWhile not necessarily the very best classi\\x0ccation algorithm, the Naive Bayes\\nclassi\\x0cer often works surprisingly well. It is often also a very good \\\\\\x0crst thing\\nto try,\" given its simplicity and ease of implementation.',\n",
       " 'Chapter 5\\nKernel methods\\n5.1 Feature maps\\nRecall that in our discussion about linear regression, we considered the prob-\\nlem of predicting the price of a house (denoted by y) from the living area of\\nthe house (denoted by x), and we \\x0ct a linear function of xto the training\\ndata. What if the price ycan be more accurately represented as a non-linear\\nfunction of x? In this case, we need a more expressive family of models than\\nlinear models.\\nWe start by considering \\x0ctting cubic functions y=\\x123x3+\\x122x2+\\x121x+\\x120.\\nIt turns out that we can view the cubic function as a linear function over\\nthe a di\\x0berent set of feature variables (de\\x0cned below). Concretely, let the\\nfunction\\x1e:R!R4be de\\x0cned as\\n\\x1e(x) =2\\n6641\\nx\\nx2\\nx33\\n7752R4: (5.1)\\nLet\\x122R4be the vector containing \\x120;\\x121;\\x122;\\x123as entries. Then we can\\nrewrite the cubic function in xas:\\n\\x123x3+\\x122x2+\\x121x+\\x120=\\x12T\\x1e(x)\\nThus, a cubic function of the variable xcan be viewed as a linear function\\nover the variables \\x1e(x). To distinguish between these two sets of variables,\\nin the context of kernel methods, we will call the \\\\original\" input value the\\ninput attributes of a problem (in this case, x, the living area). When the\\n48',\n",
       " '49\\noriginal input is mapped to some new set of quantities \\x1e(x), we will call those\\nnew quantities the features variables. (Unfortunately, di\\x0berent authors use\\ndi\\x0berent terms to describe these two things in di\\x0berent contexts.) We will\\ncall\\x1eafeature map , which maps the attributes to the features.\\n5.2 LMS (least mean squares) with features\\nWe will derive the gradient descent algorithm for \\x0ctting the model \\x12T\\x1e(x).\\nFirst recall that for ordinary least square problem where we were to \\x0ct \\x12Tx,\\nthe batch gradient descent update is (see the \\x0crst lecture note for its deriva-\\ntion):\\n\\x12:=\\x12+\\x0bnX\\ni=1\\x00\\ny(i)\\x00h\\x12(x(i))\\x01\\nx(i)\\n:=\\x12+\\x0bnX\\ni=1\\x00\\ny(i)\\x00\\x12Tx(i)\\x01\\nx(i): (5.2)\\nLet\\x1e:Rd!Rpbe a feature map that maps attribute x(inRd) to the\\nfeatures\\x1e(x) inRp. (In the motivating example in the previous subsection,\\nwe haved= 1 andp= 4.) Now our goal is to \\x0ct the function \\x12T\\x1e(x), with\\n\\x12being a vector in Rpinstead of Rd. We can replace all the occurrences of\\nx(i)in the algorithm above by \\x1e(x(i)) to obtain the new update:\\n\\x12:=\\x12+\\x0bnX\\ni=1\\x00\\ny(i)\\x00\\x12T\\x1e(x(i))\\x01\\n\\x1e(x(i)) (5.3)\\nSimilarly, the corresponding stochastic gradient descent update rule is\\n\\x12:=\\x12+\\x0b\\x00\\ny(i)\\x00\\x12T\\x1e(x(i))\\x01\\n\\x1e(x(i)) (5.4)\\n5.3 LMS with the kernel trick\\nThe gradient descent update, or stochastic gradient update above becomes\\ncomputationally expensive when the features \\x1e(x) is high-dimensional. For\\nexample, consider the direct extension of the feature map in equation (5.1)\\nto high-dimensional input x: supposex2Rd, and let\\x1e(x) be the vector that',\n",
       " '50\\ncontains all the monomials of xwith degree\\x143\\n\\x1e(x) =2\\n6666666666666666666666641\\nx1\\nx2\\n...\\nx2\\n1\\nx1x2\\nx1x3\\n...\\nx2x1\\n...\\nx3\\n1\\nx2\\n1x2\\n...3\\n777777777777777777777775: (5.5)\\nThe dimension of the features \\x1e(x) is on the order of d3.1This is a pro-\\nhibitively long vector for computational purpose | when d= 1000, each\\nupdate requires at least computing and storing a 10003= 109dimensional\\nvector, which is 106times slower than the update rule for for ordinary least\\nsquares updates (5.2).\\nIt may appear at \\x0crst that such d3runtime per update and memory usage\\nare inevitable, because the vector \\x12itself is of dimension p\\x19d3, and we may\\nneed to update every entry of \\x12and store it. However, we will introduce the\\nkernel trick with which we will not need to store \\x12explicitly, and the runtime\\ncan be signi\\x0ccantly improved.\\nFor simplicity, we assume the initialize the value \\x12= 0, and we focus\\non the iterative update (5.3). The main observation is that at any time, \\x12\\ncan be represented as a linear combination of the vectors \\x1e(x(1));:::;\\x1e (x(n)).\\nIndeed, we can show this inductively as follows. At initialization, \\x12= 0 =Pn\\ni=10\\x01\\x1e(x(i)). Assume at some point, \\x12can be represented as\\n\\x12=nX\\ni=1\\x0ci\\x1e(x(i)) (5.6)\\n1Here, for simplicity, we include all the monomials with repetitions (so that, e.g., x1x2x3\\nandx2x3x1both appear in \\x1e(x)). Therefore, there are totally 1 + d+d2+d3entries in\\n\\x1e(x).',\n",
       " \"51\\nfor some\\x0c1;:::;\\x0cn2R. Then we claim that in the next round, \\x12is still a\\nlinear combination of \\x1e(x(1));:::;\\x1e (x(n)) because\\n\\x12:=\\x12+\\x0bnX\\ni=1\\x00\\ny(i)\\x00\\x12T\\x1e(x(i))\\x01\\n\\x1e(x(i))\\n=nX\\ni=1\\x0ci\\x1e(x(i)) +\\x0bnX\\ni=1\\x00\\ny(i)\\x00\\x12T\\x1e(x(i))\\x01\\n\\x1e(x(i))\\n=nX\\ni=1(\\x0ci+\\x0b\\x00\\ny(i)\\x00\\x12T\\x1e(x(i))\\x01\\n)|{z}\\nnew\\x0ci\\x1e(x(i)) (5.7)\\nYou may realize that our general strategy is to implicitly represent the p-\\ndimensional vector \\x12by a set of coe\\x0ecients \\x0c1;:::;\\x0cn. Towards doing this,\\nwe derive the update rule of the coe\\x0ecients \\x0c1;:::;\\x0cn. Using the equation\\nabove, we see that the new \\x0cidepends on the old one via\\n\\x0ci:=\\x0ci+\\x0b\\x00\\ny(i)\\x00\\x12T\\x1e(x(i))\\x01\\n(5.8)\\nHere we still have the old \\x12on the RHS of the equation. Replacing \\x12by\\n\\x12=Pn\\nj=1\\x0cj\\x1e(x(j)) gives\\n8i2f1;:::;ng;\\x0ci:=\\x0ci+\\x0b \\ny(i)\\x00nX\\nj=1\\x0cj\\x1e(x(j))T\\x1e(x(i))!\\nWe often rewrite \\x1e(x(j))T\\x1e(x(i)) ash\\x1e(x(j));\\x1e(x(i))ito emphasize that it's the\\ninner product of the two feature vectors. Viewing \\x0ci's as the new representa-\\ntion of\\x12, we have successfully translated the batch gradient descent algorithm\\ninto an algorithm that updates the value of \\x0citeratively. It may appear that\\nat every iteration, we still need to compute the values of h\\x1e(x(j));\\x1e(x(i))ifor\\nall pairs of i;j, each of which may take roughly O(p) operation. However,\\ntwo important properties come to rescue:\\n1. We can pre-compute the pairwise inner products h\\x1e(x(j));\\x1e(x(i))ifor all\\npairs ofi;jbefore the loop starts.\\n2. For the feature map \\x1ede\\x0cned in (5.5) (or many other interesting fea-\\nture maps), computing h\\x1e(x(j));\\x1e(x(i))ican be e\\x0ecient and does not\",\n",
       " '52\\nnecessarily require computing \\x1e(x(i)) explicitly. This is because:\\nh\\x1e(x);\\x1e(z)i= 1 +dX\\ni=1xizi+X\\ni;j2f1;:::;dgxixjzizj+X\\ni;j;k2f1;:::;dgxixjxkzizjzk\\n= 1 +dX\\ni=1xizi+ dX\\ni=1xizi!2\\n+ dX\\ni=1xizi!3\\n= 1 +hx;zi+hx;zi2+hx;zi3(5.9)\\nTherefore, to compute h\\x1e(x);\\x1e(z)i, we can \\x0crst compute hx;ziwith\\nO(d) time and then take another constant number of operations to com-\\npute 1 +hx;zi+hx;zi2+hx;zi3.\\nAs you will see, the inner products between the features h\\x1e(x);\\x1e(z)iare\\nessential here. We de\\x0cne the Kernel corresponding to the feature map \\x1eas\\na function that maps X\\x02X! Rsatisfying:2\\nK(x;z),h\\x1e(x);\\x1e(z)i (5.10)\\nTo wrap up the discussion, we write the down the \\x0cnal algorithm as\\nfollows:\\n1. Compute all the values K(x(i);x(j)),h\\x1e(x(i));\\x1e(x(j))iusing equa-\\ntion (5.9) for all i;j2f1;:::;ng. Set\\x0c:= 0.\\n2.Loop:\\n8i2f1;:::;ng;\\x0ci:=\\x0ci+\\x0b \\ny(i)\\x00nX\\nj=1\\x0cjK(x(i);x(j))!\\n(5.11)\\nOr in vector notation, letting Kbe then\\x02nmatrix with Kij=\\nK(x(i);x(j)), we have\\n\\x0c:=\\x0c+\\x0b(~ y\\x00K\\x0c)\\nWith the algorithm above, we can update the representation \\x0cof the\\nvector\\x12e\\x0eciently with O(n) time per update. Finally, we need to show that\\n2Recall thatXis the space of the input x. In our running example, X=Rd',\n",
       " \"53\\nthe knowledge of the representation \\x0csu\\x0eces to compute the prediction\\n\\x12T\\x1e(x). Indeed, we have\\n\\x12T\\x1e(x) =nX\\ni=1\\x0ci\\x1e(x(i))T\\x1e(x) =nX\\ni=1\\x0ciK(x(i);x) (5.12)\\nYou may realize that fundamentally all we need to know about the feature\\nmap\\x1e(\\x01) is encapsulated in the corresponding kernel function K(\\x01;\\x01). We\\nwill expand on this in the next section.\\n5.4 Properties of kernels\\nIn the last subsection, we started with an explicitly de\\x0cned feature map \\x1e,\\nwhich induces the kernel function K(x;z),h\\x1e(x);\\x1e(z)i. Then we saw that\\nthe kernel function is so intrinsic so that as long as the kernel function is\\nde\\x0cned, the whole training algorithm can be written entirely in the language\\nof the kernel without referring to the feature map \\x1e, so can the prediction of\\na test example x(equation (5.12).)\\nTherefore, it would be tempted to de\\x0cne other kernel function K(\\x01;\\x01) and\\nrun the algorithm (5.11). Note that the algorithm (5.11) does not need to\\nexplicitly access the feature map \\x1e, and therefore we only need to ensure the\\nexistence of the feature map \\x1e, but do not necessarily need to be able to\\nexplicitly write \\x1edown.\\nWhat kinds of functions K(\\x01;\\x01) can correspond to some feature map \\x1e? In\\nother words, can we tell if there is some feature mapping \\x1eso thatK(x;z) =\\n\\x1e(x)T\\x1e(z) for allx,z?\\nIf we can answer this question by giving a precise characterization of valid\\nkernel functions, then we can completely change the interface of selecting\\nfeature maps \\x1eto the interface of selecting kernel function K. Concretely,\\nwe can pick a function K, verify that it satis\\x0ces the characterization (so\\nthat there exists a feature map \\x1ethatKcorresponds to), and then we can\\nrun update rule (5.11). The bene\\x0ct here is that we don't have to be able\\nto compute \\x1eor write it down analytically, and we only need to know its\\nexistence. We will answer this question at the end of this subsection after\\nwe go through several concrete examples of kernels.\\nSupposex;z2Rd, and let's \\x0crst consider the function K(\\x01;\\x01) de\\x0cned as:\\nK(x;z) = (xTz)2:\",\n",
       " '54\\nWe can also write this as\\nK(x;z) = dX\\ni=1xizi! dX\\nj=1xjzj!\\n=dX\\ni=1dX\\nj=1xixjzizj\\n=dX\\ni;j=1(xixj)(zizj)\\nThus, we see that K(x;z) =h\\x1e(x);\\x1e(z)iis the kernel function that corre-\\nsponds to the the feature mapping \\x1egiven (shown here for the case of d= 3)\\nby\\n\\x1e(x) =2\\n6666666666664x1x1\\nx1x2\\nx1x3\\nx2x1\\nx2x2\\nx2x3\\nx3x1\\nx3x2\\nx3x33\\n7777777777775:\\nRevisiting the computational e\\x0eciency perspective of kernel, note that whereas\\ncalculating the high-dimensional \\x1e(x) requiresO(d2) time, \\x0cnding K(x;z)\\ntakes onlyO(d) time|linear in the dimension of the input attributes.\\nFor another related example, also consider K(\\x01;\\x01) de\\x0cned by\\nK(x;z) = (xTz+c)2\\n=dX\\ni;j=1(xixj)(zizj) +dX\\ni=1(p\\n2cxi)(p\\n2czi) +c2:\\n(Check this yourself.) This function Kis a kernel function that corresponds',\n",
       " \"55\\nto the feature mapping (again shown for d= 3)\\n\\x1e(x) =2\\n666666666666666666664x1x1\\nx1x2\\nx1x3\\nx2x1\\nx2x2\\nx2x3\\nx3x1\\nx3x2\\nx3x3p\\n2cx1p\\n2cx2p\\n2cx3\\nc3\\n777777777777777777775;\\nand the parameter ccontrols the relative weighting between the xi(\\x0crst\\norder) and the xixj(second order) terms.\\nMore broadly, the kernel K(x;z) = (xTz+c)kcorresponds to a feature\\nmapping to an\\x00d+k\\nk\\x01\\nfeature space, corresponding of all monomials of the\\nformxi1xi2:::xikthat are up to order k. However, despite working in this\\nO(dk)-dimensional space, computing K(x;z) still takes only O(d) time, and\\nhence we never need to explicitly represent feature vectors in this very high\\ndimensional feature space.\\nKernels as similarity metrics. Now, let's talk about a slightly di\\x0berent\\nview of kernels. Intuitively, (and there are things wrong with this intuition,\\nbut nevermind), if \\x1e(x) and\\x1e(z) are close together, then we might expect\\nK(x;z) =\\x1e(x)T\\x1e(z) to be large. Conversely, if \\x1e(x) and\\x1e(z) are far apart|\\nsay nearly orthogonal to each other|then K(x;z) =\\x1e(x)T\\x1e(z) will be small.\\nSo, we can think of K(x;z) as some measurement of how similar are \\x1e(x)\\nand\\x1e(z), or of how similar are xandz.\\nGiven this intuition, suppose that for some learning problem that you're\\nworking on, you've come up with some function K(x;z) that you think might\\nbe a reasonable measure of how similar xandzare. For instance, perhaps\\nyou chose\\nK(x;z) = exp\\x12\\n\\x00jjx\\x00zjj2\\n2\\x1b2\\x13\\n:\\nThis is a reasonable measure of xandz's similarity, and is close to 1 when\\nxandzare close, and near 0 when xandzare far apart. Does there exist\",\n",
       " \"56\\na feature map \\x1esuch that the kernel Kde\\x0cned above satis\\x0ces K(x;z) =\\n\\x1e(x)T\\x1e(z)? In this particular example, the answer is yes. This kernel is called\\ntheGaussian kernel , and corresponds to an in\\x0cnite dimensional feature\\nmapping\\x1e. We will give a precise characterization about what properties\\na functionKneeds to satisfy so that it can be a valid kernel function that\\ncorresponds to some feature map \\x1e.\\nNecessary conditions for valid kernels. Suppose for now that Kis\\nindeed a valid kernel corresponding to some feature mapping \\x1e, and we will\\n\\x0crst see what properties it satis\\x0ces. Now, consider some \\x0cnite set of npoints\\n(not necessarily the training set) fx(1);:::;x(n)g, and let a square, n-by-n\\nmatrixKbe de\\x0cned so that its ( i;j)-entry is given by Kij=K(x(i);x(j)).\\nThis matrix is called the kernel matrix . Note that we've overloaded the\\nnotation and used Kto denote both the kernel function K(x;z) and the\\nkernel matrix K, due to their obvious close relationship.\\nNow, ifKis a valid kernel, then Kij=K(x(i);x(j)) =\\x1e(x(i))T\\x1e(x(j)) =\\n\\x1e(x(j))T\\x1e(x(i)) =K(x(j);x(i)) =Kji, and hence Kmust be symmetric. More-\\nover, letting \\x1ek(x) denote the k-th coordinate of the vector \\x1e(x), we \\x0cnd that\\nfor any vector z, we have\\nzTKz =X\\niX\\njziKijzj\\n=X\\niX\\njzi\\x1e(x(i))T\\x1e(x(j))zj\\n=X\\niX\\njziX\\nk\\x1ek(x(i))\\x1ek(x(j))zj\\n=X\\nkX\\niX\\njzi\\x1ek(x(i))\\x1ek(x(j))zj\\n=X\\nk X\\nizi\\x1ek(x(i))!2\\n\\x150:\\nThe second-to-last step uses the fact thatP\\ni;jaiaj= (P\\niai)2forai=\\nzi\\x1ek(x(i)). Sincezwas arbitrary, this shows that Kis positive semi-de\\x0cnite\\n(K\\x150).\\nHence, we've shown that if Kis a valid kernel (i.e., if it corresponds to\\nsome feature mapping \\x1e), then the corresponding kernel matrix K2Rn\\x02n\\nis symmetric positive semide\\x0cnite.\",\n",
       " '57\\nSu\\x0ecient conditions for valid kernels. More generally, the condition\\nabove turns out to be not only a necessary, but also a su\\x0ecient, condition\\nforKto be a valid kernel (also called a Mercer kernel). The following result\\nis due to Mercer.3\\nTheorem (Mercer). LetK:Rd\\x02Rd7!Rbe given. Then for K\\nto be a valid (Mercer) kernel, it is necessary and su\\x0ecient that for any\\nfx(1);:::;x(n)g, (n<1), the corresponding kernel matrix is symmetric pos-\\nitive semi-de\\x0cnite.\\nGiven a function K, apart from trying to \\x0cnd a feature mapping \\x1ethat\\ncorresponds to it, this theorem therefore gives another way of testing if it is\\na valid kernel. You\\'ll also have a chance to play with these ideas more in\\nproblem set 2.\\nIn class, we also brie\\ry talked about a couple of other examples of ker-\\nnels. For instance, consider the digit recognition problem, in which given\\nan image (16x16 pixels) of a handwritten digit (0-9), we have to \\x0cgure out\\nwhich digit it was. Using either a simple polynomial kernel K(x;z) = (xTz)k\\nor the Gaussian kernel, SVMs were able to obtain extremely good perfor-\\nmance on this problem. This was particularly surprising since the input\\nattributesxwere just 256-dimensional vectors of the image pixel intensity\\nvalues, and the system had no prior knowledge about vision, or even about\\nwhich pixels are adjacent to which other ones. Another example that we\\nbrie\\ry talked about in lecture was that if the objects xthat we are trying\\nto classify are strings (say, xis a list of amino acids, which strung together\\nform a protein), then it seems hard to construct a reasonable, \\\\small\" set of\\nfeatures for most learning algorithms, especially if di\\x0berent strings have dif-\\nferent lengths. However, consider letting \\x1e(x) be a feature vector that counts\\nthe number of occurrences of each length- ksubstring in x. If we\\'re consid-\\nering strings of English letters, then there are 26ksuch strings. Hence, \\x1e(x)\\nis a 26kdimensional vector; even for moderate values of k, this is probably\\ntoo big for us to e\\x0eciently work with. (e.g., 264\\x19460000.) However, using\\n(dynamic programming-ish) string matching algorithms, it is possible to ef-\\n\\x0cciently compute K(x;z) =\\x1e(x)T\\x1e(z), so that we can now implicitly work\\nin this 26k-dimensional feature space, but without ever explicitly computing\\nfeature vectors in this space.\\n3Many texts present Mercer\\'s theorem in a slightly more complicated form involving\\nL2functions, but when the input attributes take values in Rd, the version given here is\\nequivalent.',\n",
       " '58\\nApplication of kernel methods: We\\'ve seen the application of kernels\\nto linear regression. In the next part, we will introduce the support vector\\nmachines to which kernels can be directly applied. dwell too much longer on\\nit here. In fact, the idea of kernels has signi\\x0ccantly broader applicability than\\nlinear regression and SVMs. Speci\\x0ccally, if you have any learning algorithm\\nthat you can write in terms of only inner products hx;zibetween input\\nattribute vectors, then by replacing this with K(x;z) whereKis a kernel,\\nyou can \\\\magically\" allow your algorithm to work e\\x0eciently in the high\\ndimensional feature space corresponding to K. For instance, this kernel trick\\ncan be applied with the perceptron to derive a kernel perceptron algorithm.\\nMany of the algorithms that we\\'ll see later in this class will also be amenable\\nto this method, which has come to be known as the \\\\kernel trick.\"',\n",
       " 'Chapter 6\\nSupport vector machines\\nThis set of notes presents the Support Vector Machine (SVM) learning al-\\ngorithm. SVMs are among the best (and many believe are indeed the best)\\n\\\\o\\x0b-the-shelf\" supervised learning algorithms. To tell the SVM story, we\\'ll\\nneed to \\x0crst talk about margins and the idea of separating data with a large\\n\\\\gap.\" Next, we\\'ll talk about the optimal margin classi\\x0cer, which will lead\\nus into a digression on Lagrange duality. We\\'ll also see kernels, which give\\na way to apply SVMs e\\x0eciently in very high dimensional (such as in\\x0cnite-\\ndimensional) feature spaces, and \\x0cnally, we\\'ll close o\\x0b the story with the\\nSMO algorithm, which gives an e\\x0ecient implementation of SVMs.\\n6.1 Margins: intuition\\nWe\\'ll start our story on SVMs by talking about margins. This section will\\ngive the intuitions about margins and about the \\\\con\\x0cdence\" of our predic-\\ntions; these ideas will be made formal in Section 6.3.\\nConsider logistic regression, where the probability p(y= 1jx;\\x12) is mod-\\neled byh\\x12(x) =g(\\x12Tx). We then predict \\\\1\" on an input xif and only if\\nh\\x12(x)\\x150:5, or equivalently, if and only if \\x12Tx\\x150. Consider a positive\\ntraining example ( y= 1). The larger \\x12Txis, the larger also is h\\x12(x) =p(y=\\n1jx;\\x12), and thus also the higher our degree of \\\\con\\x0cdence\" that the label is 1.\\nThus, informally we can think of our prediction as being very con\\x0cdent that\\ny= 1 if\\x12Tx\\x1d0. Similarly, we think of logistic regression as con\\x0cdently\\npredictingy= 0, if\\x12Tx\\x1c0. Given a training set, again informally it seems\\nthat we\\'d have found a good \\x0ct to the training data if we can \\x0cnd \\x12so that\\n\\x12Tx(i)\\x1d0 whenever y(i)= 1, and\\x12Tx(i)\\x1c0 whenever y(i)= 0, since this\\nwould re\\rect a very con\\x0cdent (and correct) set of classi\\x0ccations for all the\\n59',\n",
       " \"60\\ntraining examples. This seems to be a nice goal to aim for, and we'll soon\\nformalize this idea using the notion of functional margins.\\nFor a di\\x0berent type of intuition, consider the following \\x0cgure, in which x's\\nrepresent positive training examples, o's denote negative training examples,\\na decision boundary (this is the line given by the equation \\x12Tx= 0, and\\nis also called the separating hyperplane ) is also shown, and three points\\nhave also been labeled A, B and C.\\n/0 /1\\n/0 /1\\n/0 /1BA\\nC\\nNotice that the point A is very far from the decision boundary. If we are\\nasked to make a prediction for the value of yat A, it seems we should be\\nquite con\\x0cdent that y= 1 there. Conversely, the point C is very close to\\nthe decision boundary, and while it's on the side of the decision boundary\\non which we would predict y= 1, it seems likely that just a small change to\\nthe decision boundary could easily have caused out prediction to be y= 0.\\nHence, we're much more con\\x0cdent about our prediction at A than at C. The\\npoint B lies in-between these two cases, and more broadly, we see that if\\na point is far from the separating hyperplane, then we may be signi\\x0ccantly\\nmore con\\x0cdent in our predictions. Again, informally we think it would be\\nnice if, given a training set, we manage to \\x0cnd a decision boundary that\\nallows us to make all correct and con\\x0cdent (meaning far from the decision\\nboundary) predictions on the training examples. We'll formalize this later\\nusing the notion of geometric margins.\",\n",
       " '61\\n6.2 Notation (option reading)\\nTo make our discussion of SVMs easier, we\\'ll \\x0crst need to introduce a new\\nnotation for talking about classi\\x0ccation. We will be considering a linear\\nclassi\\x0cer for a binary classi\\x0ccation problem with labels yand features x.\\nFrom now, we\\'ll use y2f\\x00 1;1g(instead off0;1g) to denote the class labels.\\nAlso, rather than parameterizing our linear classi\\x0cer with the vector \\x12, we\\nwill use parameters w;b, and write our classi\\x0cer as\\nhw;b(x) =g(wTx+b):\\nHere,g(z) = 1 ifz\\x150, andg(z) =\\x001 otherwise. This \\\\ w;b\" notation\\nallows us to explicitly treat the intercept term bseparately from the other\\nparameters. (We also drop the convention we had previously of letting x0= 1\\nbe an extra coordinate in the input feature vector.) Thus, btakes the role of\\nwhat was previously \\x120, andwtakes the role of [ \\x121:::\\x12d]T.\\nNote also that, from our de\\x0cnition of gabove, our classi\\x0cer will directly\\npredict either 1 or \\x001 (cf. the perceptron algorithm), without \\x0crst going\\nthrough the intermediate step of estimating p(y= 1) (which is what logistic\\nregression does).\\n6.3 Functional and geometric margins (op-\\ntion reading)\\nLet\\'s formalize the notions of the functional and geometric margins. Given a\\ntraining example ( x(i);y(i)), we de\\x0cne the functional margin of (w;b) with\\nrespect to the training example as\\n^\\r(i)=y(i)(wTx(i)+b):\\nNote that if y(i)= 1, then for the functional margin to be large (i.e., for\\nour prediction to be con\\x0cdent and correct), we need wTx(i)+bto be a large\\npositive number. Conversely, if y(i)=\\x001, then for the functional margin\\nto be large, we need wTx(i)+bto be a large negative number. Moreover, if\\ny(i)(wTx(i)+b)>0, then our prediction on this example is correct. (Check\\nthis yourself.) Hence, a large functional margin represents a con\\x0cdent and a\\ncorrect prediction.\\nFor a linear classi\\x0cer with the choice of ggiven above (taking values in\\nf\\x001;1g), there\\'s one property of the functional margin that makes it not a\\nvery good measure of con\\x0cdence, however. Given our choice of g, we note that',\n",
       " \"62\\nif we replace wwith 2wandbwith 2b, then since g(wTx+b) =g(2wTx+2b),\\nthis would not change hw;b(x) at all. I.e., g, and hence also hw;b(x), depends\\nonly on the sign, but not on the magnitude, of wTx+b. However, replacing\\n(w;b) with (2w;2b) also results in multiplying our functional margin by a\\nfactor of 2. Thus, it seems that by exploiting our freedom to scale wandb,\\nwe can make the functional margin arbitrarily large without really changing\\nanything meaningful. Intuitively, it might therefore make sense to impose\\nsome sort of normalization condition such as that jjwjj2= 1; i.e., we might\\nreplace (w;b) with (w=jjwjj2;b=jjwjj2), and instead consider the functional\\nmargin of ( w=jjwjj2;b=jjwjj2). We'll come back to this later.\\nGiven a training set S=f(x(i);y(i));i= 1;:::;ng, we also de\\x0cne the\\nfunction margin of ( w;b) with respect to Sas the smallest of the functional\\nmargins of the individual training examples. Denoted by ^ \\r, this can therefore\\nbe written:\\n^\\r= min\\ni=1;:::;n^\\r(i):\\nNext, let's talk about geometric margins . Consider the picture below:\\nw A\\nγ\\nB(i)\\nThe decision boundary corresponding to ( w;b) is shown, along with the\\nvectorw. Note that wis orthogonal (at 90\\x0e) to the separating hyperplane.\\n(You should convince yourself that this must be the case.) Consider the\\npoint at A, which represents the input x(i)of some training example with\\nlabely(i)= 1. Its distance to the decision boundary, \\r(i), is given by the line\\nsegment AB.\\nHow can we \\x0cnd the value of \\r(i)? Well,w=jjwjjis a unit-length vector\\npointing in the same direction as w. SinceArepresentsx(i), we therefore\",\n",
       " '63\\n\\x0cnd that the point Bis given by x(i)\\x00\\r(i)\\x01w=jjwjj. But this point lies on\\nthe decision boundary, and all points xon the decision boundary satisfy the\\nequationwTx+b= 0. Hence,\\nwT\\x12\\nx(i)\\x00\\r(i)w\\njjwjj\\x13\\n+b= 0:\\nSolving for \\r(i)yields\\n\\r(i)=wTx(i)+b\\njjwjj=\\x12w\\njjwjj\\x13T\\nx(i)+b\\njjwjj:\\nThis was worked out for the case of a positive training example at A in the\\n\\x0cgure, where being on the \\\\positive\" side of the decision boundary is good.\\nMore generally, we de\\x0cne the geometric margin of ( w;b) with respect to a\\ntraining example ( x(i);y(i)) to be\\n\\r(i)=y(i) \\x12w\\njjwjj\\x13T\\nx(i)+b\\njjwjj!\\n:\\nNote that ifjjwjj= 1, then the functional margin equals the geometric\\nmargin|this thus gives us a way of relating these two di\\x0berent notions of\\nmargin. Also, the geometric margin is invariant to rescaling of the parame-\\nters; i.e., if we replace wwith 2wandbwith 2b, then the geometric margin\\ndoes not change. This will in fact come in handy later. Speci\\x0ccally, because\\nof this invariance to the scaling of the parameters, when trying to \\x0ct wandb\\nto training data, we can impose an arbitrary scaling constraint on wwithout\\nchanging anything important; for instance, we can demand that jjwjj= 1, or\\njw1j= 5, orjw1+bj+jw2j= 2, and any of these can be satis\\x0ced simply by\\nrescalingwandb.\\nFinally, given a training set S=f(x(i);y(i));i= 1;:::;ng, we also de\\x0cne\\nthe geometric margin of ( w;b) with respect to Sto be the smallest of the\\ngeometric margins on the individual training examples:\\n\\r= min\\ni=1;:::;n\\r(i):\\n6.4 The optimal margin classi\\x0cer (option read-\\ning)\\nGiven a training set, it seems from our previous discussion that a natural\\ndesideratum is to try to \\x0cnd a decision boundary that maximizes the (ge-\\nometric) margin, since this would re\\rect a very con\\x0cdent set of predictions',\n",
       " '64\\non the training set and a good \\\\\\x0ct\" to the training data. Speci\\x0ccally, this\\nwill result in a classi\\x0cer that separates the positive and the negative training\\nexamples with a \\\\gap\" (geometric margin).\\nFor now, we will assume that we are given a training set that is linearly\\nseparable; i.e., that it is possible to separate the positive and negative ex-\\namples using some separating hyperplane. How will we \\x0cnd the one that\\nachieves the maximum geometric margin? We can pose the following opti-\\nmization problem:\\nmax\\r;w;b\\r\\ns.t.y(i)(wTx(i)+b)\\x15\\r; i = 1;:::;n\\njjwjj= 1:\\nI.e., we want to maximize \\r, subject to each training example having func-\\ntional margin at least \\r. Thejjwjj= 1 constraint moreover ensures that the\\nfunctional margin equals to the geometric margin, so we are also guaranteed\\nthat all the geometric margins are at least \\r. Thus, solving this problem will\\nresult in (w;b) with the largest possible geometric margin with respect to the\\ntraining set.\\nIf we could solve the optimization problem above, we\\'d be done. But the\\n\\\\jjwjj= 1\" constraint is a nasty (non-convex) one, and this problem certainly\\nisn\\'t in any format that we can plug into standard optimization software to\\nsolve. So, let\\'s try transforming the problem into a nicer one. Consider:\\nmax ^\\r;w;b^\\r\\njjwjj\\ns.t.y(i)(wTx(i)+b)\\x15^\\r; i = 1;:::;n\\nHere, we\\'re going to maximize ^ \\r=jjwjj, subject to the functional margins all\\nbeing at least ^ \\r. Since the geometric and functional margins are related by\\n\\r= ^\\r=jjwj, this will give us the answer we want. Moreover, we\\'ve gotten rid\\nof the constraintjjwjj= 1 that we didn\\'t like. The downside is that we now\\nhave a nasty (again, non-convex) objective^\\r\\njjwjjfunction; and, we still don\\'t\\nhave any o\\x0b-the-shelf software that can solve this form of an optimization\\nproblem.\\nLet\\'s keep going. Recall our earlier discussion that we can add an arbi-\\ntrary scaling constraint on wandbwithout changing anything. This is the\\nkey idea we\\'ll use now. We will introduce the scaling constraint that the\\nfunctional margin of w;bwith respect to the training set must be 1:\\n^\\r= 1:',\n",
       " \"65\\nSince multiplying wandbby some constant results in the functional margin\\nbeing multiplied by that same constant, this is indeed a scaling constraint,\\nand can be satis\\x0ced by rescaling w;b. Plugging this into our problem above,\\nand noting that maximizing ^ \\r=jjwjj= 1=jjwjjis the same thing as minimizing\\njjwjj2, we now have the following optimization problem:\\nminw;b1\\n2jjwjj2\\ns.t.y(i)(wTx(i)+b)\\x151; i= 1;:::;n\\nWe've now transformed the problem into a form that can be e\\x0eciently\\nsolved. The above is an optimization problem with a convex quadratic ob-\\njective and only linear constraints. Its solution gives us the optimal mar-\\ngin classi\\x0cer . This optimization problem can be solved using commercial\\nquadratic programming (QP) code.1\\nWhile we could call the problem solved here, what we will instead do is\\nmake a digression to talk about Lagrange duality. This will lead us to our\\noptimization problem's dual form, which will play a key role in allowing us to\\nuse kernels to get optimal margin classi\\x0cers to work e\\x0eciently in very high\\ndimensional spaces. The dual form will also allow us to derive an e\\x0ecient\\nalgorithm for solving the above optimization problem that will typically do\\nmuch better than generic QP software.\\n6.5 Lagrange duality (optional reading)\\nLet's temporarily put aside SVMs and maximum margin classi\\x0cers, and talk\\nabout solving constrained optimization problems.\\nConsider a problem of the following form:\\nminwf(w)\\ns.t.hi(w) = 0; i= 1;:::;l:\\nSome of you may recall how the method of Lagrange multipliers can be used\\nto solve it. (Don't worry if you haven't seen it before.) In this method, we\\nde\\x0cne the Lagrangian to be\\nL(w;\\x0c) =f(w) +lX\\ni=1\\x0cihi(w)\\n1You may be familiar with linear programming, which solves optimization problems\\nthat have linear objectives and linear constraints. QP software is also widely available,\\nwhich allows convex quadratic objectives and linear constraints.\",\n",
       " '66\\nHere, the\\x0ci\\'s are called the Lagrange multipliers . We would then \\x0cnd\\nand setL\\'s partial derivatives to zero:\\n@L\\n@wi= 0;@L\\n@\\x0ci= 0;\\nand solve for wand\\x0c.\\nIn this section, we will generalize this to constrained optimization prob-\\nlems in which we may have inequality as well as equality constraints. Due to\\ntime constraints, we won\\'t really be able to do the theory of Lagrange duality\\njustice in this class,2but we will give the main ideas and results, which we\\nwill then apply to our optimal margin classi\\x0cer\\'s optimization problem.\\nConsider the following, which we\\'ll call the primal optimization problem:\\nminwf(w)\\ns.t.gi(w)\\x140; i= 1;:::;k\\nhi(w) = 0; i= 1;:::;l:\\nTo solve it, we start by de\\x0cning the generalized Lagrangian\\nL(w;\\x0b;\\x0c ) =f(w) +kX\\ni=1\\x0bigi(w) +lX\\ni=1\\x0cihi(w):\\nHere, the\\x0bi\\'s and\\x0ci\\'s are the Lagrange multipliers. Consider the quantity\\n\\x12P(w) = max\\n\\x0b;\\x0c:\\x0bi\\x150L(w;\\x0b;\\x0c ):\\nHere, the \\\\P\" subscript stands for \\\\primal.\" Let some wbe given. If w\\nviolates any of the primal constraints (i.e., if either gi(w)>0 orhi(w)6= 0\\nfor somei), then you should be able to verify that\\n\\x12P(w) = max\\n\\x0b;\\x0c:\\x0bi\\x150f(w) +kX\\ni=1\\x0bigi(w) +lX\\ni=1\\x0cihi(w) (6.1)\\n=1: (6.2)\\nConversely, if the constraints are indeed satis\\x0ced for a particular value of w,\\nthen\\x12P(w) =f(w). Hence,\\n\\x12P(w) =\\x1af(w) ifwsatis\\x0ces primal constraints\\n1 otherwise:\\n2Readers interested in learning more about this topic are encouraged to read, e.g., R.\\nT. Rockarfeller (1970), Convex Analysis, Princeton University Press.',\n",
       " '67\\nThus,\\x12Ptakes the same value as the objective in our problem for all val-\\nues ofwthat satis\\x0ces the primal constraints, and is positive in\\x0cnity if the\\nconstraints are violated. Hence, if we consider the minimization problem\\nmin\\nw\\x12P(w) = min\\nwmax\\n\\x0b;\\x0c:\\x0bi\\x150L(w;\\x0b;\\x0c );\\nwe see that it is the same problem (i.e., and has the same solutions as) our\\noriginal, primal problem. For later use, we also de\\x0cne the optimal value of\\nthe objective to be p\\x03= minw\\x12P(w); we call this the value of the primal\\nproblem.\\nNow, let\\'s look at a slightly di\\x0berent problem. We de\\x0cne\\n\\x12D(\\x0b;\\x0c) = min\\nwL(w;\\x0b;\\x0c ):\\nHere, the \\\\D\" subscript stands for \\\\dual.\" Note also that whereas in the\\nde\\x0cnition of \\x12Pwe were optimizing (maximizing) with respect to \\x0b;\\x0c, here\\nwe are minimizing with respect to w.\\nWe can now pose the dual optimization problem:\\nmax\\n\\x0b;\\x0c:\\x0bi\\x150\\x12D(\\x0b;\\x0c) = max\\n\\x0b;\\x0c:\\x0bi\\x150min\\nwL(w;\\x0b;\\x0c ):\\nThis is exactly the same as our primal problem shown above, except that the\\norder of the \\\\max\" and the \\\\min\" are now exchanged. We also de\\x0cne the\\noptimal value of the dual problem\\'s objective to be d\\x03= max\\x0b;\\x0c:\\x0bi\\x150\\x12D(w).\\nHow are the primal and the dual problems related? It can easily be shown\\nthat\\nd\\x03= max\\n\\x0b;\\x0c:\\x0bi\\x150min\\nwL(w;\\x0b;\\x0c )\\x14min\\nwmax\\n\\x0b;\\x0c:\\x0bi\\x150L(w;\\x0b;\\x0c ) =p\\x03:\\n(You should convince yourself of this; this follows from the \\\\max min\" of a\\nfunction always being less than or equal to the \\\\min max.\") However, under\\ncertain conditions, we will have\\nd\\x03=p\\x03;\\nso that we can solve the dual problem in lieu of the primal problem. Let\\'s\\nsee what these conditions are.\\nSupposefand thegi\\'s are convex,3and thehi\\'s are a\\x0ene.4Suppose\\nfurther that the constraints giare (strictly) feasible; this means that there\\nexists some wso thatgi(w)<0 for alli.\\n3Whenfhas a Hessian, then it is convex if and only if the Hessian is positive semi-\\nde\\x0cnite. For instance, f(w) =wTwis convex; similarly, all linear (and a\\x0ene) functions\\nare also convex. (A function fcan also be convex without being di\\x0berentiable, but we\\nwon\\'t need those more general de\\x0cnitions of convexity here.)\\n4I.e., there exists ai,bi, so thathi(w) =aT\\niw+bi. \\\\A\\x0ene\" means the same thing as\\nlinear, except that we also allow the extra intercept term bi.',\n",
       " '68\\nUnder our above assumptions, there must exist w\\x03;\\x0b\\x03;\\x0c\\x03so thatw\\x03is the\\nsolution to the primal problem, \\x0b\\x03;\\x0c\\x03are the solution to the dual problem,\\nand moreover p\\x03=d\\x03=L(w\\x03;\\x0b\\x03;\\x0c\\x03). Moreover, w\\x03;\\x0b\\x03and\\x0c\\x03satisfy the\\nKarush-Kuhn-Tucker (KKT) conditions , which are as follows:\\n@\\n@wiL(w\\x03;\\x0b\\x03;\\x0c\\x03) = 0; i= 1;:::;d (6.3)\\n@\\n@\\x0ciL(w\\x03;\\x0b\\x03;\\x0c\\x03) = 0; i= 1;:::;l (6.4)\\n\\x0b\\x03\\nigi(w\\x03) = 0; i= 1;:::;k (6.5)\\ngi(w\\x03)\\x140; i= 1;:::;k (6.6)\\n\\x0b\\x03\\x150; i= 1;:::;k (6.7)\\nMoreover, if some w\\x03;\\x0b\\x03;\\x0c\\x03satisfy the KKT conditions, then it is also a solution to t he primal and dual\\nproblems.\\nWe draw attention to Equation (6.5), which is called the KKT dual\\ncomplementarity condition. Speci\\x0ccally, it implies that if \\x0b\\x03\\ni>0, then\\ngi(w\\x03) = 0. (I.e., the \\\\ gi(w)\\x140\" constraint is active , meaning it holds with\\nequality rather than with inequality.) Later on, this will be key for showing\\nthat the SVM has only a small number of \\\\support vectors\"; the KKT dual\\ncomplementarity condition will also give us our convergence test when we\\ntalk about the SMO algorithm.\\n6.6 Optimal margin classi\\x0cers: the dual form\\n(option reading)\\nNote: The equivalence of optimization problem (6.8) and the optimization\\nproblem (6.12) , and the relationship between the primary and dual variables\\nin equation (6.10) are the most important take home messages of this section.\\nPreviously, we posed the following (primal) optimization problem for \\x0cnd-\\ning the optimal margin classi\\x0cer:\\nminw;b1\\n2jjwjj2(6.8)\\ns.t.y(i)(wTx(i)+b)\\x151; i= 1;:::;n\\nWe can write the constraints as\\ngi(w) =\\x00y(i)(wTx(i)+b) + 1\\x140:',\n",
       " '69\\nWe have one such constraint for each training example. Note that from the\\nKKT dual complementarity condition, we will have \\x0bi>0 only for the train-\\ning examples that have functional margin exactly equal to one (i.e., the ones\\ncorresponding to constraints that hold with equality, gi(w) = 0). Consider\\nthe \\x0cgure below, in which a maximum margin separating hyperplane is shown\\nby the solid line.\\nThe points with the smallest margins are exactly the ones closest to the\\ndecision boundary; here, these are the three points (one negative and two pos-\\nitive examples) that lie on the dashed lines parallel to the decision boundary.\\nThus, only three of the \\x0bi\\'s|namely, the ones corresponding to these three\\ntraining examples|will be non-zero at the optimal solution to our optimiza-\\ntion problem. These three points are called the support vectors in this\\nproblem. The fact that the number of support vectors can be much smaller\\nthan the size the training set will be useful later.\\nLet\\'s move on. Looking ahead, as we develop the dual form of the prob-\\nlem, one key idea to watch out for is that we\\'ll try to write our algorithm\\nin terms of only the inner product hx(i);x(j)i(think of this as ( x(i))Tx(j))\\nbetween points in the input feature space. The fact that we can express our\\nalgorithm in terms of these inner products will be key when we apply the\\nkernel trick.\\nWhen we construct the Lagrangian for our optimization problem we have:\\nL(w;b;\\x0b ) =1\\n2jjwjj2\\x00nX\\ni=1\\x0bi\\x02\\ny(i)(wTx(i)+b)\\x001\\x03\\n: (6.9)\\nNote that there\\'re only \\\\ \\x0bi\" but no \\\\ \\x0ci\" Lagrange multipliers, since the\\nproblem has only inequality constraints.',\n",
       " \"70\\nLet's \\x0cnd the dual form of the problem. To do so, we need to \\x0crst\\nminimizeL(w;b;\\x0b ) with respect to wandb(for \\x0cxed\\x0b), to get\\x12D, which\\nwe'll do by setting the derivatives of Lwith respect to wandbto zero. We\\nhave:\\nrwL(w;b;\\x0b ) =w\\x00nX\\ni=1\\x0biy(i)x(i)= 0\\nThis implies that\\nw=nX\\ni=1\\x0biy(i)x(i): (6.10)\\nAs for the derivative with respect to b, we obtain\\n@\\n@bL(w;b;\\x0b ) =nX\\ni=1\\x0biy(i)= 0: (6.11)\\nIf we take the de\\x0cnition of win Equation (6.10) and plug that back into\\nthe Lagrangian (Equation 6.9), and simplify, we get\\nL(w;b;\\x0b ) =nX\\ni=1\\x0bi\\x001\\n2nX\\ni;j=1y(i)y(j)\\x0bi\\x0bj(x(i))Tx(j)\\x00bnX\\ni=1\\x0biy(i):\\nBut from Equation (6.11), the last term must be zero, so we obtain\\nL(w;b;\\x0b ) =nX\\ni=1\\x0bi\\x001\\n2nX\\ni;j=1y(i)y(j)\\x0bi\\x0bj(x(i))Tx(j):\\nRecall that we got to the equation above by minimizing Lwith respect to\\nwandb. Putting this together with the constraints \\x0bi\\x150 (that we always\\nhad) and the constraint (6.11), we obtain the following dual optimization\\nproblem:\\nmax\\x0bW(\\x0b) =nX\\ni=1\\x0bi\\x001\\n2nX\\ni;j=1y(i)y(j)\\x0bi\\x0bjhx(i);x(j)i: (6.12)\\ns.t.\\x0bi\\x150; i= 1;:::;n\\nnX\\ni=1\\x0biy(i)= 0;\\nYou should also be able to verify that the conditions required for p\\x03=d\\x03\\nand the KKT conditions (Equations 6.3{6.7) to hold are indeed satis\\x0ced in\",\n",
       " \"71\\nour optimization problem. Hence, we can solve the dual in lieu of solving\\nthe primal problem. Speci\\x0ccally, in the dual problem above, we have a\\nmaximization problem in which the parameters are the \\x0bi's. We'll talk later\\nabout the speci\\x0cc algorithm that we're going to use to solve the dual problem,\\nbut if we are indeed able to solve it (i.e., \\x0cnd the \\x0b's that maximize W(\\x0b)\\nsubject to the constraints), then we can use Equation (6.10) to go back and\\n\\x0cnd the optimal w's as a function of the \\x0b's. Having found w\\x03, by considering\\nthe primal problem, it is also straightforward to \\x0cnd the optimal value for\\nthe intercept term bas\\nb\\x03=\\x00maxi:y(i)=\\x001w\\x03Tx(i)+ mini:y(i)=1w\\x03Tx(i)\\n2: (6.13)\\n(Check for yourself that this is correct.)\\nBefore moving on, let's also take a more careful look at Equation (6.10),\\nwhich gives the optimal value of win terms of (the optimal value of) \\x0b.\\nSuppose we've \\x0ct our model's parameters to a training set, and now wish to\\nmake a prediction at a new point input x. We would then calculate wTx+b,\\nand predict y= 1 if and only if this quantity is bigger than zero. But\\nusing (6.10), this quantity can also be written:\\nwTx+b= nX\\ni=1\\x0biy(i)x(i)!T\\nx+b (6.14)\\n=nX\\ni=1\\x0biy(i)hx(i);xi+b: (6.15)\\nHence, if we've found the \\x0bi's, in order to make a prediction, we have to\\ncalculate a quantity that depends only on the inner product between xand\\nthe points in the training set. Moreover, we saw earlier that the \\x0bi's will all\\nbe zero except for the support vectors. Thus, many of the terms in the sum\\nabove will be zero, and we really need to \\x0cnd only the inner products between\\nxand the support vectors (of which there is often only a small number) in\\norder calculate (6.15) and make our prediction.\\nBy examining the dual form of the optimization problem, we gained sig-\\nni\\x0ccant insight into the structure of the problem, and were also able to write\\nthe entire algorithm in terms of only inner products between input feature\\nvectors. In the next section, we will exploit this property to apply the ker-\\nnels to our classi\\x0ccation problem. The resulting algorithm, support vector\\nmachines , will be able to e\\x0eciently learn in very high dimensional spaces.\",\n",
       " \"72\\n6.7 Regularization and the non-separable case\\n(optional reading)\\nThe derivation of the SVM as presented so far assumed that the data is\\nlinearly separable. While mapping data to a high dimensional feature space\\nvia\\x1edoes generally increase the likelihood that the data is separable, we\\ncan't guarantee that it always will be so. Also, in some cases it is not clear\\nthat \\x0cnding a separating hyperplane is exactly what we'd want to do, since\\nthat might be susceptible to outliers. For instance, the left \\x0cgure below\\nshows an optimal margin classi\\x0cer, and when a single outlier is added in the\\nupper-left region (right \\x0cgure), it causes the decision boundary to make a\\ndramatic swing, and the resulting classi\\x0cer has a much smaller margin.\\nTo make the algorithm work for non-linearly separable datasets as well\\nas be less sensitive to outliers, we reformulate our optimization (using `1\\nregularization ) as follows:\\nmin\\r;w;b1\\n2jjwjj2+CnX\\ni=1\\x18i\\ns.t.y(i)(wTx(i)+b)\\x151\\x00\\x18i; i= 1;:::;n\\n\\x18i\\x150; i= 1;:::;n:\\nThus, examples are now permitted to have (functional) margin less than 1,\\nand if an example has functional margin 1 \\x00\\x18i(with\\x18 >0), we would pay\\na cost of the objective function being increased by C\\x18i. The parameter C\\ncontrols the relative weighting between the twin goals of making the jjwjj2\\nsmall (which we saw earlier makes the margin large) and of ensuring that\\nmost examples have functional margin at least 1.\",\n",
       " \"73\\nAs before, we can form the Lagrangian:\\nL(w;b;\\x18;\\x0b;r ) =1\\n2wTw+CnX\\ni=1\\x18i\\x00nX\\ni=1\\x0bi\\x02\\ny(i)(xTw+b)\\x001 +\\x18i\\x03\\n\\x00nX\\ni=1ri\\x18i:\\nHere, the\\x0bi's andri's are our Lagrange multipliers (constrained to be \\x150).\\nWe won't go through the derivation of the dual again in detail, but after\\nsetting the derivatives with respect to wandbto zero as before, substituting\\nthem back in, and simplifying, we obtain the following dual form of the\\nproblem:\\nmax\\x0bW(\\x0b) =nX\\ni=1\\x0bi\\x001\\n2nX\\ni;j=1y(i)y(j)\\x0bi\\x0bjhx(i);x(j)i\\ns.t. 0\\x14\\x0bi\\x14C; i = 1;:::;n\\nnX\\ni=1\\x0biy(i)= 0;\\nAs before, we also have that wcan be expressed in terms of the \\x0bi's as\\ngiven in Equation (6.10), so that after solving the dual problem, we can con-\\ntinue to use Equation (6.15) to make our predictions. Note that, somewhat\\nsurprisingly, in adding `1regularization, the only change to the dual prob-\\nlem is that what was originally a constraint that 0 \\x14\\x0bihas now become\\n0\\x14\\x0bi\\x14C. The calculation for b\\x03also has to be modi\\x0ced (Equation 6.13 is\\nno longer valid); see the comments in the next section/Platt's paper.\\nAlso, the KKT dual-complementarity conditions (which in the next sec-\\ntion will be useful for testing for the convergence of the SMO algorithm)\\nare:\\n\\x0bi= 0)y(i)(wTx(i)+b)\\x151 (6.16)\\n\\x0bi=C)y(i)(wTx(i)+b)\\x141 (6.17)\\n0<\\x0bi<C)y(i)(wTx(i)+b) = 1: (6.18)\\nNow, all that remains is to give an algorithm for actually solving the dual\\nproblem, which we will do in the next section.\\n6.8 The SMO algorithm (optional reading)\\nThe SMO (sequential minimal optimization) algorithm, due to John Platt,\\ngives an e\\x0ecient way of solving the dual problem arising from the derivation\",\n",
       " '74\\nof the SVM. Partly to motivate the SMO algorithm, and partly because it\\'s\\ninteresting in its own right, let\\'s \\x0crst take another digression to talk about\\nthe coordinate ascent algorithm.\\n6.8.1 Coordinate ascent\\nConsider trying to solve the unconstrained optimization problem\\nmax\\n\\x0bW(\\x0b1;\\x0b2;:::;\\x0bn):\\nHere, we think of Was just some function of the parameters \\x0bi\\'s, and for now\\nignore any relationship between this problem and SVMs. We\\'ve already seen\\ntwo optimization algorithms, gradient ascent and Newton\\'s method. The\\nnew algorithm we\\'re going to consider here is called coordinate ascent :\\nLoop until convergence: f\\nFori= 1;:::;n ,f\\n\\x0bi:= arg max ^\\x0biW(\\x0b1;:::;\\x0bi\\x001;^\\x0bi;\\x0bi+1;:::;\\x0bn).\\ng\\ng\\nThus, in the innermost loop of this algorithm, we will hold all the variables\\nexcept for some \\x0bi\\x0cxed, and reoptimize Wwith respect to just the parameter\\n\\x0bi. In the version of this method presented here, the inner-loop reoptimizes\\nthe variables in order \\x0b1;\\x0b2;:::;\\x0bn;\\x0b1;\\x0b2;:::. (A more sophisticated version\\nmight choose other orderings; for instance, we may choose the next variable\\nto update according to which one we expect to allow us to make the largest\\nincrease in W(\\x0b).)\\nWhen the function Whappens to be of such a form that the \\\\arg max\"\\nin the inner loop can be performed e\\x0eciently, then coordinate ascent can be\\na fairly e\\x0ecient algorithm. Here\\'s a picture of coordinate ascent in action:',\n",
       " \"75\\n−2 −1.5 −1 −0.5 0 0.5 1 1.5 2 2.5−2−1.5−1−0.500.511.522.5\\nThe ellipses in the \\x0cgure are the contours of a quadratic function that\\nwe want to optimize. Coordinate ascent was initialized at (2 ;\\x002), and also\\nplotted in the \\x0cgure is the path that it took on its way to the global maximum.\\nNotice that on each step, coordinate ascent takes a step that's parallel to one\\nof the axes, since only one variable is being optimized at a time.\\n6.8.2 SMO\\nWe close o\\x0b the discussion of SVMs by sketching the derivation of the SMO\\nalgorithm.\\nHere's the (dual) optimization problem that we want to solve:\\nmax\\x0bW(\\x0b) =nX\\ni=1\\x0bi\\x001\\n2nX\\ni;j=1y(i)y(j)\\x0bi\\x0bjhx(i);x(j)i: (6.19)\\ns.t. 0\\x14\\x0bi\\x14C; i = 1;:::;n (6.20)\\nnX\\ni=1\\x0biy(i)= 0: (6.21)\\nLet's say we have set of \\x0bi's that satisfy the constraints (6.20-6.21). Now,\\nsuppose we want to hold \\x0b2;:::;\\x0bn\\x0cxed, and take a coordinate ascent step\\nand reoptimize the objective with respect to \\x0b1. Can we make any progress?\\nThe answer is no, because the constraint (6.21) ensures that\\n\\x0b1y(1)=\\x00nX\\ni=2\\x0biy(i):\",\n",
       " \"76\\nOr, by multiplying both sides by y(1), we equivalently have\\n\\x0b1=\\x00y(1)nX\\ni=2\\x0biy(i):\\n(This step used the fact that y(1)2f\\x00 1;1g, and hence ( y(1))2= 1.) Hence,\\n\\x0b1is exactly determined by the other \\x0bi's, and if we were to hold \\x0b2;:::;\\x0bn\\n\\x0cxed, then we can't make any change to \\x0b1without violating the con-\\nstraint (6.21) in the optimization problem.\\nThus, if we want to update some subject of the \\x0bi's, we must update at\\nleast two of them simultaneously in order to keep satisfying the constraints.\\nThis motivates the SMO algorithm, which simply does the following:\\nRepeat till convergence f\\n1. Select some pair \\x0biand\\x0bjto update next (using a heuristic that\\ntries to pick the two that will allow us to make the biggest progress\\ntowards the global maximum).\\n2. Reoptimize W(\\x0b) with respect to \\x0biand\\x0bj, while holding all the\\nother\\x0bk's (k6=i;j) \\x0cxed.\\ng\\nTo test for convergence of this algorithm, we can check whether the KKT\\nconditions (Equations 6.16-6.18) are satis\\x0ced to within some tol. Here, tolis\\nthe convergence tolerance parameter, and is typically set to around 0.01 to\\n0.001. (See the paper and pseudocode for details.)\\nThe key reason that SMO is an e\\x0ecient algorithm is that the update to\\n\\x0bi,\\x0bjcan be computed very e\\x0eciently. Let's now brie\\ry sketch the main\\nideas for deriving the e\\x0ecient update.\\nLet's say we currently have some setting of the \\x0bi's that satisfy the con-\\nstraints (6.20-6.21), and suppose we've decided to hold \\x0b3;:::;\\x0bn\\x0cxed, and\\nwant to reoptimize W(\\x0b1;\\x0b2;:::;\\x0bn) with respect to \\x0b1and\\x0b2(subject to\\nthe constraints). From (6.21), we require that\\n\\x0b1y(1)+\\x0b2y(2)=\\x00nX\\ni=3\\x0biy(i):\\nSince the right hand side is \\x0cxed (as we've \\x0cxed \\x0b3;:::\\x0bn), we can just let\\nit be denoted by some constant \\x10:\\n\\x0b1y(1)+\\x0b2y(2)=\\x10: (6.22)\\nWe can thus picture the constraints on \\x0b1and\\x0b2as follows:\",\n",
       " '77\\nα2\\nα1α1 α2\\nCC\\n(1)+(2)y y=ζH\\nL\\nFrom the constraints (6.20), we know that \\x0b1and\\x0b2must lie within the box\\n[0;C]\\x02[0;C] shown. Also plotted is the line \\x0b1y(1)+\\x0b2y(2)=\\x10, on which we\\nknow\\x0b1and\\x0b2must lie. Note also that, from these constraints, we know\\nL\\x14\\x0b2\\x14H; otherwise, ( \\x0b1;\\x0b2) can\\'t simultaneously satisfy both the box\\nand the straight line constraint. In this example, L= 0. But depending on\\nwhat the line \\x0b1y(1)+\\x0b2y(2)=\\x10looks like, this won\\'t always necessarily be\\nthe case; but more generally, there will be some lower-bound Land some\\nupper-bound Hon the permissible values for \\x0b2that will ensure that \\x0b1,\\x0b2\\nlie within the box [0 ;C]\\x02[0;C].\\nUsing Equation (6.22), we can also write \\x0b1as a function of \\x0b2:\\n\\x0b1= (\\x10\\x00\\x0b2y(2))y(1):\\n(Check this derivation yourself; we again used the fact that y(1)2f\\x00 1;1gso\\nthat (y(1))2= 1.) Hence, the objective W(\\x0b) can be written\\nW(\\x0b1;\\x0b2;:::;\\x0bn) =W((\\x10\\x00\\x0b2y(2))y(1);\\x0b2;:::;\\x0bn):\\nTreating\\x0b3;:::;\\x0bnas constants, you should be able to verify that this is\\njust some quadratic function in \\x0b2. I.e., this can also be expressed in the\\nforma\\x0b2\\n2+b\\x0b2+cfor some appropriate a,b, andc. If we ignore the \\\\box\"\\nconstraints (6.20) (or, equivalently, that L\\x14\\x0b2\\x14H), then we can easily\\nmaximize this quadratic function by setting its derivative to zero and solving.\\nWe\\'ll let\\x0bnew;unclipped\\n2 denote the resulting value of \\x0b2. You should also be\\nable to convince yourself that if we had instead wanted to maximize Wwith\\nrespect to\\x0b2but subject to the box constraint, then we can \\x0cnd the resulting\\nvalue optimal simply by taking \\x0bnew;unclipped\\n2 and \\\\clipping\" it to lie in the',\n",
       " \"78\\n[L;H] interval, to get\\n\\x0bnew\\n2 =8\\n<\\n:H if\\x0bnew;unclipped\\n2 >H\\n\\x0bnew;unclipped\\n2 ifL\\x14\\x0bnew;unclipped\\n2\\x14H\\nL if\\x0bnew;unclipped\\n2 <L\\nFinally, having found the \\x0bnew\\n2, we can use Equation (6.22) to go back and\\n\\x0cnd the optimal value of \\x0bnew\\n1.\\nThere're a couple more details that are quite easy but that we'll leave you\\nto read about yourself in Platt's paper: One is the choice of the heuristics\\nused to select the next \\x0bi,\\x0bjto update; the other is how to update bas the\\nSMO algorithm is run.\",\n",
       " 'Part II\\nDeep learning\\n79',\n",
       " \"Chapter 7\\nDeep learning\\nWe now begin our study of deep learning. In this set of notes, we give an\\noverview of neural networks, discuss vectorization and discuss training neural\\nnetworks with backpropagation.\\n7.1 Supervised learning with non-linear mod-\\nels\\nIn the supervised learning setting (predicting yfrom the input x), suppose\\nour model/hypothesis is h\\x12(x). In the past lectures, we have considered the\\ncases when h\\x12(x) =\\x12>x(in linear regression) or h\\x12(x) =\\x12>\\x1e(x) (where\\x1e(x)\\nis the feature map). A commonality of these two models is that they are\\nlinear in the parameters \\x12. Next we will consider learning general family of\\nmodels that are non-linear in both the parameters \\x12and the inputs x. The\\nmost common non-linear models are neural networks, which we will de\\x0cne\\nstaring from the next section. For this section, it su\\x0eces to think h\\x12(x) as\\nan abstract non-linear model.1\\nSupposef(x(i);y(i))gn\\ni=1are the training examples. We will de\\x0cne the\\nnonlinear model and the loss/cost function for learning it.\\nRegression problems. For simplicity, we start with the case where the\\noutput is a real number, that is, y(i)2R, and thus the model h\\x12also outputs\\na real number h\\x12(x)2R. We de\\x0cne the least square cost function for the\\n1If a concrete example is helpful, perhaps think about the model h\\x12(x) =\\x122\\n1x2\\n1+\\x122\\n2x2\\n2+\\n\\x01\\x01\\x01+\\x122\\ndx2\\ndin this subsection, even though it's not a neural network.\\n80\",\n",
       " '81\\ni-th example ( x(i);y(i)) as\\nJ(i)(\\x12) =1\\n2(h\\x12(x(i))\\x00y(i))2; (7.1)\\nand de\\x0cne the mean-square cost function for the dataset as\\nJ(\\x12) =1\\nnnX\\ni=1J(i)(\\x12); (7.2)\\nwhich is same as in linear regression except that we introduce a constant\\n1=nin front of the cost function to be consistent with the convention. Note\\nthat multiplying the cost function with a scalar will not change the local\\nminima or global minima of the cost function. Also note that the underlying\\nparameterization for h\\x12(x) is di\\x0berent from the case of linear regression,\\neven though the form of the cost function is the same mean-squared loss.\\nThroughout the notes, we use the words \\\\loss\" and \\\\cost\" interchangeably.\\nBinary classi\\x0ccation. Next we de\\x0cne the model and loss function for\\nbinary classi\\x0ccation. Suppose the inputs x2Rd. Let \\x16h\\x12:Rd!Rbe a\\nparameterized model (the analog of \\x12>xin logistic linear regression). We\\ncall the output \\x16h\\x12(x)2Rthe logit. Analogous to Section 2.1, we use the\\nlogistic function g(\\x01) to turn the logit \\x16h\\x12(x) to a probability h\\x12(x)2[0;1]:\\nh\\x12(x) =g(\\x16h\\x12(x)) = 1=(1 + exp(\\x00\\x16h\\x12(x)): (7.3)\\nWe model the conditional distribution of ygivenxand\\x12by\\nP(y= 1jx;\\x12) =h\\x12(x)\\nP(y= 0jx;\\x12) = 1\\x00h\\x12(x)\\nFollowing the same derivation in Section 2.1 and using the derivation in\\nRemark 2.1.1, the negative likelihood loss function is equal to:\\nJ(i)(\\x12) =\\x00logp(y(i)jx(i);\\x12) =`logistic (\\x16h\\x12(x(i));y(i)) (7.4)\\nAs done in equation (7.2), the total loss function is also de\\x0cned as the average\\nof the loss function over individual training examples, J(\\x12) =1\\nnPn\\ni=1J(i)(\\x12):',\n",
       " '82\\nMulti-class classi\\x0ccation. Following Section 2.3, we consider a classi\\x0cca-\\ntion problem where the response variable ycan take on any one of kvalues,\\ni.e.y2f1;2;:::;kg. Let \\x16h\\x12:Rd!Rkbe a parameterized model. We\\ncall the outputs \\x16h\\x12(x)2Rkthe logits. Each logit corresponds to the predic-\\ntion for one of the kclasses. Analogous to Section 2.3, we use the softmax\\nfunction to turn the logits \\x16h\\x12(x) into a probability vector with non-negative\\nentries that sum up to 1:\\nP(y=jjx;\\x12) =exp(\\x16h\\x12(x)j)\\nPk\\ns=1exp(\\x16h\\x12(x)s); (7.5)\\nwhere \\x16h\\x12(x)sdenotes the s-th coordinate of \\x16h\\x12(x).\\nSimilarly to Section 2.3, the loss function for a single training example\\n(x(i);y(i)) is its negative log-likelihood:\\nJ(i)(\\x12) =\\x00logp(y(i)jx(i);\\x12) =\\x00log \\nexp(\\x16h\\x12(x(i))y(i))\\nPk\\ns=1exp(\\x16h\\x12(x(i))s)!\\n: (7.6)\\nUsing the notations of Section 2.3, we can simply write in an abstract way:\\nJ(i)(\\x12) =`ce(\\x16h\\x12(x(i));y(i)): (7.7)\\nThe loss function is also de\\x0cned as the average of the loss function of indi-\\nvidual training examples, J(\\x12) =1\\nnPn\\ni=1J(i)(\\x12):\\nWe also note that the approach above can also be generated to any con-\\nditional probabilistic model where we have an exponential distribution for\\ny, Exponential-family( y;\\x11), where\\x11=\\x16h\\x12(x) is a parameterized nonlinear\\nfunction of x. However, the most widely used situations are the three cases\\ndiscussed above.\\nOptimizers (SGD). Commonly, people use gradient descent (GD), stochas-\\ntic gradient (SGD), or their variants to optimize the loss function J(\\x12). GD\\'s\\nupdate rule can be written as2\\n\\x12:=\\x12\\x00\\x0br\\x12J(\\x12) (7.8)\\nwhere\\x0b > 0 is often referred to as the learning rate or step size. Next, we\\nintroduce a version of the SGD (Algorithm 1), which is lightly di\\x0berent from\\nthat in the \\x0crst lecture notes.\\n2Recall that, as de\\x0cned in the previous lecture notes, we use the notation \\\\ a:=b\" to\\ndenote an operation (in a computer program) in which we setthe value of a variable ato\\nbe equal to the value of b. In other words, this operation overwrites awith the value of\\nb. In contrast, we will write \\\\ a=b\" when we are asserting a statement of fact, that the\\nvalue ofais equal to the value of b.',\n",
       " '83\\nAlgorithm 1 Stochastic Gradient Descent\\n1:Hyperparameter: learning rate \\x0b, number of total iteration niter.\\n2:Initialize\\x12randomly.\\n3:fori= 1 toniterdo\\n4: Samplejuniformly fromf1;:::;ng, and update \\x12by\\n\\x12:=\\x12\\x00\\x0br\\x12J(j)(\\x12) (7.9)\\nOftentimes computing the gradient of Bexamples simultaneously for the\\nparameter \\x12can be faster than computing Bgradients separately due to\\nhardware parallelization. Therefore, a mini-batch version of SGD is most\\ncommonly used in deep learning, as shown in Algorithm 2. There are also\\nother variants of the SGD or mini-batch SGD with slightly di\\x0berent sampling\\nschemes.\\nAlgorithm 2 Mini-batch Stochastic Gradient Descent\\n1:Hyperparameters: learning rate \\x0b, batch size B, # iterations niter.\\n2:Initialize\\x12randomly\\n3:fori= 1 toniterdo\\n4: SampleBexamplesj1;:::;jB(without replacement) uniformly from\\nf1;:::;ng, and update \\x12by\\n\\x12:=\\x12\\x00\\x0b\\nBBX\\nk=1r\\x12J(jk)(\\x12) (7.10)\\nWith these generic algorithms, a typical deep learning model is learned\\nwith the following steps. 1. De\\x0cne a neural network parametrization h\\x12(x),\\nwhich we will introduce in Section 7.2, and 2. write the backpropagation\\nalgorithm to compute the gradient of the loss function J(j)(\\x12) e\\x0eciently,\\nwhich will be covered in Section 7.4, and 3. run SGD or mini-batch SGD (or\\nother gradient-based optimizers) with the loss function J(\\x12).',\n",
       " '84\\n7.2 Neural networks\\nNeural networks refer to a broad type of non-linear models/parametrizations\\n\\x16h\\x12(x) that involve combinations of matrix multiplications and other entry-\\nwise non-linear operations. To have a uni\\x0ced treatment for regression prob-\\nlem and classi\\x0ccation problem, here we consider \\x16h\\x12(x) as the output of the\\nneural network. For regression problem, the \\x0cnal prediction h\\x12(x) =\\x16h\\x12(x),\\nand for classi\\x0ccation problem, \\x16h\\x12(x) is the logits and the predicted probability\\nwill beh\\x12(x) = 1=(1+exp(\\x00\\x16h\\x12(x)) (see equation 7.3) for binary classi\\x0ccation\\norh\\x12(x) = softmax( \\x16h\\x12(x)) for multi-class classi\\x0ccation (see equation 7.5).\\nWe will start small and slowly build up a neural network, step by step.\\nA Neural Network with a Single Neuron. Recall the housing price\\nprediction problem from before: given the size of the house, we want to\\npredict the price. We will use it as a running example in this subsection.\\nPreviously, we \\x0ct a straight line to the graph of size vs. housing price.\\nNow, instead of \\x0ctting a straight line, we wish to prevent negative housing\\nprices by setting the absolute minimum price as zero. This produces a \\\\kink\"\\nin the graph as shown in Figure 7.1. How do we represent such a function\\nwith a single kink as \\x16h\\x12(x) with unknown parameter? (After doing so, we\\ncan invoke the machinery in Section 7.1.)\\nWe de\\x0cne a parameterized function \\x16h\\x12(x) with input x, parameterized by\\n\\x12, which outputs the price of the house y. Formally, \\x16h\\x12:x!y. Perhaps\\none of the simplest parametrization would be\\n\\x16h\\x12(x) = max(wx+b;0);where\\x12= (w;b)2R2(7.11)\\nHere \\x16h\\x12(x) returns a single value: ( wx+b) or zero, whichever is greater. In\\nthe context of neural networks, the function max ft;0gis called a ReLU (pro-\\nnounced \\\\ray-lu\"), or recti\\x0ced linear unit, and often denoted by ReLU( t),\\nmaxft;0g.\\nGenerally, a one-dimensional non-linear function that maps RtoRsuch as\\nReLU is often referred to as an activation function . The model \\x16h\\x12(x) is said\\nto have a single neuron partly because it has a single non-linear activation\\nfunction. (We will discuss more about why a non-linear activation is called\\nneuron.)\\nWhen the input x2Rdhas multiple dimensions, a neural network with\\na single neuron can be written as\\n\\x16h\\x12(x) = ReLU(w>x+b);wherew2Rd,b2R, and\\x12= (w;b) (7.12)',\n",
       " '85\\n500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices \\nsquare feet price (in $1000) \\nFigure 7.1: Housing prices with a \\\\kink\" in the graph.\\nThe termbis often referred to as the \\\\bias\", and the vector wis referred\\nto as the weight vector. Such a neural network has 1 layer. (We will de\\x0cne\\nwhat multiple layers mean in the sequel.)\\nStacking Neurons. A more complex neural network may take the single\\nneuron described above and \\\\stack\" them together such that one neuron\\npasses its output as input into the next neuron, resulting in a more complex\\nfunction.\\nLet us now deepen the housing prediction example. In addition to the size\\nof the house, suppose that you know the number of bedrooms, the zip code\\nand the wealth of the neighborhood. Building neural networks is analogous\\nto Lego bricks: you take individual bricks and stack them together to build\\ncomplex structures. The same applies to neural networks: we take individual\\nneurons and stack them together to create complex neural networks.\\nGiven these features (size, number of bedrooms, zip code, and wealth),\\nwe might then decide that the price of the house depends on the maximum\\nfamily size it can accommodate. Suppose the family size is a function of the\\nsize of the house and number of bedrooms (see Figure 7.2). The zip code\\nmay provide additional information such as how walkable the neighborhood\\nis (i.e., can you walk to the grocery store or do you need to drive everywhere).\\nCombining the zip code with the wealth of the neighborhood may predict\\nthe quality of the local elementary school. Given these three derived features\\n(family size, walkable, school quality), we may conclude that the price of the',\n",
       " '86\\nhome ultimately depends on these three features.\\nFamily Size \\nSchool Quality Walkable Size \\n# Bedrooms \\nZip Code \\nWealth Price \\ny\\nFigure 7.2: Diagram of a small neural network for predicting housing prices.\\nFormally, the input to a neural network is a set of input features\\nx1;x2;x3;x4. We denote the intermediate variables for \\\\family size\", \\\\walk-\\nable\", and \\\\school quality\" by a1;a2;a3(theseai\\'s are often referred to as\\n\\\\hidden units\" or \\\\hidden neurons\"). We represent each of the ai\\'s as a neu-\\nral network with a single neuron with a subset of x1;:::;x 4as inputs. Then\\nas in Figure 7.1, we will have the parameterization:\\na1= ReLU(\\x121x1+\\x122x2+\\x123)\\na2= ReLU(\\x124x3+\\x125)\\na3= ReLU(\\x126x3+\\x127x4+\\x128)\\nwhere (\\x121;\\x01\\x01\\x01;\\x128) are parameters. Now we represent the \\x0cnal output \\x16h\\x12(x)\\nas another linear function with a1;a2;a3as inputs, and we get3\\n\\x16h\\x12(x) =\\x129a1+\\x1210a2+\\x1211a3+\\x1212 (7.13)\\nwhere\\x12contains all the parameters ( \\x121;\\x01\\x01\\x01;\\x1212).\\nNow we represent the output as a quite complex function of xwith pa-\\nrameters\\x12. Then you can use this parametrization \\x16h\\x12with the machinery of\\nSection 7.1 to learn the parameters \\x12.\\nInspiration from Biological Neural Networks. As the name suggests,\\narti\\x0ccial neural networks were inspired by biological neural networks. The\\nhidden units a1;:::;amcorrespond to the neurons in a biological neural net-\\nwork, and the parameters \\x12i\\'s correspond to the synapses. However, it\\'s\\nunclear how similar the modern deep arti\\x0ccial neural networks are to the bi-\\nological ones. For example, perhaps not many neuroscientists think biological\\n3Typically, for multi-layer neural network, at the end, near the output, we don\\'t apply\\nReLU, especially when the output is not necessarily a positive number.',\n",
       " '87\\nneural networks could have 1000 layers, while some modern arti\\x0ccial neural\\nnetworks do (we will elaborate more on the notion of layers.) Moreover, it\\'s\\nan open question whether human brains update their neural networks in a\\nway similar to the way that computer scientists learn arti\\x0ccial neural net-\\nworks (using backpropagation, which we will introduce in the next section.).\\nTwo-layer Fully-Connected Neural Networks. We constructed the\\nneural network in equation (7.13) using a signi\\x0ccant amount of prior knowl-\\nedge/belief about how the \\\\family size\", \\\\walkable\", and \\\\school quality\" are\\ndetermined by the inputs. We implicitly assumed that we know the family\\nsize is an important quantity to look at and that it can be determined by\\nonly the \\\\size\" and \\\\# bedrooms\". Such a prior knowledge might not be\\navailable for other applications. It would be more \\rexible and general to have\\na generic parameterization. A simple way would be to write the intermediate\\nvariablea1as a function of all x1;:::;x 4:\\na1= ReLU(w>\\n1x+b1);wherew12R4andb12R (7.14)\\na2= ReLU(w>\\n2x+b2);wherew22R4andb22R\\na3= ReLU(w>\\n3x+b3);wherew32R4andb32R\\nWe still de\\x0cne \\x16h\\x12(x) using equation (7.13) with a1;a2;a3being de\\x0cned as\\nabove. Thus we have a so-called fully-connected neural network because\\nall the intermediate variables ai\\'s depend on all the inputs xi\\'s.\\nFor full generality, a two-layer fully-connected neural network with m\\nhidden units and ddimensional input x2Rdis de\\x0cned as\\n8j2[1;:::;m ]; zj=w[1]\\nj>x+b[1]\\njwherew[1]\\nj2Rd;b[1]\\nj2R (7.15)\\naj= ReLU(zj);\\na= [a1;:::;am]>2Rm\\n\\x16h\\x12(x) =w[2]>a+b[2]wherew[2]2Rm;b[2]2R; (7.16)\\nNote that by default the vectors in Rdare viewed as column vectors, and\\nin particular ais a column vector with components a1;a2;:::;am. The indices\\n[1]and[2]are used to distinguish two sets of parameters: the w[1]\\nj\\'s (each of\\nwhich is a vector in Rd) andw[2](which is a vector in Rm). We will have\\nmore of these later.\\nVectorization. Before we introduce neural networks with more layers and\\nmore complex structures, we will simplify the expressions for neural networks',\n",
       " \"88\\nwith more matrix and vector notations. Another important motivation of\\nvectorization is the speed perspective in the implementation. In order to\\nimplement a neural network e\\x0eciently, one must be careful when using for\\nloops. The most natural way to implement equation (7.15) in code is perhaps\\nto use a for loop. In practice, the dimensionalities of the inputs and hidden\\nunits are high. As a result, code will run very slowly if you use for loops.\\nLeveraging the parallelism in GPUs is/was crucial for the progress of deep\\nlearning.\\nThis gave rise to vectorization . Instead of using for loops, vectorization\\ntakes advantage of matrix algebra and highly optimized numerical linear\\nalgebra packages (e.g., BLAS) to make neural network computations run\\nquickly. Before the deep learning era, a for loop may have been su\\x0ecient\\non smaller datasets, but modern deep networks and state-of-the-art datasets\\nwill be infeasible to run with for loops.\\nWe vectorize the two-layer fully-connected neural network as below. We\\nde\\x0cne a weight matrix W[1]inRm\\x02das the concatenation of all the vectors\\nw[1]\\nj's in the following way:\\nW[1]=2\\n66664|w[1]\\n1>|\\n|w[1]\\n2>|\\n...\\n|w[1]\\nm>|3\\n777752Rm\\x02d(7.17)\\nNow by the de\\x0cnition of matrix vector multiplication, we can write z=\\n[z1;:::;zm]>2Rmas\\n2\\n6664z1\\n...\\n...\\nzm3\\n7775\\n|{z}\\nz2Rm\\x021=2\\n66664|w[1]\\n1>|\\n|w[1]\\n2>|\\n...\\n|w[1]\\nm>|3\\n77775\\n|{z}\\nW[1]2Rm\\x02d2\\n6664x1\\nx2\\n...\\nxd3\\n7775\\n|{z}\\nx2Rd\\x021+2\\n6664b[1]\\n1\\nb[1]\\n2...\\nb[1]\\nm3\\n7775\\n|{z}\\nb[1]2Rm\\x021(7.18)\\nOr succinctly,\\nz=W[1]x+b[1](7.19)\\nWe remark again that a vector in Rdin this notes, following the conventions\\npreviously established, is automatically viewed as a column vector, and can\",\n",
       " '89\\nalso be viewed as a d\\x021 dimensional matrix. (Note that this is di\\x0berent\\nfrom numpy where a vector is viewed as a row vector in broadcasting.)\\nComputing the activations a2Rmfromz2Rminvolves an element-\\nwise non-linear application of the ReLU function, which can be computed in\\nparallel e\\x0eciently. Overloading ReLU for element-wise application of ReLU\\n(meaning, for a vector t2Rd, ReLU(t) is a vector such that ReLU( t)i=\\nReLU(ti)), we have\\na= ReLU(z) (7.20)\\nDe\\x0cneW[2]= [w[2]>]2R1\\x02msimilarly. Then, the model in equa-\\ntion (7.16) can be summarized as\\na= ReLU(W[1]x+b[1])\\n\\x16h\\x12(x) =W[2]a+b[2](7.21)\\nHere\\x12consists of W[1];W[2](often referred to as the weight matrices) and\\nb[1];b[2](referred to as the biases). The collection of W[1];b[1]is referred to as\\nthe \\x0crst layer, and W[2];b[2]the second layer. The activation ais referred to as\\nthe hidden layer. A two-layer neural network is also called one-hidden-layer\\nneural network.\\nMulti-layer fully-connected neural networks. With this succinct no-\\ntations, we can stack more layers to get a deeper fully-connected neu-\\nral network. Let rbe the number of layers (weight matrices). Let\\nW[1];:::;W[r];b[1];:::;b[r]be the weight matrices and biases of all the layers.\\nThen a multi-layer neural network can be written as\\na[1]= ReLU(W[1]x+b[1])\\na[2]= ReLU(W[2]a[1]+b[2])\\n\\x01\\x01\\x01\\na[r\\x001]= ReLU(W[r\\x001]a[r\\x002]+b[r\\x001])\\n\\x16h\\x12(x) =W[r]a[r\\x001]+b[r](7.22)\\nWe note that the weight matrices and biases need to have compatible\\ndimensions for the equations above to make sense. If a[k]has dimension mk,\\nthen the weight matrix W[k]should be of dimension mk\\x02mk\\x001, and the bias\\nb[k]2Rmk. Moreover, W[1]2Rm1\\x02dandW[r]2R1\\x02mr\\x001.',\n",
       " '90\\nThe total number of neurons in the network is m1+\\x01\\x01\\x01+mr, and the\\ntotal number of parameters in this network is ( d+ 1)m1+ (m1+ 1)m2+\\x01\\x01\\x01+\\n(mr\\x001+ 1)mr.\\nSometimes for notational consistency we also write a[0]=x, anda[r]=\\nh\\x12(x). Then we have simple recursion that\\na[k]= ReLU(W[k]a[k\\x001]+b[k]);8k= 1;:::;r\\x001 (7.23)\\nNote that this would have be true for k=rif there were an additional\\nReLU in equation (7.22), but often people like to make the last layer linear\\n(aka without a ReLU) so that negative outputs are possible and it\\'s easier\\nto interpret the last layer as a linear model. (More on the interpretability at\\nthe \\\\connection to kernel method\" paragraph of this section.)\\nOther activation functions. The activation function ReLU can be re-\\nplaced by many other non-linear function \\x1b(\\x01) that maps RtoRsuch as\\n\\x1b(z) =1\\n1 +e\\x00z(sigmoid) (7.24)\\n\\x1b(z) =ez\\x00e\\x00z\\nez+e\\x00z(tanh) (7.25)\\n\\x1b(z) = maxfz;\\rzg;\\r2(0;1) (leaky ReLU) (7.26)\\n\\x1b(z) =z\\n2\\x14\\n1 + erf(zp\\n2)\\x15\\n(GELU) (7.27)\\n\\x1b(z) =1\\n\\x0clog(1 + exp( \\x0cz));\\x0c > 0 (Softplus) (7.28)\\nThe activation functions are plotted in Figure 7.3. Sigmoid and tanh are\\nless and less used these days partly because their are bounded from both sides\\nand the gradient of them vanishes as zgoes to both positive and negative\\nin\\x0cnity (whereas all the other activation functions still have gradients as the\\ninput goes to positive in\\x0cnity.) Softplus is not used very often either in\\npractice and can be viewed as a smoothing of the ReLU so that it has a\\nproper second order derivative. GELU and leaky ReLU are both variants of\\nReLU but they have some non-zero gradient even when the input is negative.\\nGELU (or its slight variant) is used in NLP models such as BERT and GPT\\n(which we will discuss in Chapter 14.)\\nWhy do we not use the identity function for \\x1b(z)?That is, why\\nnot use\\x1b(z) =z? Assume for sake of argument that b[1]andb[2]are zeros.',\n",
       " '91\\nFigure 7.3: Activation functions in deep learning.\\nSuppose\\x1b(z) =z, then for two-layer neural network, we have that\\n\\x16h\\x12(x) =W[2]a[1](7.29)\\n=W[2]\\x1b(z[1]) by de\\x0cnition (7.30)\\n=W[2]z[1]since\\x1b(z) =z (7.31)\\n=W[2]W[1]x from Equation (7.18) (7.32)\\n=~Wx where ~W=W[2]W[1](7.33)\\nNotice how W[2]W[1]collapsed into ~W.\\nThis is because applying a linear function to another linear function will\\nresult in a linear function over the original input (i.e., you can construct a ~W\\nsuch that ~Wx=W[2]W[1]x). This loses much of the representational power\\nof the neural network as often times the output we are trying to predict\\nhas a non-linear relationship with the inputs. Without non-linear activation\\nfunctions, the neural network will simply perform linear regression.\\nConnection to the Kernel Method. In the previous lectures, we covered\\nthe concept of feature maps. Recall that the main motivation for feature\\nmaps is to represent functions that are non-linear in the input xby\\x12>\\x1e(x),\\nwhere\\x12are the parameters and \\x1e(x), the feature map, is a handcrafted\\nfunction non-linear in the raw input x. The performance of the learning\\nalgorithms can signi\\x0ccantly depends on the choice of the feature map \\x1e(x).\\nOftentimes people use domain knowledge to design the feature map \\x1e(x) that',\n",
       " '92\\nsuits the particular applications. The process of choosing the feature maps\\nis often referred to as feature engineering .\\nWe can view deep learning as a way to automatically learn the right\\nfeature map (sometimes also referred to as \\\\the representation\") as follows.\\nSuppose we denote by \\x0cthe collection of the parameters in a fully-connected\\nneural networks (equation (7.22)) except those in the last layer. Then we\\ncan abstract right a[r\\x001]as a function of the input xand the parameters in\\n\\x0c:a[r\\x001]=\\x1e\\x0c(x). Now we can write the model as\\n\\x16h\\x12(x) =W[r]\\x1e\\x0c(x) +b[r](7.34)\\nWhen\\x0cis \\x0cxed, then \\x1e\\x0c(\\x01) can viewed as a feature map, and therefore \\x16h\\x12(x)\\nis just a linear model over the features \\x1e\\x0c(x). However, we will train the\\nneural networks, both the parameters in \\x0cand the parameters W[r];b[r]are\\noptimized, and therefore we are not learning a linear model in the feature\\nspace, but also learning a good feature map \\x1e\\x0c(\\x01) itself so that it\\'s possi-\\nble to predict accurately with a linear model on top of the feature map.\\nTherefore, deep learning tends to depend less on the domain knowledge of\\nthe particular applications and requires often less feature engineering. The\\npenultimate layer a[r]is often (informally) referred to as the learned features\\nor representations in the context of deep learning.\\nIn the example of house price prediction, a fully-connected neural network\\ndoes not need us to specify the intermediate quantity such \\\\family size\", and\\nmay automatically discover some useful features in the last penultimate layer\\n(the activation a[r\\x001]), and use them to linearly predict the housing price.\\nOften the feature map / representation obtained from one datasets (that is,\\nthe function \\x1e\\x0c(\\x01) can be also useful for other datasets, which indicates they\\ncontain essential information about the data. However, oftentimes, the neural\\nnetwork will discover complex features which are very useful for predicting\\nthe output but may be di\\x0ecult for a human to understand or interpret. This\\nis why some people refer to neural networks as a black box , as it can be\\ndi\\x0ecult to understand the features it has discovered.\\n7.3 Modules in Modern Neural Networks\\nThe multi-layer neural network introduced in equation (7.22) of Section 7.2\\nis often called multi-layer perceptron (MLP) these days. Modern neural net-\\nworks used in practice are often much more complex and consist of multiple\\nbuilding blocks or multiple layers of building blocks. In this section, we will',\n",
       " '93\\nintroduce some of the other building blocks and discuss possible ways to\\ncombine them.\\nFirst, each matrix multiplication can be viewed as a building block. Con-\\nsider a matrix multiplication operation with parameters ( W;b) whereWis\\nthe weight matrix and bis the bias vector, operating on an input z,\\nMMW;b(z) =Wz+b: (7.35)\\nNote that we implicitly assume all the dimensions are chosen to be compat-\\nible. We will also drop the subscripts under MM when they are clear in the\\ncontext or just for convenience when they are not essential to the discussion.\\nThen, the MLP can be written as as a composition of multiple matrix\\nmultiplication modules and nonlinear activation modules (which can also be\\nviewed as a building block):\\nMLP(x) = MMW[r];b[r](\\x1b(MMW[r\\x001];b[r\\x001](\\x1b(\\x01\\x01\\x01MMW[1];b[1](x)))):(7.36)\\nAlternatively, when we drop the subscripts that indicate the parameters for\\nconvenience, we can write\\nMLP(x) = MM(\\x1b(MM\\x1b(\\x01\\x01\\x01MM(x)))): (7.37)\\nNote that in this lecture notes, by default, all the modules have di\\x0berent\\nsets of parameters, and the dimensions of the parameters are chosen such\\nthat the composition is meaningful.\\nLarger modules can be de\\x0cned via smaller modules as well, e.g., one\\nactivation layer \\x1band a matrix multiplication layer MM are often combined\\nand called a \\\\layer\" in many papers. People often draw the architecture\\nwith the basic modules in a \\x0cgure by indicating the dependency between\\nthese modules. E.g., see an illustration of an MLP in Figure 7.4, Left.\\nResidual connections. One of the very in\\ruential neural network archi-\\ntecture for vision application is ResNet, which uses the residual connections\\nthat are essentially used in almost all large-scale deep learning architectures\\nthese days. Using our notation above, a very much simpli\\x0ced residual block\\ncan be de\\x0cned as\\nRes(z) =z+\\x1b(MM(\\x1b(MM(z)))): (7.38)\\nA much simpli\\x0ced ResNet is a composition of many residual blocks followed\\nby a matrix multiplication,\\nResNet-S(x) = MM(Res(Res( \\x01\\x01\\x01Res(x)))): (7.39)',\n",
       " '94\\n𝑥Layer 𝑟−1Layer 𝑖...Layer 1MLP(𝑥)...Layer𝑖MM![\"],#[\"]𝜎MM![$],#[$]\\n𝑥ResRes...ResResNet-S(𝑥)...Res\\nMM𝜎MM𝜎\\nFigure 7.4: Illustrative Figures for Architecture. Left: An MLP with r\\nlayers. Right : A residual network.\\nWe also draw the dependency of these modules in Figure 7.4, Right.\\nWe note that the ResNet-S is still not the same as the ResNet architec-\\nture introduced in the seminal paper [He et al., 2016] because ResNet uses\\nconvolution layers instead of vanilla matrix multiplication, and adds batch\\nnormalization between convolutions and activations. We will introduce con-\\nvolutional layers and some variants of batch normalization below. ResNet-S\\nand layer normalization are part of the Transformer architecture that are\\nwidely used in modern large language models.\\nLayer normalization. Layer normalization, denoted by LN in this text,\\nis a module that maps a vector z2Rmto a more normalized vector LN( z)2\\nRm. It is oftentimes used after the nonlinear activations.\\nWe \\x0crst de\\x0cne a sub-module of the layer normalization, denoted by LN-S.\\nLN-S(z) =2\\n6664z1\\x00^\\x16\\n^\\x1bz2\\x00^\\x16\\n^\\x1b...\\nzm\\x00^\\x16\\n^\\x1b3\\n7775; (7.40)\\nwhere ^\\x16=Pm\\ni=1zi\\nmis the empirical mean of the vector zand ^\\x1b=qPm\\ni=1(zi\\x00^\\x162)\\nm\\nis the empirical standard deviation of the entries of z.4Intuitively, LN-S( z)\\nis a vector that is normalized to having empirical mean zero and empirical\\nstandard deviation 1.\\n4Note that we divide by minstead ofm\\x001 in the empirical standard deviation here\\nbecause we are interested in making the output of LN-S( z) have sum of squares equal to\\n1 (as opposed to estimating the standard deviation in statistics.)',\n",
       " \"95\\nOftentimes zero mean and standard deviation 1 is not the most desired\\nnormalization scheme, and thus layernorm introduces to parameters learnable\\nscalars\\x0cand\\ras the desired mean and standard deviation, and use an a\\x0ene\\ntransformation to turn the output of LN-S( z) into a vector with mean \\x0cand\\nstandard deviation \\r.\\nLN(z) =\\x0c+\\r\\x01LN-S(z) =2\\n6664\\x0c+\\r\\x00z1\\x00^\\x16\\n^\\x1b\\x01\\n\\x0c+\\r\\x00z2\\x00^\\x16\\n^\\x1b\\x01\\n...\\n\\x0c+\\r\\x00zm\\x00^\\x16\\n^\\x1b\\x013\\n7775: (7.41)\\nHere the \\x0crst occurrence of \\x0cshould be technically interpreted as a vector\\nwith all the entries being \\x0c. in We also note that ^ \\x16and ^\\x1bare also functions\\nofzand shouldn't be treated as constants when computing the derivatives of\\nlayernorm. Moreover, \\x0cand\\rare learnable parameters and thus layernorm\\nis a parameterized module (as opposed to the activation layer which doesn't\\nhave any parameters.)\\nScaling-invariant property. One important property of layer normalization\\nis that it will make the model invariant to scaling of the parameters in the\\nfollowing sense. Suppose we consider composing LN with MM W;band get\\na subnetwork LN(MM W;b(z)). Then, we have that the output of this sub-\\nnetwork does not change when the parameter in MM W;bis scaled:\\nLN(MM \\x0bW;\\x0bb (z)) = LN(MM W;b(z));8\\x0b>0: (7.42)\\nTo see this, we \\x0crst know that LN-S( \\x01) is scale-invariant\\nLN-S(\\x0bz) =2\\n6664\\x0bz1\\x00\\x0b^\\x16\\n\\x0b^\\x1b\\x0bz2\\x00\\x0b^\\x16\\n\\x0b^\\x1b...\\n\\x0bzm\\x00\\x0b^\\x16\\n\\x0b^\\x1b3\\n7775=2\\n6664z1\\x00^\\x16\\n^\\x1bz2\\x00^\\x16\\n^\\x1b...\\nzm\\x00^\\x16\\n^\\x1b3\\n7775= LN-S(z): (7.43)\\nThen we have\\nLN(MM \\x0bW;\\x0bb (z)) =\\x0c+\\rLN-S(MM \\x0bW;\\x0bb (z)) (7.44)\\n=\\x0c+\\rLN-S(\\x0bMMW;b(z)) (7.45)\\n=\\x0c+\\rLN-S(MM W;b(z)) (7.46)\\n= LN(MM W;b(z)): (7.47)\\nDue to this property, most of the modern DL architectures for large-scale\\ncomputer vision and language applications have the following scale-invariant\",\n",
       " \"96\\nproperty w.r.t all the weights that are not at the last layer. Suppose the\\nnetworkfhas last layer' weights Wlast, and all the rest of the weights are\\ndenote byW. Then, we have fWlast;\\x0bW(x) =fWlast;W(x) for all\\x0b>0. Here,\\nthe last layers weights are special because there are typically no layernorm\\nor batchnorm after the last layer's weights.\\nOther normalization layers. There are several other normalization layers that\\naim to normalize the intermediate layers of the neural networks to a more\\n\\x0cxed and controllable scaling, such as batch-normalization [Io\\x0be and Szegedy,\\n2015], and group normalization [Wu and He, 2018]. Batch normalization and\\ngroup normalization are more often used in computer vision applications\\nwhereas layer norm is used more often in language applications.\\nConvolutional Layers. Convolutional Neural Networks are neural net-\\nworks that consist of convolution layers (and many other modules), and are\\nparticularly useful for computer vision applications. For the simplicity of\\nexposition, we focus on 1-D convolution in this text and only brie\\ry mention\\n2-D convolution informally at the end of this subsection. (2-D convolution\\nis more suitable for images which have two dimensions. 1-D convolution is\\nalso used in natural language processing.)\\nWe start by introducing a simpli\\x0ced version of the 1-D convolution layer,\\ndenoted by Conv1D-S( \\x01) which is a type of matrix multiplication layer with\\na special structure. The parameters of Conv1D-S are a \\x0clter vector w2Rk\\nwherekis called the \\x0clter size (oftentimes k\\x1cm), and a bias scalar b.\\nOftentimes the \\x0clter is also called a kernel (but it does not have much to do\\nwith the kernel in kernel method.) For simplicity, we assume k= 2`+ 1 is\\nan odd number. We \\x0crst pad zeros to the input vector zin the sense that we\\nletz1\\x00`=z1\\x00`+1=::=z0= 0 andzm+1=zm+2=::=zm+`= 0, and treat\\nzas an (m+ 2`)-dimension vector. Conv1D-S outputs a vector of dimension\\nRmwhere each output dimension is a linear combination of subsets of zj's\\nwith coe\\x0ecients from w,\\nConv1D-S(z)i=w1zi\\x00`+w2zi\\x00`+1+\\x01\\x01\\x01+w2`+1zi+`=2`+1X\\nj=1wjzi\\x00`+(j\\x001):\\n(7.48)\\nTherefore, one can view Conv1D-S as a matrix multiplication with shared\",\n",
       " '97\\nparameters: Conv1D-S( z) =Qz, where\\nQ=2\\n666666666666666666664w`+1\\x01\\x01\\x01w2`+1 0 0\\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 0\\nw`\\x01\\x01\\x01w2`w2`+1 0\\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 0\\n...\\nw1\\x01\\x01\\x01w`+1\\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 w2`+1 0\\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 0\\n0w1\\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 w2`w2`+1 0\\x01\\x01\\x01 \\x01\\x01\\x01 0\\n...\\n...\\n0\\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 0w1\\x01\\x01\\x01 \\x01\\x01\\x01 w2`+1\\n...\\n0\\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 \\x01\\x01\\x01 0w1\\x01\\x01\\x01w`+13\\n777777777777777777775:(7.49)\\nNote thatQi;j=Qi\\x001;j\\x001for alli;j2f2;:::;mg, and thus convoluation is a\\nmatrix multiplication with parameter sharing. We also note that computing\\nthe convolution only takes O(km) times but computing a generic matrix\\nmultiplication takes O(m2) time. Convolution has kparameters but generic\\nmatrix multiplication will have m2parameters. Thus convolution is supposed\\nto be much more e\\x0ecient than a generic matrix multiplication (as long as\\nthe additional structure imposed does not hurt the \\rexibility of the model\\nto \\x0ct the data).\\nWe also note that in practice there are many variants of the convolutional\\nlayers that we de\\x0cne here, e.g., there are other ways to pad zeros or sometimes\\nthe dimension of the output of the convolutional layers could be di\\x0berent from\\nthe input. We omit some of this subtleties here for simplicity.\\nThe convolutional layers used in practice have also many \\\\channels\" and\\nthe simpli\\x0ced version above corresponds to the 1-channel version. Formally,\\nConv1D takes in Cvectorsz1;:::;zC2Rmas inputs, where Cis referred\\nto as the number of channels. In other words, the more general version,\\ndenoted by Conv1D, takes in a matrix as input, which is the concatenation\\nofz1;:::;zCand has dimension m\\x02C. It can output C0vectors of dimension\\nm, denoted by Conv1D( z)1;:::; Conv1D(z)C0, whereC0is referred to as the\\noutput channel, or equivalently a matrix of dimension m\\x02C0. Each of the\\noutput is a sum of the simpli\\x0ced convolutions applied on various channels.\\n8i2[C0];Conv1D(z)i=CX\\nj=1Conv1D-S i;j(zj): (7.50)\\nNote that each Conv1D-S i;jare modules with di\\x0berent parameters, and\\nthus the total number of parameters is k(the number of parameters in a\\nConv1D-S)\\x02CC0(the number of Conv1D-S i:j\\'s) =kCC0. In contrast, a\\ngeneric linear mapping from Rm\\x02CandRm\\x02C0hasm2CC0parameters. The',\n",
       " '98\\nparameters can also be represented as a three-dimensional tensor of dimen-\\nsionk\\x02C\\x02C0.\\n2-D convolution (brief). A 2-D convolution with one channel, denoted by\\nConv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional input\\nz2Rm\\x02mand applies a \\x0clter of size k\\x02k, and outputs Conv2D-S( z)2\\nRm\\x02m. The full 2-D convolutional layer, denoted by Conv2D, takes in\\na sequence of matrices z1;:::;zC2Rm\\x02m, or equivalently a 3-D ten-\\nsorz= (z1;:::;zC)2Rm\\x02m\\x02Cand outputs a sequence of matrices,\\nConv2D(z)1;:::; Conv2D(z)C02Rm\\x02m, which can also be viewed as a 3D\\ntensor in Rm\\x02m\\x02C0. Each channel of the output is sum of the outcomes of\\napplying Conv2D-S layers on all the input channels.\\n8i2[C0];Conv2D(z)i=CX\\nj=1Conv2D-S i;j(zj): (7.51)\\nBecause there are CC0number of Conv2D-S modules and each of the\\nConv2D-S module has k2parameters, the total number of parameters is\\nCC0k2. The parameters can also be viewed as a 4D tensor of dimension\\nC\\x02C0\\x02k\\x02k.\\n7.4 Backpropagation\\nIn this section, we introduce backpropgation or auto-di\\x0berentiation, which\\ncomputes the gradient of the loss rJ(\\x12) e\\x0eciently. We will start with an\\ninformal theorem that states that as long as a real-valued function fcan be\\ne\\x0eciently computed/evaluated by a di\\x0berentiable network or circuit, then its\\ngradient can be e\\x0eciently computed in a similar time. We will then show\\nhow to do this concretely for neural networks.\\nBecause the formality of the general theorem is not the main focus here,\\nwe will introduce the terms with informal de\\x0cnitions. By a di\\x0berentiable\\ncircuit or a di\\x0berentiable network, we mean a composition of a sequence of\\ndi\\x0berentiable arithmetic operations (additions, subtraction, multiplication,\\ndivisions, etc) and elementary di\\x0berentiable functions (ReLU, exp, log, sin,\\ncos, etc.). Let the size of the circuit be the total number of such operations\\nand elementary functions. We assume that each of the operations and func-\\ntions, and their derivatives or partial derivatives ecan be computed in O(1)\\ntime.\\nTheorem 7.4.1: [backpropagation or auto-di\\x0berentiation, informally stated]\\nSuppose a di\\x0berentiable circuit of size Ncomputes a real-valued function',\n",
       " '99\\nf:R`!R. Then, the gradient rfcan be computed in time O(N), by a\\ncircuit of size O(N).5\\nWe note that the loss function J(j)(\\x12) forj-th example can be indeed\\ncomputed by a sequence of operations and functions involving additions,\\nsubtraction, multiplications, and non-linear activations. Thus the theorem\\nsuggests that we should be able to compute the rJ(j)(\\x12) in a similar time\\nto that for computing J(j)(\\x12) itself. This does not only apply to the fully-\\nconnected neural network introduced in the Section 7.2, but also many other\\ntypes of neural networks that uses more advance modules.\\nWe remark that auto-di\\x0berentiation or backpropagation is already imple-\\nmented in all the deep learning packages such as tensor\\row and pytorch, and\\nthus in practice, in most of cases a researcher does not need to write their\\nbackpropagation algorithms. However, understanding it is very helpful for\\ngaining insights into the working of deep learning.\\nOrganization of the rest of the section. In Section 7.4.1, we will start review-\\ning the basic Chain rule with a new perspective that is particularly useful\\nfor understanding backpropgation. Section 7.4.2 will introduce the general\\nstrategy for backpropagation. Section 7.4.2 will discuss how to compute the\\nso-called backward function for basic modules used in neural networks, and\\nSection 7.4.4 will put everything together to get a concrete backprop algo-\\nrithm for MLPs.\\n7.4.1 Preliminaries on partial derivatives\\nSuppose a scalar variable Jdepend on some variables z(which could be a\\nscalar, matrix, or high-order tensor), we write@J\\n@zas the partial derivatives\\nofJw.r.t to the variable z. We stress that the convention here is that@J\\n@z\\nhas exactly the same dimension as zitself. For example, if z2Rm\\x02n, then\\n@J\\n@z2Rm\\x02n, and the (i;j)-entry of@J\\n@zis equal to@J\\n@zij.\\nRemark 7.4.2: When both Jandzare not scalars, the partial derivatives of\\nJw.r.tzbecomes either a matrix or tensor and the notation becomes some-\\nwhat tricky. Besides the mathematical or notational challenges in dealing\\n5We note if the output of the function fdoes not depend on some of the input co-\\nordinates, then we set by default the gradient w.r.t that coordinate to zero. Setting to\\nzero does not count towards the total runtime here in our accounting scheme. This is why\\nwhenN\\x14`, we can compute the gradient in O(N) time, which might be potentially even\\nless than`.',\n",
       " \"100\\nwith these partial derivatives of multi-variate functions, they are also expen-\\nsive to compute and store, and thus rarely explicitly constructed empirically.\\nThe experience of authors of this note is that it's generally more productive\\nto think only about derivatives of scalar function w.r.t to vector, matrices,\\nor tensors. For example, in this note, we will not deal with derivatives of\\nmulti-variate functions.\\nChain rule. We review the chain rule in calculus but with a perspective\\nand notions that are more relevant for auto-di\\x0berentiation.\\nConsider a scalar variable Jwhich is obtained by the composition of f\\nandgon some variable z,\\nz2Rm\\nu=g(z)2Rn\\nJ=f(u)2R: (7.52)\\nThe same derivations below can be easily extend to the cases when zandu\\nare matrices or tensors; but we insist that the \\x0cnal variable Jis a scalar. (See\\nalso Remark 7.4.2.) Let u= (u1;:::;un) and letg(z) = (g1(z);\\x01\\x01\\x01;gn(z)):\\nThen, the standard chain rule gives us that\\n8i2f1;:::;mg;@J\\n@zi=nX\\nj=1@J\\n@uj\\x01@gj\\n@zi: (7.53)\\nAlternatively, when zanduare both vectors, in a vectorized notation:\\n@J\\n@z=2\\n64@g1\\n@z1\\x01\\x01\\x01@gn\\n@z1.........\\n@g1\\n@zm\\x01\\x01\\x01@gn\\n@zm3\\n75\\x01@J\\n@u: (7.54)\\nIn other words, the backward function is always a linear map from@J\\n@uto\\n@J\\n@z, though note that the mapping itself can depend on zin complex ways.\\nThe matrix on the RHS of (7.54) is actually the transpose of the Jacobian\\nmatrix of the function g. However, we do not discuss in-depth about Jacobian\\nmatrices to avoid complications. Part of the reason is that when zis a matrix\\n(or tensor), to write an analog of equation (7.54), one has to either \\ratten z\\ninto a vector or introduce additional notations on tensor-matrix product. In\\nthis sense, equation (7.53) is more convenient and e\\x0bective to use in all cases.\\nFor example, when z2Rr\\x02sis a matrix, we can easily rewrite equation (7.53)\",\n",
       " \"101\\nto\\n8i;k;@J\\n@zik=nX\\nj=1@J\\n@uj\\x01@gj\\n@zik: (7.55)\\nwhich will indeed be used in some of the derivations in Section 7.4.3.\\nKey interpretation of the chain rule. We can view the formula above (equa-\\ntion (7.53) or (7.54)) as a way to compute@J\\n@zfrom@J\\n@u. Consider the following\\nabstract problem. Suppose Jdepends on zviauas de\\x0cned in equation (7.52).\\nHowever, suppose the function fis not given or the function fis complex,\\nbut we are given the value of@J\\n@u. Then, the formula in equation (7.54) gives\\nus a way to compute@J\\n@zfrom@J\\n@u.\\n@J\\n@uchain rule, formula (7.54)= = = = = = = = = = = = = = = = = = = = )\\nonly requires info about g(\\x01) andz@J\\n@z: (7.56)\\nMoreover, this formula only involves knowledge about g(more precisely@gj\\n@zi).\\nWe will repeatedly use this fact in situations where gis a building blocks of\\na complex network f.\\nEmpirically, it's often useful to modularized the mapping in (7.53) or\\n(7.54) into a black-box, and mathematically it's also convenient to de\\x0cne a\\nnotation for it.6We useB[g;z] to de\\x0cne the function that maps@J\\n@uto@J\\n@z,\\nand write\\n@J\\n@z=B[g;z]\\x12@J\\n@u\\x13\\n: (7.57)\\nWe callB[g;z] thebackward function for the module g. Note that when z\\nis \\x0cxed,B[g;z] is merely a linear map from RntoRm. Using equation (7.53),\\nwe have\\n(B[g;z](v))i=mX\\nj=1@gj\\n@zi\\x01vj: (7.58)\\nOr in vectorized notation, using (7.54), we have\\nB[g;z](v) =2\\n64@g1\\n@z1\\x01\\x01\\x01@gn\\n@z1.........\\n@g1\\n@zm\\x01\\x01\\x01@gn\\n@zm3\\n75\\x01v: (7.59)\\n6e.g., the function is the .backward() method of the module in pytorch.\",\n",
       " \"102\\nand thereforeB[g;z] can be viewed as a matrix. However, in reality, zwill be\\nchanging and thus the backward mapping has to be recomputed for di\\x0berent\\nz's whilegis often \\x0cxed. Thus, empirically, the backward function B[g;z](v)\\nis often viewed as a function which takes in z(=the input to g) andv(=a\\nvector that is supposed to be the gradient of some variable Jw.r.t to the\\noutput ofg) as the inputs, and outputs a vector that is supposed to be the\\ngradient of Jw.r.t toz.\\n7.4.2 General strategy of backpropagation\\nWe discuss the general strategy of auto-di\\x0berentiation in this section to build\\na high-level understanding. Then, we will instantiate the approach to con-\\ncrete neural networks. We take the viewpoint that neural networks are com-\\nplex compositions of small building blocks such as MM, \\x1b, Conv2D, LN,\\netc., de\\x0cned in Section 7.3. Note that the losses (e.g., mean-squared loss, or\\nthe cross-entropy loss) can also be abstractly viewed as additional modules.\\nThus, we can abstractly write the loss function J(on a single example ( x;y))\\nas a composition of many modules:7\\nJ=Mk(Mk\\x001(\\x01\\x01\\x01M1(x))): (7.60)\\nFor example, for a binary classi\\x0ccation problem with a MLP \\x16h\\x12(x) (de-\\n\\x0cned in equation (7.36) and (7.37)), the loss function has ber written in the\\nform of equation (7.60) with M1= MMW[1];b[1],M2=\\x1b,M3= MMW[2];b[2],\\n:::, andMk\\x001= MMW[r];b[r]andMk=`logistic .\\nWe can see from this example that some modules involve parameters, and\\nother modules might only involve a \\x0cxed set of operations. For generality,\\nwe assume that eachj Miinvolves a set of parameters \\x12[i], though\\x12[i]could\\npossibly be an empty set when Miis a \\x0cxed operation such as the nonlinear\\nactivations. We will discuss more on the granularity of the modularization,\\nbut so far we assume all the modules Mi's are simple enough.\\nWe introduce the intermediate variables for the computation in (7.60).\\n7Technically, we should write J=Mk(Mk\\x001(\\x01\\x01\\x01M1(x));y). However, yis treated as a\\nconstant for the purpose of computing the derivatives w.r.t to the parameters, and thus\\nwe can view it as part of Mkfor the sake of simplicity of notations.\",\n",
       " \"103\\nLet\\nu[0]=x\\nu[1]=M1(u[0])\\nu[2]=M2(u[1])\\n...\\nJ=u[k]=Mk(u[k\\x001]): (F)\\nBackpropgation consists of two passes, the forward pass and backward\\npass. In the forward pass, the algorithm simply computes u[1];:::;u[k]from\\ni= 1;:::;k , sequentially using the de\\x0cnition in (F), and save all the in-\\ntermediate variables u[i]'s in the memory.\\nIn the backward pass , we \\x0crst compute the derivatives w.r.t to the\\nintermediate variables, that is,@J\\n@u[k];:::;@J\\n@u[1], sequentially in this backward\\norder, and then compute the derivatives of the parameters@J\\n@\\x12[i]from@J\\n@u[i]and\\nu[i\\x001]. These two type of computations can be also interleaved with each\\nother because@J\\n@\\x12[i]only depends on@J\\n@u[i]andu[i\\x001]but not any@J\\n@u[k]with\\nk<i .\\nWe \\x0crst see why@J\\n@u[i\\x001]can be computed e\\x0eciently from@J\\n@u[i]andu[i\\x001]\\nby invoking the discussion in Section 7.4.1 on the chain rule. We in-\\nstantiate the discussion by setting u=u[i]andz=u[i\\x001], andf(u) =\\nMk(Mk\\x001(\\x01\\x01\\x01Mi+1(u[i]))), andg(\\x01) =Mi(\\x01). Note that fis very complex\\nbut we don't need any concrete information about f. Then, the conclusive\\nequation (7.56) corresponds to\\n@J\\n@u[i]chain rule= = = = = = = = = = = = = = = = = = = = = = = = = = )\\nonly requires info about Mi(\\x01)andu[i\\x001]@J\\n@u[i\\x001]: (7.61)\\nMore precisely, we can write, following equation (7.57)\\n@J\\n@u[i\\x001]=B[Mi;u[i\\x001]]\\x12@J\\n@u[i]\\x13\\n: (B1)\\nInstantiating the chain rule with z=\\x12[i]andu=u[i], we also have\\n@J\\n@\\x12[i]=B[Mi;\\x12[i]]\\x12@J\\n@u[i]\\x13\\n: (B2)\\nSee Figure 7.5 for an illustration of the algorithm.\",\n",
       " '104\\n𝑥...𝑀!𝐽...𝑀\"\\n𝑢[!]𝑢[\"%!]𝜕𝐽𝜕𝐽ℬ[𝑀\",𝑢[\"%!]]𝜕𝐽𝜕𝑢!\"#𝑢[&]𝑀&𝑢[&%!]ℬ[𝑀&,𝑢[&%!]]𝜕𝐽𝜕𝑢$\"#𝜕𝐽𝜕𝑢$...\\n𝜕𝐽𝜕𝑢#...\\nForward passBackward passℬ[𝑀!,𝜃!]𝜕𝐽𝜕𝜃#ℬ[𝑀&,𝜃&]𝜕𝐽𝜕𝜃$ℬ[𝑀\",𝜃\"]𝜕𝐽𝜕𝜃!\\nFigure 7.5: Back-propagation.\\nRemark 7.4.3: [Computational e\\x0eciency and granularity of the modules]\\nThe main underlying purpose of treating a complex network as compositions\\nof small modules is that small modules tend to have e\\x0eciently implementable\\nbackward function. In fact, the backward functions of all the atomic modules\\nsuch as addition, multiplication and ReLU can be computed as e\\x0eciently as\\nthe the evaluation of these modules (up to multiplicative constant factor).\\nUsing this fact, we can prove Theorem 7.4.1 by viewing neural networks as\\ncompositions of many atomic operations, and invoking the backpropagation\\ndiscussed above. However, in practice, it\\'s oftentimes more convenient to\\nmodularize the networks using modules on the level of matrix multiplication,\\nlayernorm, etc. As we will see, naive implementation of these operations\\'\\nbackward functions also have the same runtime as the evaluation of these\\nfunctions.',\n",
       " \"105\\n7.4.3 Backward functions for basic modules\\nUsing the general strategy in Section 7.4.2, it su\\x0eces to compute the back-\\nward function for all modules Mi's used in the networks. We compute the\\nbackward function for the basic module MM, activations \\x1b, and loss functions\\nin this section.\\nBackward function for MM.Suppose MM W;b(z) =Wz+bis a matrix multi-\\nplication module where z2RmandW2Rn\\x02m. Then, using equation (7.59),\\nwe have for v2Rn\\nB[MM;z](v) =2\\n64@(Wz+b)1\\n@z1\\x01\\x01\\x01@(Wz+b)n\\n@z1.........\\n@(Wz+b)1\\n@zm\\x01\\x01\\x01@(Wz+b)n\\n@zm3\\n75v: (7.62)\\nUsing the fact that 8i2[m];j2[n],@(Wz+b)j\\n@zi=@bj+Pm\\nk=1Wjkzk\\n@zi=Wji, we\\nhave\\nB[MM;z](v) =W>v2Rm: (7.63)\\nIn the derivation above, we have treated MM as a function of z. If we treat\\nMM as a function of Wandb, then we can also compute the backward\\nfunction for the parameter variables Wandb. It's less convenient to use\\nequation (7.59) because the variable Wis a matrix and the matrix in (7.59)\\nwill be a 4-th order tensor that is challenging for us to mathematically write\\ndown. We use (7.58) instead:\\n(B[MM;W](v))ij=mX\\nk=1@(Wz+b)k\\n@Wij\\x01vk=mX\\nk=1@Pm\\ns=1Wkszs\\n@Wij\\x01vk=vizj:\\n(7.64)\\nIn vectorized notation, we have\\nB[MM;W](v) =vz>2Rn\\x02\\x02m: (7.65)\\nUsing equation (7.59) for the variable b, we have,\\nB[MM;b](v) =2\\n64@(Wz+b)1\\n@b1\\x01\\x01\\x01@(Wz+b)n\\n@b1.........\\n@(Wz+b)1\\n@bn\\x01\\x01\\x01@(Wz+b)n\\n@bn3\\n75v=v: (7.66)\",\n",
       " \"106\\nHere we used that@(Wz+b)j\\n@bi= 0 ifi6=jand@(Wz+b)j\\n@bi= 1 ifi=j.\\nThe computational e\\x0eciency for computing the backward function is\\nO(mn), the same as evaluating the result of matrix multiplication up to\\nconstant factor.\\nBackward function for the activations. SupposeM(z) =\\x1b(z) where\\x1bis an\\nelement-wise activation function and z2Rm. Then, using equation (7.59),\\nwe have\\nB[\\x1b;z](v) =2\\n64@\\x1b(z1)\\n@z1\\x01\\x01\\x01@\\x1b(zm)\\n@z1.........\\n@\\x1b(z1)\\n@zm\\x01\\x01\\x01@\\x1b(zm)\\n@zm3\\n75v (7.67)\\n= diag(\\x1b0(z1);\\x01\\x01\\x01;\\x1b0(zm))v (7.68)\\n=\\x1b0(z)\\x0cv2Rm: (7.69)\\nHere, we used the fact that@\\x1b(zj)\\n@zi= 0 whenj6=i, diag(\\x151;:::;\\x15m) denotes\\nthe diagonal matrix with \\x151;:::;\\x15mon the diagonal, and \\x0cdenotes the\\nelement-wise product of two vectors with the same dimension, and \\x1b0(\\x01) is\\nthe element-wise application of the derivative of the activation function \\x1b.\\nRegarding computation e\\x0eciency, we note that at the \\x0crst sight, equa-\\ntion (7.67) appears to indicate the backward function takes O(m2) time, but\\nequation (7.69) shows that it's implementable in O(m) time (which is the\\nsame as the time for evaluating of the function.) We are not supposed to be\\nsurprised by that the possibility of simplifying equation (7.67) to (7.69)|if\\nwe use smaller modules, that is, treating the vector-to-vector nonlinear ac-\\ntivation as mscalar-to-scalar non-linear activation, then it's more obvious\\nthat the backward pass should have similar time to the forward pass.\\nBackward function for loss functions. When a module Mtakes in a vector\\nzand outputs a scalar, by equation (7.59), the backward function takes in a\\nscalarvand outputs a vector with entries ( B[M;z](v))i=@M\\n@ziv. Therefore,\\nin vectorized notation, B[M;z](v) =@M\\n@z\\x01v.\\nRecall that squared loss `MSE(z;y) =1\\n2(z\\x00y)2:Thus,B[`MSE;z](v) =\\n@1\\n2(z\\x00y)2\\n@z\\x01v= (z\\x00y)\\x01v.\\nFor logistics loss, by equation (2.6), we have\\nB[`logistic;t](v) =@`logistic (t;y)\\n@t\\x01v= (1=(1 + exp(\\x00t))\\x00y)\\x01v: (7.70)\",\n",
       " \"107\\nFor cross-entropy loss, by equation (2.17), we have\\nB[`ce;t](v) =@`ce(t;y)\\n@t\\x01v= (\\x1e\\x00ey)\\x01v; (7.71)\\nwhere\\x1e= softmax( t).\\n7.4.4 Back-propagation for MLPs\\nGiven the backward functions for every module needed in evaluating the loss\\nof an MLP, we follow the strategy in Section 7.4.2 to compute the gradient\\nof the loss w.r.t to the hidden activations and the parameters.\\nWe consider the an r-layer MLP with a logistic loss. The loss function\\ncan be computed via a sequence of operations (that is, the forward pass),\\nz[1]= MMW[1];b[1](x);\\na[1]=\\x1b(z[1])\\nz[2]= MMW[2];b[2](a[1])\\na[2]=\\x1b(z[2])\\n...\\nz[r]= MMW[r];b[r](a[r\\x001])\\nJ=`logistic (z[r];y): (7.72)\\nWe apply the backward function sequentially in a backward order. First, we\\nhave that\\n@J\\n@z[r]=B[`logistic;z[r]]\\x12@J\\n@J\\x13\\n=B[`logistic;z[r]](1): (7.73)\\nThen, we iteratively compute@J\\n@a[i]and@J\\n@z[i]'s by repeatedly invoking the chain\\nrule (equation (7.58)),\\n@J\\n@a[r\\x001]=B[MM;a[r\\x001]]\\x12@J\\n@z[r]\\x13\\n@J\\n@z[r\\x001]=B[\\x1b;z[r\\x001]]\\x12@J\\n@a[r\\x001]\\x13\\n...\\n@J\\n@z[1]=B[\\x1b;z[1]]\\x12@J\\n@a[1]\\x13\\n: (7.74)\",\n",
       " '108\\nNumerically, we compute these quantities by repeatedly invoking equa-\\ntions (7.69) and (7.63) with di\\x0berent choices of variables.\\nWe note that the intermediate values of a[i]andz[i]are used in the back-\\npropagation (equation (7.74)), and therefore these values need to be stored\\nin the memory after the forward pass.\\nNext, we compute the gradient of the parameters by invoking equa-\\ntions (7.65) and (7.66),\\n@J\\n@W[r]=B[MM;W[r]]\\x12@J\\n@z[r]\\x13\\n@J\\n@b[r]=B[MM;b[r]]\\x12@J\\n@z[r]\\x13\\n...\\n@J\\n@W[1]=B[MM;W[1]]\\x12@J\\n@z[1]\\x13\\n@J\\n@b[1]=B[MM;b[1]]\\x12@J\\n@z[1]\\x13\\n: (7.75)\\nWe also note that the block of computations in equations (7.75) can be\\ninterleaved with the block of computation in equations (7.74) because the\\n@J\\n@W[i]and@J\\n@b[i]can be computed as soon as@J\\n@z[i]is computed.\\nPutting all of these together, and explicitly invoking the equa-\\ntions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3).',\n",
       " \"109\\nAlgorithm 3 Back-propagation for multi-layer neural networks.\\n1:Forward pass. Compute and store the values of a[k]'s,z[k]'s, andJ\\nusing the equations (7.72).\\n2:Backward pass. Compute the gradient of loss Jwith respect to z[r]:\\n@J\\n@z[r]=B[`logistic;z[r]](1) =\\x00\\n1=(1 + exp(\\x00z[r]))\\x00y\\x01\\n: (7.76)\\n3:fork=r\\x001 to 0 do\\n4: Compute the gradient with respect to parameters W[k+1]andb[k+1].\\n@J\\n@W[k+1]=B[MM;W[k+1]]\\x12@J\\n@z[k+1]\\x13\\n=@J\\n@z[k+1]a[k]>: (7.77)\\n@J\\n@b[k+1]=B[MM;b[k+1]]\\x12@J\\n@z[k+1]\\x13\\n=@J\\n@z[k+1]: (7.78)\\n5: Whenk\\x151, compute the gradient with respect to z[k]anda[k].\\n@J\\n@a[k]=B[\\x1b;a[k]]\\x12@J\\n@z[k+1]\\x13\\n=W[k+1]>@J\\n@z[k+1]: (7.79)\\n@J\\n@z[k]=B[\\x1b;z[k]]\\x12@J\\n@a[k]\\x13\\n=\\x1b0(z[k])\\x0c@J\\n@a[k]: (7.80)\\n7.5 Vectorization over training examples\\nAs we discussed in Section 7.1, in the implementation of neural networks,\\nwe will leverage the parallelism across the multiple examples. This means\\nthat we will need to write the forward pass (the evaluation of the outputs)\\nof the neural network and the backward pass (backpropagation) for multiple\",\n",
       " '110\\ntraining examples in matrix notation.\\nThe basic idea. The basic idea is simple. Suppose you have a training\\nset with three examples x(1);x(2);x(3). The \\x0crst-layer activations for each\\nexample are as follows:\\nz[1](1)=W[1]x(1)+b[1]\\nz[1](2)=W[1]x(2)+b[1]\\nz[1](3)=W[1]x(3)+b[1]\\nNote the di\\x0berence between square brackets [ \\x01], which refer to the layer num-\\nber, and parenthesis ( \\x01), which refer to the training example number. In-\\ntuitively, one would implement this using a for loop. It turns out, we can\\nvectorize these operations as well. First, de\\x0cne:\\nX=2\\n4j j j\\nx(1)x(2)x(3)\\nj j j3\\n52Rd\\x023(7.81)\\nNote that we are stacking training examples in columns and notrows. We\\ncan then combine this into a single uni\\x0ced formulation:\\nZ[1]=2\\n4j j j\\nz[1](1)z[1](2)z[1](3)\\nj j j3\\n5=W[1]X+b[1](7.82)\\nYou may notice that we are attempting to add b[1]2R4\\x021toW[1]X2\\nR4\\x023. Strictly following the rules of linear algebra, this is not allowed. In\\npractice however, this addition is performed using broadcasting . We create\\nan intermediate ~b[1]2R4\\x023:\\n~b[1]=2\\n4j j j\\nb[1]b[1]b[1]\\nj j j3\\n5 (7.83)\\nWe can then perform the computation: Z[1]=W[1]X+~b[1]. Often times, it\\nis not necessary to explicitly construct ~b[1]. By inspecting the dimensions in\\n(7.82), you can assume b[1]2R4\\x021is correctly broadcast to W[1]X2R4\\x023.\\nThe matricization approach as above can easily generalize to multiple\\nlayers, with one subtlety though, as discussed below.',\n",
       " '111\\nComplications/Subtlety in the Implementation. All the deep learn-\\ning packages or implementations put the data points in the rows of a data\\nmatrix. (If the data point itself is a matrix or tensor, then the data are con-\\ncentrated along the zero-th dimension.) However, most of the deep learning\\npapers use a similar notation to these notes where the data points are treated\\nas column vectors.8There is a simple conversion to deal with the mismatch:\\nin the implementation, all the columns become row vectors, row vectors be-\\ncome column vectors, all the matrices are transposed, and the orders of the\\nmatrix multiplications are \\ripped. In the example above, using the row ma-\\njor convention, the data matrix is X2R3\\x02d, the \\x0crst layer weight matrix\\nhas dimensionality d\\x02m(instead of m\\x02das in the two layer neural net\\nsection), and the bias vector b[1]2R1\\x02m. The computation for the hidden\\nactivation becomes\\nZ[1]=XW[1]+b[1]2R3\\x02m(7.84)\\n8The instructor suspects that this is mostly because in mathematics we naturally mul-\\ntiply a matrix to a vector on the left hand side.',\n",
       " 'Part III\\nGeneralization and\\nregularization\\n112',\n",
       " \"Chapter 8\\nGeneralization\\nThis chapter discusses tools to analyze and understand the generaliza-\\ntion of machine learning models, i.e, their performances on unseen test\\nexamples. Recall that for supervised learning problems, given a train-\\ning datasetf(x(i);y(i))gn\\ni=1, we typically learn a model h\\x12by minimizing a\\nloss/cost function J(\\x12), which encourages h\\x12to \\x0ct the data. E.g., when\\nthe loss function is the least square loss (aka mean squared error), we have\\nJ(\\x12) =1\\nnPn\\ni=1(y(i)\\x00h\\x12(x(i)))2. This loss function for training purposes is\\noftentimes referred to as the training loss/error/cost.\\nHowever, minimizing the training loss is not our ultimate goal|it is\\nmerely our approach towards the goal of learning a predictive model. The\\nmost important evaluation metric of a model is the loss on unseen test exam-\\nples, which is oftentimes referred to as the test error. Formally, we sample a\\ntest example ( x;y) from the so-called test distribution D, and measure the\\nmodel's error on it, by, e.g., the mean squared error, ( h\\x12(x)\\x00y)2. The ex-\\npected loss/error over the randomness of the test example is called the test\\nloss/error,1\\nL(\\x12) =E(x;y)\\x18D[(y\\x00h\\x12(x))2] (8.1)\\nNote that the measurement of the error involves computing the expectation,\\nand in practice, it can be approximated by the average error on many sampled\\ntest examples, which are referred to as the test dataset. Note that the key\\ndi\\x0berence here between training and test datasets is that the test examples\\n1In theoretical and statistical literature, we oftentimes call the uniform distribution\\nover the training set f(x(i);y(i))gn\\ni=1, denoted by bD, an empirical distribution, and call\\nDthe population distribution. Partly because of this, the training loss is also referred\\nto as the empirical loss/risk/error, and the test loss is also referred to as the population\\nloss/risk/error.\\n113\",\n",
       " '114\\nareunseen , in the sense that the training procedure has not used the test\\nexamples. In classical statistical learning settings, the training examples are\\nalso drawn from the same distribution as the test distribution D, but still\\nthe test examples are unseen by the learning procedure whereas the training\\nexamples are seen.2\\nBecause of this key di\\x0berence between training and test datasets, even\\nif they are both drawn from the same distribution D, the test error is not\\nnecessarily always close to the training error.3As a result, successfully min-\\nimizing the training error may not always lead to a small test error. We\\ntypically say the model over\\x0cts the data if the model predicts accurately on\\nthe training dataset but doesn\\'t generalize well to other test examples, that\\nis, if the training error is small but the test error is large. We say the model\\nunder\\x0cts the data if the training error is relatively large4(and in this case,\\ntypically the test error is also relatively large.)\\nThis chapter studies how the test error is in\\ruenced by the learning pro-\\ncedure, especially the choice of model parameterizations. We will decompose\\nthe test error into \\\\bias\" and \\\\variance\" terms and study how each of them is\\na\\x0bected by the choice of model parameterizations and their tradeo\\x0bs. Using\\nthe bias-variance tradeo\\x0b, we will discuss when over\\x0ctting and under\\x0ctting\\nwill occur and be avoided. We will also discuss the double descent phe-\\nnomenon in Section 8.2 and some classical theoretical results in Section 8.3.\\n2These days, researchers have increasingly been more interested in the setting with\\n\\\\domain shift\", that is, the training distribution and test distribution are di\\x0berent.\\n3the di\\x0berence between test error and training error is often referred to as the gener-\\nalization gap. The term generalization error in some literature means the test error, and\\nin some other literature means the generalization gap.\\n4e.g., larger than the intrinsic noise level of the data in regression problems.',\n",
       " '115\\n8.1 Bias-variance tradeo\\x0b\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining dataset\\ntraining data\\nground truth h*\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytest dataset\\ntest data\\nground truth h*\\nFigure 8.1: A running example of training and test dataset for this section.\\nAs an illustrating example, we consider the following training dataset and\\ntest dataset, which are also shown in Figure 8.1. The training inputs x(i)\\'s are\\nrandomly chosen and the outputs y(i)are generated by y(i)=h?(x(i)) +\\x18(i)\\nwhere the function h?(\\x01) is a quadratic function and is shown in Figure 8.1\\nas the solid line, and \\x18(i)is the a observation noise assumed to be generated\\nfrom\\x18N(0;\\x1b2). A test example ( x;y) also has the same input-output\\nrelationship y=h?(x) +\\x18where\\x18\\x18N(0;\\x1b2). It\\'s impossible to predict the\\nnoise\\x18, and therefore essentially our goal is to recover the function h?(\\x01).\\nWe will consider the test error of learning various types of models. When\\ntalking about linear regression, we discussed the problem of whether to \\x0ct\\na \\\\simple\" model such as the linear \\\\ y=\\x120+\\x121x,\" or a more \\\\complex\"\\nmodel such as the polynomial \\\\ y=\\x120+\\x121x+\\x01\\x01\\x01\\x125x5.\"\\nWe start with \\x0ctting a linear model, as shown in Figure 8.2. The best\\n\\x0ctted linear model cannot predict yfromxaccurately even on the training\\ndataset, let alone on the test dataset. This is because the true relationship\\nbetweenyandxis not linear|any linear model is far away from the true\\nfunctionh?(\\x01). As a result, the training error is large and this is a typical\\nsituation of under\\x0ctting .',\n",
       " \"116\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining data\\nbest fit linear model\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytest data\\nbest fit linear model\\nFigure 8.2: The best \\x0ct linear model has large training and test errors.\\nThe issue cannot be mitigated with more training examples|even with\\na very large amount of, or even in\\x0cnite training examples, the best \\x0ctted\\nlinear model is still inaccurate and fails to capture the structure of the data\\n(Figure 8.3). Even if the noise is not present in the training data, the issue\\nstill occurs (Figure 8.4). Therefore, the fundamental bottleneck here is the\\nlinear model family's inability to capture the structure in the data|linear\\nmodels cannot represent the true quadratic function h?|, but not the lack of\\nthe data. Informally, we de\\x0cne the bias of a model to be the test error even\\nif we were to \\x0ct it to a very (say, in\\x0cnitely) large training dataset. Thus, in\\nthis case, the linear model su\\x0bers from large bias, and under\\x0cts (i.e., fails to\\ncapture structure exhibited by) the data.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5yfitting linear models on a large dataset\\ntraining data\\nground truth h*\\nbest fit linear model\\nFigure 8.3: The best \\x0ct linear\\nmodel on a much larger dataset\\nstill has a large training error.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5yfitting linear models on a noiseless dataset\\ntraining data\\nground truth h*\\nbest fit linear modelFigure 8.4: The best \\x0ct linear\\nmodel on a noiseless dataset also\\nhas a large training/test error.\\nNext, we \\x0ct a 5th-degree polynomial to the data. Figure 8.5 shows that\\nit fails to learn a good model either. However, the failure pattern is di\\x0berent\\nfrom the linear model case. Speci\\x0ccally, even though the learnt 5th-degree\",\n",
       " \"117\\npolynomial did a very good job predicting y(i)'s fromx(i)'s for training ex-\\namples, it does not work well on test examples (Figure 8.5). In other words,\\nthe model learnt from the training set does not generalize well to other test\\nexamples|the test error is high. Contrary to the behavior of linear models,\\nthe bias of the 5-th degree polynomials is small|if we were to \\x0ct a 5-th de-\\ngree polynomial to an extremely large dataset, the resulting model would be\\nclose to a quadratic function and be accurate (Figure 8.6). This is because\\nthe family of 5-th degree polynomials contains all the quadratic functions\\n(setting\\x125=\\x124=\\x123= 0 results in a quadratic function), and, therefore,\\n5-th degree polynomials are in principle capable of capturing the structure\\nof the data.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining data\\nbest fit 5-th degree model\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytest data\\nground truth h*\\nbest fit 5-th degree model\\nFigure 8.5: Best \\x0ct 5-th degree polynomial has zero training error, but still\\nhas a large test error and does not recover the the ground truth. This is a\\nclassic situation of over\\x0ctting.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining data\\nbest fit 5-th degree model\\nground truth h*fitting 5-th degree model on large dataset\\nFigure 8.6: The best \\x0ct 5-th degree polynomial on a huge dataset nearly\\nrecovers the ground-truth|suggesting that the culprit in Figure 8.5 is the\\nvariance (or lack of data) but not bias.\\nThe failure of \\x0ctting 5-th degree polynomials can be captured by another\",\n",
       " '118\\ncomponent of the test error, called variance of a model \\x0ctting procedure.\\nSpeci\\x0ccally, when \\x0ctting a 5-th degree polynomial as in Figure 8.7, there is a\\nlarge risk that we\\'re \\x0ctting patterns in the data that happened to be present\\nin our small, \\x0cnite training set, but that do not re\\rect the wider pattern of\\nthe relationship between xandy. These \\\\spurious\" patterns in the training\\nset are (mostly) due to the observation noise \\x18(i), and \\x0ctting these spurious\\npatters results in a model with large test error. In this case, we say the model\\nhas a large variance.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining data\\nbest fit 5-th degree model\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining data\\nbest fit 5-th degree model\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining data\\nbest fit 5-th degree modelfitting 5-th degree model on different datasets\\nFigure 8.7: The best \\x0ct 5-th degree models on three di\\x0berent datasets gen-\\nerated from the same distribution behave quite di\\x0berently, suggesting the\\nexistence of a large variance.\\nThe variance can be intuitively (and mathematically, as shown in Sec-\\ntion 8.1.1) characterized by the amount of variations across models learnt\\non multiple di\\x0berent training datasets (drawn from the same underlying dis-\\ntribution). The \\\\spurious patterns\" are speci\\x0cc to the randomness of the\\nnoise (and inputs) in a particular dataset, and thus are di\\x0berent across mul-\\ntiple training datasets. Therefore, over\\x0ctting to the \\\\spurious patterns\" of\\nmultiple datasets should result in very di\\x0berent models. Indeed, as shown\\nin Figure 8.7, the models learned on the three di\\x0berent training datasets are\\nquite di\\x0berent, over\\x0ctting to the \\\\spurious patterns\" of each datasets.\\nOften, there is a tradeo\\x0b between bias and variance. If our model is too\\n\\\\simple\" and has very few parameters, then it may have large bias (but small\\nvariance), and it typically may su\\x0ber from under\\x0cttng. If it is too \\\\complex\"\\nand has very many parameters, then it may su\\x0ber from large variance (but\\nhave smaller bias), and thus over\\x0ctting. See Figure 8.8 for a typical tradeo\\x0b\\nbetween bias and variance.',\n",
       " '119\\nModel ComplexityError\\nBias2VarianceTest Error (= Bias2+Variance) Optimal Tradeoff\\nFigure 8.8: An illustration of the typical bias-variance tradeo\\x0b.\\nAs we will see formally in Section 8.1.1, the test error can be decomposed\\nas a summation of bias and variance. This means that the test error will\\nhave a convex curve as the model complexity increases, and in practice we\\nshould tune the model complexity to achieve the best tradeo\\x0b. For instance,\\nin the example above, \\x0ctting a quadratic function does better than either of\\nthe extremes of a \\x0crst or a 5-th degree polynomial, as shown in Figure 8.9.\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytraining data\\nbest fit quadratic model\\n0.0 0.2 0.4 0.6 0.8 1.0\\nx0.00.51.01.5ytest data\\nbest fit quadratic model\\nground truth h*\\nFigure 8.9: Best \\x0ct quadratic model has small training and test error because\\nquadratic model achieves a better tradeo\\x0b.\\nInterestingly, the bias-variance tradeo\\x0b curves or the test error curves\\ndo not universally follow the shape in Figure 8.8, at least not universally\\nwhen the model complexity is simply measured by the number of parameters.\\n(We will discuss the so-called double descent phenomenon in Section 8.2.)\\nNevertheless, the principle of bias-variance tradeo\\x0b is perhaps still the \\x0crst\\nresort when analyzing and predicting the behavior of test errors.',\n",
       " '120\\n8.1.1 A mathematical decomposition (for regression)\\nTo formally state the bias-variance tradeo\\x0b for regression problems, we con-\\nsider the following setup (which is an extension of the beginning paragraph\\nof Section 8.1).\\n•Draw a training dataset S=fx(i);y(i)gn\\ni=1such thaty(i)=h?(x(i)) +\\x18(i)\\nwhere\\x18(i)2N(0;\\x1b2).\\n•Train a model on the dataset S, denoted by ^hS.\\n•Take a test example ( x;y) such that y=h?(x) +\\x18where\\x18\\x18N(0;\\x1b2),\\nand measure the expected test error (averaged over the random draw of\\nthe training set Sand the randomness of \\x18)56\\nMSE(x) =ES;\\x18[(y\\x00hS(x))2] (8.2)\\nWe will decompose the MSE into a bias and variance term. We start by\\nstating a following simple mathematical tool that will be used twice below.\\nClaim 8.1.1: SupposeAandBare two independent real random variables\\nandE[A] = 0. Then, E[(A+B)2] =E[A2] +E[B2].\\nAs a corollary, because a random variable Ais independent with a con-\\nstantc, when E[A] = 0, we have E[(A+c)2] =E[A2] +c2.\\nThe proof of the claim follows from expanding the square: E[(A+B)2] =\\nE[A2] +E[B2] + 2E[AB] =E[A2] +E[B2]. Here we used the independence to\\nshow that E[AB] =E[A]E[B] = 0.\\nUsing Claim 8.1.1 with A=\\x18andB=h?(x)\\x00^hS(x), we have\\nMSE(x) =E[(y\\x00hS(x))2] =E[(\\x18+ (h?(x)\\x00hS(x)))2] (8.3)\\n=E[\\x182] +E[(h?(x)\\x00hS(x))2] (by Claim 8.1.1)\\n=\\x1b2+E[(h?(x)\\x00hS(x))2] (8.4)\\nThen, let\\'s de\\x0cne havg(x) =ES[hS(x)] as the \\\\average model\"|the model\\nobtained by drawing an in\\x0cnite number of datasets, training on them, and\\naveraging their predictions on x. Note that havgis a hypothetical model for\\nanalytical purposes that can not be obtained in reality (because we don\\'t\\n5For simplicity, the test input xis considered to be \\x0cxed here, but the same conceptual\\nmessage holds when we average over the choice of x\\'s.\\n6The subscript under the expectation symbol is to emphasize the variables that are\\nconsidered as random by the expectation operation.',\n",
       " '121\\nhave in\\x0cnite number of datasets). It turns out that for many cases, havg\\nis (approximately) equal to the the model obtained by training on a single\\ndataset with in\\x0cnite samples. Thus, we can also intuitively interpret havgthis\\nway, which is consistent with our intuitive de\\x0cnition of bias in the previous\\nsubsection.\\nWe can further decompose MSE( x) by letting c=h?(x)\\x00havg(x) (which is\\na constant that does not depend on the choice of S!) andA=havg(x)\\x00hS(x)\\nin the corollary part of Claim 8.1.1:\\nMSE(x) =\\x1b2+E[(h?(x)\\x00hS(x))2] (8.5)\\n=\\x1b2+ (h?(x)\\x00havg(x))2+E[(havg\\x00hS(x))2] (8.6)\\n=\\x1b2\\n|{z}\\nunavoidable+ (h?(x)\\x00havg(x))2\\n|{z}\\n,bias2+ var(hS(x))|{z}\\n,variance(8.7)\\nWe call the second term the bias (square) and the third term the variance. As\\ndiscussed before, the bias captures the part of the error that are introduced\\ndue to the lack of expressivity of the model. Recall that havgcan be thought\\nof as the best possible model learned even with in\\x0cnite data. Thus, the bias is\\nnot due to the lack of data, but is rather caused by that the family of models\\nfundamentally cannot approximate the h?. For example, in the illustrating\\nexample in Figure 8.2, because any linear model cannot approximate the\\ntrue quadratic function h?, neither can havg, and thus the bias term has to\\nbe large.\\nThe variance term captures how the random nature of the \\x0cnite dataset\\nintroduces errors in the learned model. It measures the sensitivity of the\\nlearned model to the randomness in the dataset. It often decreases as the\\nsize of the dataset increases.\\nThere is nothing we can do about the \\x0crst term \\x1b2as we can not predict\\nthe noise\\x18by de\\x0cnition.\\nFinally, we note that the bias-variance decomposition for classi\\x0ccation\\nis much less clear than for regression problems. There have been several\\nproposals, but there is as yet no agreement on what is the \\\\right\" and/or\\nthe most useful formalism.\\n8.2 The double descent phenomenon\\nModel-wise double descent. Recent works have demonstrated that the\\ntest error can present a \\\\double descent\" phenomenon in a range of machine',\n",
       " \"122\\nlearning models including linear models and deep neural networks.7The\\nconventional wisdom, as discussed in Section 8.1, is that as we increase the\\nmodel complexity, the test error \\x0crst decreases and then increases, as illus-\\ntrated in Figure 8.8. However, in many cases, we empirically observe that\\nthe test error can have a second descent|it \\x0crst decreases, then increases\\nto a peak around when the model size is large enough to \\x0ct all the training\\ndata very well, and then decreases again in the so-called overparameterized\\nregime, where the number of parameters is larger than the number of data\\npoints. See Figure 8.10 for an illustration of the typical curves of test errors\\nagainst model complexity (measured by the number of parameters). To some\\nextent, the overparameterized regime with the second descent is considered as\\nnew to the machine learning community|partly because lightly-regularized,\\noverparameterized models are only extensively used in the deep learning era.\\nA practical implication of the phenomenon is that one should not hold back\\nfrom scaling into and experimenting with over-parametrized models because\\nthe test error may well decrease again to a level even smaller than the previ-\\nous lowest point. Actually, in many cases, larger overparameterized models\\nalways lead to a better test performance (meaning there won't be a second\\nascent after the second descent).\\n# parameterstest errortypically when # parameters\\nis sufficient to fit the dataclassical regime:\\nbias-variance tradeoffmodern regime:\\nover-parameterization\\nFigure 8.10: A typical model-wise double descent phenomenon. As the num-\\nber of parameters increases, the test error \\x0crst decreases when the number of\\nparameters is smaller than the training data. Then in the overparameterized\\nregime, the test error decreases again.\\n7The discovery of the phenomenon perhaps dates back to Opper [1995, 2001], and has\\nbeen recently popularized by Belkin et al. [2020], Hastie et al. [2019], etc.\",\n",
       " \"123\\nSample-wise double descent. A priori, we would expect that more\\ntraining examples always lead to smaller test errors|more samples give\\nstrictly more information for the algorithm to learn from. However, recent\\nwork [Nakkiran, 2019] observes that the test error is not monotonically de-\\ncreasing as we increase the sample size. Instead, as shown in Figure 8.11, the\\ntest error decreases, and then increases and peaks around when the number\\nof examples (denoted by n) is similar to the number of parameters (denoted\\nbyd), and then decreases again. We refer to this as the sample-wise dou-\\nble descent phenomenon. To some extent, sample-wise double descent and\\nmodel-wise double descent are essentially describing similar phenomena|the\\ntest error is peaked when n\\x19d.\\nExplanation and mitigation strategy. The sample-wise double descent,\\nor, in particular, the peak of test error at n\\x19d, suggests that the existing\\ntraining algorithms evaluated in these experiments are far from optimal when\\nn\\x19d. We will be better o\\x0b by tossing away some examples and run the\\nalgorithms with a smaller sample size to steer clear of the peak. In other\\nwords, in principle, there are other algorithms that can achieve smaller test\\nerror when n\\x19d, but the algorithms evaluated in these experiments fail to\\ndo so. The sub-optimality of the learning procedure appears to be the culprit\\nof the peak in both sample-wise and model-wise double descent.\\nIndeed, with an optimally-tuned regularization (which will be discussed\\nmore in Section 9), the test error in the n\\x19dregime can be dramatically\\nimproved, and the model-wise and sample-wise double descent are both mit-\\nigated. See Figure 8.11.\\nThe intuition above only explains the peak in the model-wise and sample-\\nwise double descent, but does not explain the second descent in the model-\\nwise double descent|why overparameterized models are able to generalize\\nso well. The theoretical understanding of overparameterized models is an ac-\\ntive research area with many recent advances. A typical explanation is that\\nthe commonly-used optimizers such as gradient descent provide an implicit\\nregularization e\\x0bect (which will be discussed in more detail in Section 9.2).\\nIn other words, even in the overparameterized regime and with an unregular-\\nized loss function, the model is still implicitly regularized, and thus exhibits\\na better test performance than an arbitrary solution that \\x0cts the data. For\\nexample, for linear models, when n\\x1cd, the gradient descent optimizer with\\nzero initialization \\x0cnds the minimum norm solution that \\x0cts the data (in-\\nstead of an arbitrary solution that \\x0cts the data), and the minimum norm reg-\\nularizer turns out to be a su\\x0eciently good for the overparameterized regime\\n(but it's not a good regularizer when n\\x19d, resulting in the peak of test\",\n",
       " '124\\nerror).\\n0 200 400 600 800 1000\\nNum Samples0.000.250.500.751.001.251.501.752.00T est ErrorT est Error vs. # Samples\\nT est Error\\nFigure 8.11: Left: The sample-wise double descent phenomenon for linear\\nmodels. Right: The sample-wise double descent with di\\x0berent regularization\\nstrength for linear models. Using the optimal regularization parameter \\x15\\n(optimally tuned for each n, shown in green solid curve) mitigates double\\ndescent. Setup: The data distribution of ( x;y) isx\\x18N (0;Id) andy\\x18\\nx>\\x0c+N(0;\\x1b2) whered= 500;\\x1b= 0:5 andk\\x0ck2= 1.8\\nFinally, we also remark that the double descent phenomenon has been\\nmostly observed when the model complexity is measured by the number of\\nparameters. It is unclear if and when the number of parameters is the best\\ncomplexity measure of a model. For example, in many situations, the norm\\nof the models is used as a complexity measure. As shown in Figure 8.12\\nright, for a particular linear case, if we plot the test error against the norm\\nof the learnt model, the double descent phenomenon no longer occurs. This\\nis partly because the norm of the learned model is also peaked around n\\x19d\\n(See Figure 8.12 (middle) or Belkin et al. [2019], Mei and Montanari [2022],\\nand discussions in Section 10.8 of James et al. [2021]). For deep neural\\nnetworks, the correct complexity measure is even more elusive. The study of\\ndouble descent phenomenon is an active research topic.\\n8The \\x0cgure is reproduced from Figure 1 of Nakkiran et al. [2020]. Similar phenomenon\\nare also observed in Hastie et al. [2022], Mei and Montanari [2022]',\n",
       " '125\\n0 250 500 750 1000\\n# parameters0.00.20.40.60.81.0test errortest error vs. # params\\n0 200 400 600 800 1000\\n# parameters010203040normnorm vs. # params\\n0 10 20 30 40\\nnorm0.00.20.40.60.81.0test errord=n\\n# parameterstest error vs. norm\\n02004006008001000\\nFigure 8.12: Left: The double descent phenomenon, where the number of pa-\\nrameters is used as the model complexity. Middle: The norm of the learned\\nmodel is peaked around n\\x19d.Right: The test error against the norm of\\nthe learnt model. The color bar indicate the number of parameters and the\\narrows indicates the direction of increasing model size. Their relationship\\nare closer to the convention wisdom than to a double descent. Setup: We\\nconsider a linear regression with a \\x0cxed dataset of size n= 500:The input\\nxis a random ReLU feature on Fashion-MNIST, and output y2R10is the\\none-hot label. This is the same setting as in Section 5.2 of Nakkiran et al.\\n[2020].',\n",
       " \"126\\n8.3 Sample complexity bounds (optional\\nreadings)\\n8.3.1 Preliminaries\\nIn this set of notes, we begin our foray into learning theory. Apart from\\nbeing interesting and enlightening in its own right, this discussion will also\\nhelp us hone our intuitions and derive rules of thumb about how to best\\napply learning algorithms in di\\x0berent settings. We will also seek to answer\\na few questions: First, can we make formal the bias/variance tradeo\\x0b that\\nwas just discussed? This will also eventually lead us to talk about model\\nselection methods, which can, for instance, automatically decide what order\\npolynomial to \\x0ct to a training set. Second, in machine learning it's really\\ngeneralization error that we care about, but most learning algorithms \\x0ct their\\nmodels to the training set. Why should doing well on the training set tell us\\nanything about generalization error? Speci\\x0ccally, can we relate error on the\\ntraining set to generalization error? Third and \\x0cnally, are there conditions\\nunder which we can actually prove that learning algorithms will work well?\\nWe start with two simple but very useful lemmas.\\nLemma. (The union bound). Let A1;A2;:::;Akbekdi\\x0berent events (that\\nmay not be independent). Then\\nP(A1[\\x01\\x01\\x01[Ak)\\x14P(A1) +:::+P(Ak):\\nIn probability theory, the union bound is usually stated as an axiom\\n(and thus we won't try to prove it), but it also makes intuitive sense: The\\nprobability of any one of kevents happening is at most the sum of the\\nprobabilities of the kdi\\x0berent events.\\nLemma. (Hoe\\x0bding inequality) Let Z1;:::;Znbenindependent and iden-\\ntically distributed (iid) random variables drawn from a Bernoulli( \\x1e) distri-\\nbution. I.e., P(Zi= 1) =\\x1e, andP(Zi= 0) = 1\\x00\\x1e. Let ^\\x1e= (1=n)Pn\\ni=1Zi\\nbe the mean of these random variables, and let any \\r >0 be \\x0cxed. Then\\nP(j\\x1e\\x00^\\x1ej>\\r)\\x142 exp(\\x002\\r2n)\\nThis lemma (which in learning theory is also called the Cherno\\x0b bound )\\nsays that if we take ^\\x1e|the average of nBernoulli(\\x1e) random variables|to\\nbe our estimate of \\x1e, then the probability of our being far from the true value\\nis small, so long as nis large. Another way of saying this is that if you have\\na biased coin whose chance of landing on heads is \\x1e, then if you toss it n\",\n",
       " '127\\ntimes and calculate the fraction of times that it came up heads, that will be\\na good estimate of \\x1ewith high probability (if nis large).\\nUsing just these two lemmas, we will be able to prove some of the deepest\\nand most important results in learning theory.\\nTo simplify our exposition, let\\'s restrict our attention to binary classi\\x0cca-\\ntion in which the labels are y2f0;1g. Everything we\\'ll say here generalizes\\nto other problems, including regression and multi-class classi\\x0ccation.\\nWe assume we are given a training set S=f(x(i);y(i));i= 1;:::;ngof size\\nn, where the training examples ( x(i);y(i)) are drawn iid from some probability\\ndistributionD. For a hypothesis h, we de\\x0cne the training error (also called\\ntheempirical risk orempirical error in learning theory) to be\\n^\"(h) =1\\nnnX\\ni=11fh(x(i))6=y(i)g:\\nThis is just the fraction of training examples that hmisclassi\\x0ces. When we\\nwant to make explicit the dependence of ^ \"(h) on the training set S, we may\\nalso write this a ^ \"S(h). We also de\\x0cne the generalization error to be\\n\"(h) =P(x;y)\\x18D(h(x)6=y):\\nI.e. this is the probability that, if we now draw a new example ( x;y) from\\nthe distributionD,hwill misclassify it.\\nNote that we have assumed that the training data was drawn from the\\nsame distributionDwith which we\\'re going to evaluate our hypotheses (in\\nthe de\\x0cnition of generalization error). This is sometimes also referred to as\\none of the PAC assumptions.9\\nConsider the setting of linear classi\\x0ccation, and let h\\x12(x) = 1f\\x12Tx\\x150g.\\nWhat\\'s a reasonable way of \\x0ctting the parameters \\x12? One approach is to try\\nto minimize the training error, and pick\\n^\\x12= arg min\\n\\x12^\"(h\\x12):\\nWe call this process empirical risk minimization (ERM), and the resulting\\nhypothesis output by the learning algorithm is ^h=h^\\x12. We think of ERM\\nas the most \\\\basic\" learning algorithm, and it will be this algorithm that we\\n9PAC stands for \\\\probably approximately correct,\" which is a framework and set of\\nassumptions under which numerous results on learning theory were proved. Of these, the\\nassumption of training and testing on the same distribution, and the assumption of the\\nindependently drawn training examples, were the most important.',\n",
       " '128\\nfocus on in these notes. (Algorithms such as logistic regression can also be\\nviewed as approximations to empirical risk minimization.)\\nIn our study of learning theory, it will be useful to abstract away from\\nthe speci\\x0cc parameterization of hypotheses and from issues such as whether\\nwe\\'re using a linear classi\\x0cer. We de\\x0cne the hypothesis class Hused by a\\nlearning algorithm to be the set of all classi\\x0cers considered by it. For linear\\nclassi\\x0ccation,H=fh\\x12:h\\x12(x) = 1f\\x12Tx\\x150g;\\x122Rd+1gis thus the set of\\nall classi\\x0cers over X(the domain of the inputs) where the decision boundary\\nis linear. More broadly, if we were studying, say, neural networks, then we\\ncould letHbe the set of all classi\\x0cers representable by some neural network\\narchitecture.\\nEmpirical risk minimization can now be thought of as a minimization over\\nthe class of functions H, in which the learning algorithm picks the hypothesis:\\n^h= arg min\\nh2H^\"(h)\\n8.3.2 The case of \\x0cnite H\\nLet\\'s start by considering a learning problem in which we have a \\x0cnite hy-\\npothesis classH=fh1;:::;hkgconsisting of khypotheses. Thus, His just a\\nset ofkfunctions mapping from Xtof0;1g, and empirical risk minimization\\nselects ^hto be whichever of these kfunctions has the smallest training error.\\nWe would like to give guarantees on the generalization error of ^h. Our\\nstrategy for doing so will be in two parts: First, we will show that ^ \"(h) is a\\nreliable estimate of \"(h) for allh. Second, we will show that this implies an\\nupper-bound on the generalization error of ^h.\\nTake any one, \\x0cxed, hi2H. Consider a Bernoulli random variable Z\\nwhose distribution is de\\x0cned as follows. We\\'re going to sample ( x;y)\\x18D.\\nThen, we set Z= 1fhi(x)6=yg. I.e., we\\'re going to draw one example,\\nand letZindicate whether himisclassi\\x0ces it. Similarly, we also de\\x0cne Zj=\\n1fhi(x(j))6=y(j)g. Since our training set was drawn iid from D,Zand the\\nZj\\'s have the same distribution.\\nWe see that the misclassi\\x0ccation probability on a randomly drawn\\nexample|that is, \"(h)|is exactly the expected value of Z(andZj). More-\\nover, the training error can be written\\n^\"(hi) =1\\nnnX\\nj=1Zj:\\nThus, ^\"(hi) is exactly the mean of the nrandom variables Zjthat are drawn\\niid from a Bernoulli distribution with mean \"(hi). Hence, we can apply the',\n",
       " '129\\nHoe\\x0bding inequality, and obtain\\nP(j\"(hi)\\x00^\"(hi)j>\\r)\\x142 exp(\\x002\\r2n):\\nThis shows that, for our particular hi, training error will be close to\\ngeneralization error with high probability, assuming nis large. But we don\\'t\\njust want to guarantee that \"(hi) will be close to ^ \"(hi) (with high probability)\\nfor just only one particular hi. We want to prove that this will be true\\nsimultaneously for allh2H. To do so, let Aidenote the event that j\"(hi)\\x00\\n^\"(hi)j> \\r. We\\'ve already shown that, for any particular Ai, it holds true\\nthatP(Ai)\\x142 exp(\\x002\\r2n). Thus, using the union bound, we have that\\nP(9h2H:j\"(hi)\\x00^\"(hi)j>\\r) =P(A1[\\x01\\x01\\x01[Ak)\\n\\x14kX\\ni=1P(Ai)\\n\\x14kX\\ni=12 exp(\\x002\\r2n)\\n= 2kexp(\\x002\\r2n)\\nIf we subtract both sides from 1, we \\x0cnd that\\nP(:9h2H:j\"(hi)\\x00^\"(hi)j>\\r) =P(8h2H:j\"(hi)\\x00^\"(hi)j\\x14\\r)\\n\\x151\\x002kexp(\\x002\\r2n)\\n(The \\\\:\" symbol means \\\\not.\") So, with probability at least 1 \\x00\\n2kexp(\\x002\\r2n), we have that \"(h) will be within \\rof ^\"(h) for allh2H.\\nThis is called a uniform convergence result, because this is a bound that\\nholds simultaneously for all (as opposed to just one) h2H.\\nIn the discussion above, what we did was, for particular values of nand\\n\\r, give a bound on the probability that for some h2H,j\"(h)\\x00^\"(h)j> \\r.\\nThere are three quantities of interest here: n,\\r, and the probability of error;\\nwe can bound either one in terms of the other two.\\nFor instance, we can ask the following question: Given \\rand some\\x0e>0,\\nhow large must nbe before we can guarantee that with probability at least\\n1\\x00\\x0e, training error will be within \\rof generalization error? By setting\\n\\x0e= 2kexp(\\x002\\r2n) and solving for n, [you should convince yourself this is\\nthe right thing to do!], we \\x0cnd that if\\nn\\x151\\n2\\r2log2k\\n\\x0e;',\n",
       " '130\\nthen with probability at least 1 \\x00\\x0e, we have thatj\"(h)\\x00^\"(h)j\\x14\\rfor all\\nh2H. (Equivalently, this shows that the probability that j\"(h)\\x00^\"(h)j>\\r\\nfor someh2 H is at most \\x0e.) This bound tells us how many training\\nexamples we need in order make a guarantee. The training set size nthat\\na certain method or algorithm requires in order to achieve a certain level of\\nperformance is also called the algorithm\\'s sample complexity .\\nThe key property of the bound above is that the number of training\\nexamples needed to make this guarantee is only logarithmic ink, the number\\nof hypotheses inH. This will be important later.\\nSimilarly, we can also hold nand\\x0e\\x0cxed and solve for \\rin the previous\\nequation, and show [again, convince yourself that this is right!] that with\\nprobability 1\\x00\\x0e, we have that for all h2H,\\nj^\"(h)\\x00\"(h)j\\x14r\\n1\\n2nlog2k\\n\\x0e:\\nNow, let\\'s assume that uniform convergence holds, i.e., that j\"(h)\\x00^\"(h)j\\x14\\n\\rfor allh2H. What can we prove about the generalization of our learning\\nalgorithm that picked ^h= arg min h2H^\"(h)?\\nDe\\x0cneh\\x03= arg min h2H\"(h) to be the best possible hypothesis in H. Note\\nthath\\x03is the best that we could possibly do given that we are using H, so\\nit makes sense to compare our performance to that of h\\x03. We have:\\n\"(^h)\\x14^\"(^h) +\\r\\n\\x14^\"(h\\x03) +\\r\\n\\x14\"(h\\x03) + 2\\r\\nThe \\x0crst line used the fact that j\"(^h)\\x00^\"(^h)j\\x14\\r(by our uniform convergence\\nassumption). The second used the fact that ^hwas chosen to minimize ^ \"(h),\\nand hence ^\"(^h)\\x14^\"(h) for allh, and in particular ^ \"(^h)\\x14^\"(h\\x03). The third\\nline used the uniform convergence assumption again, to show that ^ \"(h\\x03)\\x14\\n\"(h\\x03) +\\r. So, what we\\'ve shown is the following: If uniform convergence\\noccurs, then the generalization error of ^his at most 2 \\rworse than the best\\npossible hypothesis in H!\\nLet\\'s put all this together into a theorem.\\nTheorem. LetjHj=k, and let any n;\\x0ebe \\x0cxed. Then with probability at\\nleast 1\\x00\\x0e, we have that\\n\"(^h)\\x14\\x12\\nmin\\nh2H\"(h)\\x13\\n+ 2r\\n1\\n2nlog2k\\n\\x0e:',\n",
       " '131\\nThis is proved by letting \\requal thep\\x01term, using our previous argu-\\nment that uniform convergence occurs with probability at least 1 \\x00\\x0e, and\\nthen noting that uniform convergence implies \"(h) is at most 2 \\rhigher than\\n\"(h\\x03) = minh2H\"(h) (as we showed previously).\\nThis also quanti\\x0ces what we were saying previously saying about the\\nbias/variance tradeo\\x0b in model selection. Speci\\x0ccally, suppose we have some\\nhypothesis classH, and are considering switching to some much larger hy-\\npothesis classH0\\x13H . If we switch to H0, then the \\x0crst term min h\"(h)\\ncan only decrease (since we\\'d then be taking a min over a larger set of func-\\ntions). Hence, by learning using a larger hypothesis class, our \\\\bias\" can\\nonly decrease. However, if k increases, then the second 2p\\x01term would also\\nincrease. This increase corresponds to our \\\\variance\" increasing when we use\\na larger hypothesis class.\\nBy holding \\rand\\x0e\\x0cxed and solving for nlike we did before, we can also\\nobtain the following sample complexity bound:\\nCorollary. LetjHj=k, and let any \\x0e;\\rbe \\x0cxed. Then for \"(^h)\\x14\\nminh2H\"(h) + 2\\rto hold with probability at least 1 \\x00\\x0e, it su\\x0eces that\\nn\\x151\\n2\\r2log2k\\n\\x0e\\n=O\\x121\\n\\r2logk\\n\\x0e\\x13\\n;\\n8.3.3 The case of in\\x0cnite H\\nWe have proved some useful theorems for the case of \\x0cnite hypothesis classes.\\nBut many hypothesis classes, including any parameterized by real numbers\\n(as in linear classi\\x0ccation) actually contain an in\\x0cnite number of functions.\\nCan we prove similar results for this setting?\\nLet\\'s start by going through something that is notthe \\\\right\" argument.\\nBetter and more general arguments exist , but this will be useful for honing\\nour intuitions about the domain.\\nSuppose we have an Hthat is parameterized by dreal numbers. Since we\\nare using a computer to represent real numbers, and IEEE double-precision\\n\\roating point ( double \\'s in C) uses 64 bits to represent a \\roating point num-\\nber, this means that our learning algorithm, assuming we\\'re using double-\\nprecision \\roating point, is parameterized by 64 dbits. Thus, our hypothesis\\nclass really consists of at most k= 264ddi\\x0berent hypotheses. From the Corol-\\nlary at the end of the previous section, we therefore \\x0cnd that, to guarantee',\n",
       " '132\\n\"(^h)\\x14\"(h\\x03)+2\\r, with to hold with probability at least 1 \\x00\\x0e, it su\\x0eces that\\nn\\x15O\\x10\\n1\\n\\r2log264d\\n\\x0e\\x11\\n=O\\x10\\nd\\n\\r2log1\\n\\x0e\\x11\\n=O\\r;\\x0e(d). (The\\r;\\x0esubscripts indicate\\nthat the last big- Ois hiding constants that may depend on \\rand\\x0e.) Thus,\\nthe number of training examples needed is at most linear in the parameters\\nof the model.\\nThe fact that we relied on 64-bit \\roating point makes this argument not\\nentirely satisfying, but the conclusion is nonetheless roughly correct: If what\\nwe try to do is minimize training error, then in order to learn \\\\well\" using a\\nhypothesis class that has dparameters, generally we\\'re going to need on the\\norder of a linear number of training examples in d.\\n(At this point, it\\'s worth noting that these results were proved for an al-\\ngorithm that uses empirical risk minimization. Thus, while the linear depen-\\ndence of sample complexity on ddoes generally hold for most discriminative\\nlearning algorithms that try to minimize training error or some approxima-\\ntion to training error, these conclusions do not always apply as readily to\\ndiscriminative learning algorithms. Giving good theoretical guarantees on\\nmany non-ERM learning algorithms is still an area of active research.)\\nThe other part of our previous argument that\\'s slightly unsatisfying is\\nthat it relies on the parameterization of H. Intuitively, this doesn\\'t seem like\\nit should matter: We had written the class of linear classi\\x0cers as h\\x12(x) =\\n1f\\x120+\\x121x1+\\x01\\x01\\x01\\x12dxd\\x150g, withn+ 1 parameters \\x120;:::;\\x12d. But it could\\nalso be written hu;v(x) = 1f(u2\\n0\\x00v2\\n0) + (u2\\n1\\x00v2\\n1)x1+\\x01\\x01\\x01(u2\\nd\\x00v2\\nd)xd\\x150g\\nwith 2d+ 2 parameters ui;vi. Yet, both of these are just de\\x0cning the same\\nH: The set of linear classi\\x0cers in ddimensions.\\nTo derive a more satisfying argument, let\\'s de\\x0cne a few more things.\\nGiven a set S=fx(i);:::;x(D)g(no relation to the training set) of points\\nx(i)2X, we say thatHshattersSifHcan realize any labeling on S.\\nI.e., if for any set of labels fy(1);:::;y(D)g, there exists some h2H so that\\nh(x(i)) =y(i)for alli= 1;:::D.\\nGiven a hypothesis class H, we then de\\x0cne its Vapnik-Chervonenkis\\ndimension , written VC(H), to be the size of the largest set that is shattered\\nbyH. (IfHcan shatter arbitrarily large sets, then VC( H) =1.)\\nFor instance, consider the following set of three points:',\n",
       " '133\\n/0 /1\\n/0 /1/0 /1\\nx\\nx12\\nCan the setHof linear classi\\x0cers in two dimensions ( h(x) = 1f\\x120+\\x121x1+\\n\\x122x2\\x150g) can shatter the set above? The answer is yes. Speci\\x0ccally, we\\nsee that, for any of the eight possible labelings of these points, we can \\x0cnd a\\nlinear classi\\x0cer that obtains \\\\zero training error\" on them:\\nx\\nx12 x\\nx12 x\\nx12 x\\nx12\\nx\\nx12 x\\nx12 x\\nx12 x\\nx12\\nMoreover, it is possible to show that there is no set of 4 points that this\\nhypothesis class can shatter. Thus, the largest set that Hcan shatter is of\\nsize 3, and hence VC( H) = 3.\\nNote that the VC dimension of Hhere is 3 even though there may be\\nsets of size 3 that it cannot shatter. For instance, if we had a set of three\\npoints lying in a straight line (left \\x0cgure), then there is no way to \\x0cnd a linear\\nseparator for the labeling of the three points shown below (right \\x0cgure):',\n",
       " '134\\nx\\nx12/0 /1\\n/0 /1\\n/0 /1x\\nx12\\nIn order words, under the de\\x0cnition of the VC dimension, in order to\\nprove that VC(H) is at least D, we need to show only that there\\'s at least\\noneset of size DthatHcan shatter.\\nThe following theorem, due to Vapnik, can then be shown. (This is, many\\nwould argue, the most important theorem in all of learning theory.)\\nTheorem. LetHbe given, and let D= VC(H). Then with probability at\\nleast 1\\x00\\x0e, we have that for all h2H,\\nj\"(h)\\x00^\"(h)j\\x14O r\\nD\\nnlogn\\nD+1\\nnlog1\\n\\x0e!\\n:\\nThus, with probability at least 1 \\x00\\x0e, we also have that:\\n\"(^h)\\x14\"(h\\x03) +O r\\nD\\nnlogn\\nD+1\\nnlog1\\n\\x0e!\\n:\\nIn other words, if a hypothesis class has \\x0cnite VC dimension, then uniform\\nconvergence occurs as nbecomes large. As before, this allows us to give a\\nbound on\"(h) in terms of \"(h\\x03). We also have the following corollary:\\nCorollary. Forj\"(h)\\x00^\"(h)j\\x14\\rto hold for all h2H (and hence \"(^h)\\x14\\n\"(h\\x03) + 2\\r) with probability at least 1 \\x00\\x0e, it su\\x0eces that n=O\\r;\\x0e(D).\\nIn other words, the number of training examples needed to learn \\\\well\"\\nusingHis linear in the VC dimension of H. It turns out that, for \\\\most\"\\nhypothesis classes, the VC dimension (assuming a \\\\reasonable\" parameter-\\nization) is also roughly linear in the number of parameters. Putting these\\ntogether, we conclude that for a given hypothesis class H(and for an algo-\\nrithm that tries to minimize training error), the number of training examples\\nneeded to achieve generalization error close to that of the optimal classi\\x0cer\\nis usually roughly linear in the number of parameters of H.',\n",
       " 'Chapter 9\\nRegularization and model\\nselection\\n9.1 Regularization\\nRecall that as discussed in Section 8.1, overftting is typically a result of using\\ntoo complex models, and we need to choose a proper model complexity to\\nachieve the optimal bias-variance tradeo\\x0b. When the model complexity is\\nmeasured by the number of parameters, we can vary the size of the model\\n(e.g., the width of a neural net). However, the correct, informative complex-\\nity measure of the models can be a function of the parameters (e.g., `2norm\\nof the parameters), which may not necessarily depend on the number of pa-\\nrameters. In such cases, we will use regularization, an important technique\\nin machine learning, control the model complexity and prevent over\\x0ctting.\\nRegularization typically involves adding an additional term, called a reg-\\nularizer and denoted by R(\\x12) here, to the training loss/cost function:\\nJ\\x15(\\x12) =J(\\x12) +\\x15R(\\x12) (9.1)\\nHereJ\\x15is often called the regularized loss, and \\x15\\x150 is called the regular-\\nization parameter. The regularizer R(\\x12) is a nonnegative function (in almost\\nall cases). In classical methods, R(\\x12) is purely a function of the parameter \\x12,\\nbut some modern approach allows R(\\x12) to depend on the training dataset.1\\nThe regularizer R(\\x12) is typically chosen to be some measure of the com-\\nplexity of the model \\x12. Thus, when using the regularized loss, we aim to\\n\\x0cnd a model that both \\x0ct the data (a small loss J(\\x12)) and have a small\\n1Here our notations generally omit the dependency on the training dataset for\\nsimplicity|we write J(\\x12) even though it obviously needs to depend on the training dataset.\\n135',\n",
       " \"136\\nmodel complexity (a small R(\\x12)). The balance between the two objectives is\\ncontrolled by the regularization parameter \\x15. When\\x15= 0, the regularized\\nloss is equivalent to the original loss. When \\x15is a su\\x0eciently small positive\\nnumber, minimizing the regularized loss is e\\x0bectively minimizing the original\\nloss with the regularizer as the tie-breaker. When the regularizer is extremely\\nlarge, then the original loss is not e\\x0bective (and likely the model will have a\\nlarge bias.)\\nThe most commonly used regularization is perhaps `2regularization,\\nwhereR(\\x12) =1\\n2k\\x12k2\\n2. It encourages the optimizer to \\x0cnd a model with\\nsmall`2norm. In deep learning, it's oftentimes referred to as weight de-\\ncay, because gradient descent with learning rate \\x11on the regularized loss\\nR\\x15(\\x12) is equivalent to shrinking/decaying \\x12by a scalar factor of 1 \\x00\\x11\\x15and\\nthen applying the standard gradient\\n\\x12 \\x12\\x00\\x11rJ\\x15(\\x12) =\\x12\\x00\\x11\\x15\\x12\\x00\\x11rJ(\\x12)\\n= (1\\x00\\x15\\x11)\\x12|{z}\\ndecaying weights\\x00\\x11rJ(\\x12) (9.2)\\nBesides encouraging simpler models, regularization can also impose in-\\nductive biases or structures on the model parameters. For example, suppose\\nwe had a prior belief that the number of non-zeros in the ground-truth model\\nparameters is small,2|which is oftentimes called sparsity of the model|, we\\ncan impose a regularization on the number of non-zeros in \\x12, denoted by\\nk\\x12k0, to leverage such a prior belief. Imposing additional structure of the\\nparameters narrows our search space and makes the complexity of the model\\nfamily smaller,|e.g., the family of sparse models can be thought of as having\\nlower complexity than the family of all models|, and thus tends to lead to a\\nbetter generalization. On the other hand, imposing additional structure may\\nrisk increasing the bias. For example, if we regularize the sparsity strongly\\nbut no sparse models can predict the label accurately, we will su\\x0ber from\\nlarge bias (analogously to the situation when we use linear models to learn\\ndata than can only be represented by quadratic functions in Section 8.1.)\\nThe sparsity of the parameters is not a continuous function of the param-\\neters, and thus we cannot optimize it with (stochastic) gradient descent. A\\ncommon relaxation is to use R(\\x12) =k\\x12k1as a continuous surrogate.3\\n2For linear models, this means the model just uses a few coordinates of the inputs to\\nmake an accurate prediction.\\n3There has been a rich line of theoretical work that explains why k\\x12k1is a good sur-\\nrogate for encouraging sparsity, but it's beyond the scope of this course. An intuition is:\\nassuming the parameter is on the unit sphere, the parameter with smallest `1norm also\",\n",
       " \"137\\nTheR(\\x12) =k\\x12k1(also called LASSO) and R(\\x12) =1\\n2k\\x12k2\\n2are perhaps\\namong the most commonly used regularizers for linear models. Other norm\\nand powers of norms are sometimes also used. The `2norm regularization is\\nmuch more commonly used with kernel methods because `1regularization is\\ntypically not compatible with the kernel trick (the optimal solution cannot\\nbe written as functions of inner products of features.)\\nIn deep learning, the most commonly used regularizer is `2regularization\\nor weight decay. Other common ones include dropout, data augmentation,\\nregularizing the spectral norm of the weight matrices, and regularizing the\\nLipschitzness of the model, etc. Regularization in deep learning is an ac-\\ntive research area, and it's known that there is another implicit source of\\nregularization, as discussed in the next section.\\n9.2 Implicit regularization e\\x0bect\\nThe implicit regularization e\\x0bect of optimizers, or implicit bias or algorithmic\\nregularization, is a new concept/phenomenon observed in the deep learning\\nera. It largely refers to that the optimizers can implicitly impose structures\\non parameters beyond what has been imposed by the regularized loss.\\nIn most classical settings, the loss or regularized loss has a unique global\\nminimum, and thus any reasonable optimizer should converge to that global\\nminimum and cannot impose any additional preferences. However, in deep\\nlearning, oftentimes the loss or regularized loss has more than one (approx-\\nimate) global minima, and di\\x0berence optimizers may converge to di\\x0berent\\nglobal minima. Though these global minima have the same or similar train-\\ning losses, they may be of di\\x0berent nature and have dramatically di\\x0berent\\ngeneralization performance. See Figures 9.1 and 9.2 and its caption for an\\nillustration and some experiment results. For example, it's possible that one\\nglobal minimum gives a much more Lipschitz or sparse model than others\\nand thus has a better test error. It turns out that many commonly-used op-\\ntimizers (or their components) prefer or bias towards \\x0cnding global minima\\nof certain properties, leading to a better test performance.\\nhappen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and\\n`1norm gives the same extremal points to some extent.\",\n",
       " \"138\\nθloss\\nFigure 9.1: An Illustration that di\\x0berent global minima of the training loss\\ncan have di\\x0berent test performance.\\nFigure 9.2: Left: Performance of neural networks trained by two di\\x0berent\\nlearning rates schedules on the CIFAR-10 dataset. Although both exper-\\niments used exactly the same regularized losses and the optimizers \\x0ct the\\ntraining data perfectly, the models' generalization performance di\\x0ber much.\\nRight: On a di\\x0berent synthetic dataset, optimizers with di\\x0berent initializa-\\ntions have the same training error but di\\x0berent generalization performance.4\\nIn summary, the takehome message here is that the choice of optimizer\\ndoes not only a\\x0bect minimizing the training loss, but also imposes implicit\\nregularization and a\\x0bects the generalization of the model. Even if your cur-\\nrent optimizer already converges to a small training error perfectly, you may\\nstill need to tune your optimizer for a better generalization, .\\n4The setting is the same as in Woodworth et al. [2020], HaoChen et al. [2020]\",\n",
       " '139\\nOne may wonder which components of the optimizers bias towards what\\ntype of global minima and what type of global minima may generalize bet-\\nter. These are open questions that researchers are actively investigating.\\nEmpirical and theoretical research have o\\x0bered some clues and heuristics.\\nIn many (but de\\x0cnitely far from all) situations, among those setting where\\noptimization can succeed in minimizing the training loss, the use of larger\\ninitial learning rate, smaller initialization, smaller batch size, and momen-\\ntum appears to help with biasing towards more generalizable solutions. A\\nconjecture (that can be proven in certain simpli\\x0ced case) is that stochas-\\nticity in the optimization process help the optimizer to \\x0cnd \\ratter global\\nminima (global minima where the curvature of the loss is small), and \\rat\\nglobal minima tend to give more Lipschitz models and better generalization.\\nCharacterizing the implicit regularization e\\x0bect formally is still a challenging\\nopen research question.\\n9.3 Model selection via cross validation\\nSuppose we are trying select among several di\\x0berent models for a learning\\nproblem. For instance, we might be using a polynomial regression model\\nh\\x12(x) =g(\\x120+\\x121x+\\x122x2+\\x01\\x01\\x01+\\x12kxk), and wish to decide if kshould be\\n0, 1, . . . , or 10. How can we automatically select a model that represents\\na good tradeo\\x0b between the twin evils of bias and variance5? Alternatively,\\nsuppose we want to automatically choose the bandwidth parameter \\x1cfor\\nlocally weighted regression, or the parameter Cfor our`1-regularized SVM.\\nHow can we do that?\\nFor the sake of concreteness, in these notes we assume we have some\\n\\x0cnite set of models M=fM1;:::;Mdgthat we\\'re trying to select among.\\nFor instance, in our \\x0crst example above, the model Miwould be an i-th\\ndegree polynomial regression model. (The generalization to in\\x0cnite Mis\\nnot hard.6) Alternatively, if we are trying to decide between using an SVM,\\na neural network or logistic regression, then Mmay contain these models.\\n5Given that we said in the previous set of notes that bias and variance are two very\\ndi\\x0berent beasts, some readers may be wondering if we should be calling them \\\\twin\" evils\\nhere. Perhaps it\\'d be better to think of them as non-identical twins. The phrase \\\\the\\nfraternal twin evils of bias and variance\" doesn\\'t have the same ring to it, though.\\n6If we are trying to choose from an in\\x0cnite set of models, say corresponding to the\\npossible values of the bandwidth \\x1c2R+, we may discretize \\x1cand consider only a \\x0cnite\\nnumber of possible values for it. More generally, most of the algorithms described here\\ncan all be viewed as performing optimization search in the space of models, and we can\\nperform this search over in\\x0cnite model classes as well.',\n",
       " '140\\nCross validation. Lets suppose we are, as usual, given a training set S.\\nGiven what we know about empirical risk minimization, here\\'s what might\\ninitially seem like a algorithm, resulting from using empirical risk minimiza-\\ntion for model selection:\\n1. Train each model MionS, to get some hypothesis hi.\\n2. Pick the hypotheses with the smallest training error.\\nThis algorithm does notwork. Consider choosing the degree of a poly-\\nnomial. The higher the degree of the polynomial, the better it will \\x0ct the\\ntraining set S, and thus the lower the training error. Hence, this method will\\nalways select a high-variance, high-degree polynomial model, which we saw\\npreviously is often poor choice.\\nHere\\'s an algorithm that works better. In hold-out cross validation\\n(also called simple cross validation ), we do the following:\\n1. Randomly split SintoStrain(say, 70% of the data) and Scv(the remain-\\ning 30%). Here, Scvis called the hold-out cross validation set.\\n2. Train each model MionStrainonly, to get some hypothesis hi.\\n3. Select and output the hypothesis hithat had the smallest error ^ \"Scv(hi)\\non the hold out cross validation set. (Here ^ \"Scv(h) denotes the average\\nerror ofhon the set of examples in Scv.) The error on the hold out\\nvalidation set is also referred to as the validation error.\\nBy testing/validating on a set of examples Scvthat the models were not\\ntrained on, we obtain a better estimate of each hypothesis hi\\'s true general-\\nization/test error. Thus, this approach is essentially picking the model with\\nthe smallest estimated generalization/test error. The size of the validation\\nset depends on the total number of available examples. Usually, somewhere\\nbetween 1=4\\x001=3 of the data is used in the hold out cross validation set, and\\n30% is a typical choice. However, when the total dataset is huge, validation\\nset can be a smaller fraction of the total examples as long as the absolute\\nnumber of validation examples is decent. For example, for the ImageNet\\ndataset that has about 1M training images, the validation set is sometimes\\nset to be 50K images, which is only about 5% of the total examples.\\nOptionally, step 3 in the algorithm may also be replaced with selecting\\nthe modelMiaccording to arg min i^\"Scv(hi), and then retraining Mion the\\nentire training set S. (This is often a good idea, with one exception being\\nlearning algorithms that are be very sensitive to perturbations of the initial',\n",
       " '141\\nconditions and/or data. For these methods, Midoing well on Straindoes not\\nnecessarily mean it will also do well on Scv, and it might be better to forgo\\nthis retraining step.)\\nThe disadvantage of using hold out cross validation is that it \\\\wastes\"\\nabout 30% of the data. Even if we were to take the optional step of retraining\\nthe model on the entire training set, it\\'s still as if we\\'re trying to \\x0cnd a good\\nmodel for a learning problem in which we had 0 :7ntraining examples, rather\\nthanntraining examples, since we\\'re testing models that were trained on\\nonly 0:7nexamples each time. While this is \\x0cne if data is abundant and/or\\ncheap, in learning problems in which data is scarce (consider a problem with\\nn= 20, say), we\\'d like to do something better.\\nHere is a method, called k-fold cross validation , that holds out less\\ndata each time:\\n1. Randomly split Sintokdisjoint subsets of m=k training examples each.\\nLets call these subsets S1;:::;Sk.\\n2. For each model Mi, we evaluate it as follows:\\nForj= 1;:::;k\\nTrain the model MionS1[\\x01\\x01\\x01[Sj\\x001[Sj+1[\\x01\\x01\\x01Sk(i.e., train\\non all the data except Sj) to get some hypothesis hij.\\nTest the hypothesis hijonSj, to get ^\"Sj(hij).\\nThe estimated generalization error of model Miis then calculated\\nas the average of the ^ \"Sj(hij)\\'s (averaged over j).\\n3. Pick the model Miwith the lowest estimated generalization error, and\\nretrain that model on the entire training set S. The resulting hypothesis\\nis then output as our \\x0cnal answer.\\nA typical choice for the number of folds to use here would be k= 10.\\nWhile the fraction of data held out each time is now 1 =k|much smaller\\nthan before|this procedure may also be more computationally expensive\\nthan hold-out cross validation, since we now need train to each model k\\ntimes.\\nWhilek= 10 is a commonly used choice, in problems in which data is\\nreally scarce, sometimes we will use the extreme choice of k=min order\\nto leave out as little data as possible each time. In this setting, we would\\nrepeatedly train on all but one of the training examples in S, and test on that\\nheld-out example. The resulting m=kerrors are then averaged together to\\nobtain our estimate of the generalization error of a model. This method has',\n",
       " '142\\nits own name; since we\\'re holding out one training example at a time, this\\nmethod is called leave-one-out cross validation.\\nFinally, even though we have described the di\\x0berent versions of cross vali-\\ndation as methods for selecting a model, they can also be used more simply to\\nevaluate a single model or algorithm. For example, if you have implemented\\nsome learning algorithm and want to estimate how well it performs for your\\napplication (or if you have invented a novel learning algorithm and want to\\nreport in a technical paper how well it performs on various test sets), cross\\nvalidation would give a reasonable way of doing so.\\n9.4 Bayesian statistics and regularization\\nIn this section, we will talk about one more tool in our arsenal for our battle\\nagainst over\\x0ctting.\\nAt the beginning of the quarter, we talked about parameter \\x0ctting using\\nmaximum likelihood estimation (MLE), and chose our parameters according\\nto\\n\\x12MLE= arg max\\n\\x12nY\\ni=1p(y(i)jx(i);\\x12):\\nThroughout our subsequent discussions, we viewed \\x12as an unknown param-\\neter of the world. This view of the \\x12as being constant-valued but unknown\\nis taken in frequentist statistics. In the frequentist this view of the world, \\x12\\nis not random|it just happens to be unknown|and it\\'s our job to come up\\nwith statistical procedures (such as maximum likelihood) to try to estimate\\nthis parameter.\\nAn alternative way to approach our parameter estimation problems is to\\ntake the Bayesian view of the world, and think of \\x12as being a random\\nvariable whose value is unknown. In this approach, we would specify a\\nprior distribution p(\\x12) on\\x12that expresses our \\\\prior beliefs\" about the\\nparameters. Given a training set S=f(x(i);y(i))gn\\ni=1, when we are asked to\\nmake a prediction on a new value of x, we can then compute the posterior\\ndistribution on the parameters\\np(\\x12jS) =p(Sj\\x12)p(\\x12)\\np(S)\\n=\\x00Qn\\ni=1p(y(i)jx(i);\\x12)\\x01\\np(\\x12)R\\n\\x12(Qn\\ni=1p(y(i)jx(i);\\x12)p(\\x12))d\\x12(9.3)\\nIn the equation above, p(y(i)jx(i);\\x12) comes from whatever model you\\'re using',\n",
       " '143\\nfor your learning problem. For example, if you are using Bayesian logistic re-\\ngression, then you might choose p(y(i)jx(i);\\x12) =h\\x12(x(i))y(i)(1\\x00h\\x12(x(i)))(1\\x00y(i)),\\nwhereh\\x12(x(i)) = 1=(1 + exp(\\x00\\x12Tx(i))).7\\nWhen we are given a new test example xand asked to make it prediction\\non it, we can compute our posterior distribution on the class label using the\\nposterior distribution on \\x12:\\np(yjx;S) =Z\\n\\x12p(yjx;\\x12)p(\\x12jS)d\\x12 (9.4)\\nIn the equation above, p(\\x12jS) comes from Equation (9.3). Thus, for example,\\nif the goal is to the predict the expected value of ygivenx, then we would\\noutput8\\nE[yjx;S] =Z\\nyyp(yjx;S)dy\\nThe procedure that we\\'ve outlined here can be thought of as doing \\\\fully\\nBayesian\" prediction, where our prediction is computed by taking an average\\nwith respect to the posterior p(\\x12jS) over\\x12. Unfortunately, in general it is\\ncomputationally very di\\x0ecult to compute this posterior distribution. This is\\nbecause it requires taking integrals over the (usually high-dimensional) \\x12as\\nin Equation (9.3), and this typically cannot be done in closed-form.\\nThus, in practice we will instead approximate the posterior distribution\\nfor\\x12. One common approximation is to replace our posterior distribution for\\n\\x12(as in Equation 9.4) with a single point estimate. The MAP (maximum\\na posteriori) estimate for \\x12is given by\\n\\x12MAP= arg max\\n\\x12nY\\ni=1p(y(i)jx(i);\\x12)p(\\x12): (9.5)\\nNote that this is the same formulas as for the MLE (maximum likelihood)\\nestimate for \\x12, except for the prior p(\\x12) term at the end.\\nIn practical applications, a common choice for the prior p(\\x12) is to assume\\nthat\\x12\\x18N(0;\\x1c2I). Using this choice of prior, the \\x0ctted parameters \\x12MAPwill\\nhave smaller norm than that selected by maximum likelihood. In practice,\\nthis causes the Bayesian MAP estimate to be less susceptible to over\\x0ctting\\nthan the ML estimate of the parameters. For example, Bayesian logistic\\nregression turns out to be an e\\x0bective algorithm for text classi\\x0ccation, even\\nthough in text classi\\x0ccation we usually have d\\x1dn.\\n7Since we are now viewing \\x12as a random variable, it is okay to condition on it value,\\nand write \\\\ p(yjx;\\x12)\" instead of \\\\ p(yjx;\\x12).\"\\n8The integral below would be replaced by a summation if yis discrete-valued.',\n",
       " 'Part IV\\nUnsupervised learning\\n144',\n",
       " 'Chapter 10\\nClustering and the k-means\\nalgorithm\\nIn the clustering problem, we are given a training set fx(1);:::;x(n)g, and\\nwant to group the data into a few cohesive \\\\clusters.\" Here, x(i)2Rd\\nas usual; but no labels y(i)are given. So, this is an unsupervised learning\\nproblem.\\nThek-means clustering algorithm is as follows:\\n1. Initialize cluster centroids \\x161;\\x162;:::;\\x16k2Rdrandomly.\\n2. Repeat until convergence: f\\nFor everyi, set\\nc(i):= arg min\\njjjx(i)\\x00\\x16jjj2:\\nFor eachj, set\\n\\x16j:=Pn\\ni=11fc(i)=jgx(i)\\nPn\\ni=11fc(i)=jg:\\ng\\nIn the algorithm above, k(a parameter of the algorithm) is the number\\nof clusters we want to \\x0cnd; and the cluster centroids \\x16jrepresent our current\\nguesses for the positions of the centers of the clusters. To initialize the cluster\\ncentroids (in step 1 of the algorithm above), we could choose ktraining\\nexamples randomly, and set the cluster centroids to be equal to the values of\\nthesekexamples. (Other initialization methods are also possible.)\\nThe inner-loop of the algorithm repeatedly carries out two steps: (i)\\n\\\\Assigning\" each training example x(i)to the closest cluster centroid \\x16j, and\\n145',\n",
       " '146\\nFigure 10.1: K-means algorithm. Training examples are shown as dots, and\\ncluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-\\ntial cluster centroids (in this instance, not chosen to be equal to two training\\nexamples). (c-f) Illustration of running two iterations of k-means. In each\\niteration, we assign each training example to the closest cluster centroid\\n(shown by \\\\painting\" the training examples the same color as the cluster\\ncentroid to which is assigned); then we move each cluster centroid to the\\nmean of the points assigned to it. (Best viewed in color.) Images courtesy\\nMichael Jordan.\\n(ii) Moving each cluster centroid \\x16jto the mean of the points assigned to it.\\nFigure 10.1 shows an illustration of running k-means.\\nIs thek-means algorithm guaranteed to converge? Yes it is, in a certain\\nsense. In particular, let us de\\x0cne the distortion function to be:\\nJ(c;\\x16) =nX\\ni=1jjx(i)\\x00\\x16c(i)jj2\\nThus,Jmeasures the sum of squared distances between each training exam-\\nplex(i)and the cluster centroid \\x16c(i)to which it has been assigned. It can\\nbe shown that k-means is exactly coordinate descent on J. Speci\\x0ccally, the\\ninner-loop of k-means repeatedly minimizes Jwith respect to cwhile holding\\n\\x16\\x0cxed, and then minimizes Jwith respect to \\x16while holding c\\x0cxed. Thus,',\n",
       " '147\\nJmust monotonically decrease, and the value of Jmust converge. (Usu-\\nally, this implies that cand\\x16will converge too. In theory, it is possible for\\nk-means to oscillate between a few di\\x0berent clusterings|i.e., a few di\\x0berent\\nvalues forcand/or\\x16|that have exactly the same value of J, but this almost\\nnever happens in practice.)\\nThe distortion function Jis a non-convex function, and so coordinate\\ndescent on Jis not guaranteed to converge to the global minimum. In other\\nwords,k-means can be susceptible to local optima. Very often k-means will\\nwork \\x0cne and come up with very good clusterings despite this. But if you\\nare worried about getting stuck in bad local minima, one common thing to\\ndo is runk-means many times (using di\\x0berent random initial values for the\\ncluster centroids \\x16j). Then, out of all the di\\x0berent clusterings found, pick\\nthe one that gives the lowest distortion J(c;\\x16).',\n",
       " \"Chapter 11\\nEM algorithms\\nIn this set of notes, we discuss the EM (Expectation-Maximization) algorithm\\nfor density estimation.\\n11.1 EM for mixture of Gaussians\\nSuppose that we are given a training set fx(1);:::;x(n)gas usual. Since we\\nare in the unsupervised learning setting, these points do not come with any\\nlabels.\\nWe wish to model the data by specifying a joint distribution p(x(i);z(i)) =\\np(x(i)jz(i))p(z(i)). Here,z(i)\\x18Multinomial( \\x1e) (where\\x1ej\\x150,Pk\\nj=1\\x1ej= 1,\\nand the parameter \\x1ejgivesp(z(i)=j)), andx(i)jz(i)=j\\x18N (\\x16j;\\x06j). We\\nletkdenote the number of values that the z(i)'s can take on. Thus, our\\nmodel posits that each x(i)was generated by randomly choosing z(i)from\\nf1;:::;kg, and then x(i)was drawn from one of kGaussians depending on\\nz(i). This is called the mixture of Gaussians model. Also, note that the\\nz(i)'s are latent random variables, meaning that they're hidden/unobserved.\\nThis is what will make our estimation problem di\\x0ecult.\\nThe parameters of our model are thus \\x1e,\\x16and \\x06. To estimate them, we\\ncan write down the likelihood of our data:\\n`(\\x1e;\\x16; \\x06) =nX\\ni=1logp(x(i);\\x1e;\\x16; \\x06)\\n=nX\\ni=1logkX\\nz(i)=1p(x(i)jz(i);\\x16;\\x06)p(z(i);\\x1e):\\nHowever, if we set to zero the derivatives of this formula with respect to\\n148\",\n",
       " '149\\nthe parameters and try to solve, we\\'ll \\x0cnd that it is not possible to \\x0cnd the\\nmaximum likelihood estimates of the parameters in closed form. (Try this\\nyourself at home.)\\nThe random variables z(i)indicate which of the kGaussians each x(i)\\nhad come from. Note that if we knew what the z(i)\\'s were, the maximum\\nlikelihood problem would have been easy. Speci\\x0ccally, we could then write\\ndown the likelihood as\\n`(\\x1e;\\x16; \\x06) =nX\\ni=1logp(x(i)jz(i);\\x16;\\x06) + logp(z(i);\\x1e):\\nMaximizing this with respect to \\x1e,\\x16and \\x06 gives the parameters:\\n\\x1ej=1\\nnnX\\ni=11fz(i)=jg;\\n\\x16j=Pn\\ni=11fz(i)=jgx(i)\\nPn\\ni=11fz(i)=jg;\\n\\x06j=Pn\\ni=11fz(i)=jg(x(i)\\x00\\x16j)(x(i)\\x00\\x16j)T\\nPn\\ni=11fz(i)=jg:\\nIndeed, we see that if the z(i)\\'s were known, then maximum likelihood\\nestimation becomes nearly identical to what we had when estimating the\\nparameters of the Gaussian discriminant analysis model, except that here\\nthez(i)\\'s playing the role of the class labels.1\\nHowever, in our density estimation problem, the z(i)\\'s are notknown.\\nWhat can we do?\\nThe EM algorithm is an iterative algorithm that has two main steps.\\nApplied to our problem, in the E-step, it tries to \\\\guess\" the values of the\\nz(i)\\'s. In the M-step, it updates the parameters of our model based on our\\nguesses. Since in the M-step we are pretending that the guesses in the \\x0crst\\npart were correct, the maximization becomes easy. Here\\'s the algorithm:\\nRepeat until convergence: f\\n(E-step) For each i;j, set\\nw(i)\\nj:=p(z(i)=jjx(i);\\x1e;\\x16; \\x06)\\n1There are other minor di\\x0berences in the formulas here from what we\\'d obtained in\\nPS1 with Gaussian discriminant analysis, \\x0crst because we\\'ve generalized the z(i)\\'s to be\\nmultinomial rather than Bernoulli, and second because here we are using a di\\x0berent \\x06 j\\nfor each Gaussian.',\n",
       " '150\\n(M-step) Update the parameters:\\n\\x1ej:=1\\nnnX\\ni=1w(i)\\nj;\\n\\x16j:=Pn\\ni=1w(i)\\njx(i)\\nPn\\ni=1w(i)\\nj;\\n\\x06j:=Pn\\ni=1w(i)\\nj(x(i)\\x00\\x16j)(x(i)\\x00\\x16j)T\\nPn\\ni=1w(i)\\nj\\ng\\nIn the E-step, we calculate the posterior probability of our parameters\\nthez(i)\\'s, given the x(i)and using the current setting of our parameters. I.e.,\\nusing Bayes rule, we obtain:\\np(z(i)=jjx(i);\\x1e;\\x16; \\x06) =p(x(i)jz(i)=j;\\x16;\\x06)p(z(i)=j;\\x1e)Pk\\nl=1p(x(i)jz(i)=l;\\x16;\\x06)p(z(i)=l;\\x1e)\\nHere,p(x(i)jz(i)=j;\\x16;\\x06) is given by evaluating the density of a Gaussian\\nwith mean \\x16jand covariance \\x06 jatx(i);p(z(i)=j;\\x1e) is given by \\x1ej, and so\\non. The values w(i)\\njcalculated in the E-step represent our \\\\soft\" guesses2for\\nthe values of z(i).\\nAlso, you should contrast the updates in the M-step with the formulas we\\nhad when the z(i)\\'s were known exactly. They are identical, except that in-\\nstead of the indicator functions \\\\1 fz(i)=jg\" indicating from which Gaussian\\neach datapoint had come, we now instead have the w(i)\\nj\\'s.\\nThe EM-algorithm is also reminiscent of the K-means clustering algo-\\nrithm, except that instead of the \\\\hard\" cluster assignments c(i), we instead\\nhave the \\\\soft\" assignments w(i)\\nj. Similar to K-means, it is also susceptible\\nto local optima, so reinitializing at several di\\x0berent initial parameters may\\nbe a good idea.\\nIt\\'s clear that the EM algorithm has a very natural interpretation of\\nrepeatedly trying to guess the unknown z(i)\\'s; but how did it come about,\\nand can we make any guarantees about it, such as regarding its convergence?\\nIn the next set of notes, we will describe a more general view of EM, one\\n2The term \\\\soft\" refers to our guesses being probabilities and taking values in [0 ;1]; in\\ncontrast, a \\\\hard\" guess is one that represents a single best guess (such as taking values\\ninf0;1gorf1;:::;kg).',\n",
       " \"151\\nthat will allow us to easily apply it to other estimation problems in which\\nthere are also latent variables, and which will allow us to give a convergence\\nguarantee.\\n11.2 Jensen's inequality\\nWe begin our discussion with a very useful result called Jensen's inequality\\nLetfbe a function whose domain is the set of real numbers. Recall that\\nfis a convex function if f00(x)\\x150 (for allx2R). In the case of ftaking\\nvector-valued inputs, this is generalized to the condition that its hessian H\\nis positive semi-de\\x0cnite ( H\\x150). Iff00(x)>0 for allx, then we say fis\\nstrictly convex (in the vector-valued case, the corresponding statement is\\nthatHmust be positive de\\x0cnite, written H > 0). Jensen's inequality can\\nthen be stated as follows:\\nTheorem. Letfbe a convex function, and let Xbe a random variable.\\nThen:\\nE[f(X)]\\x15f(EX):\\nMoreover, if fis strictly convex, then E[ f(X)] =f(EX) holds true if and\\nonly ifX= E[X] with probability 1 (i.e., if Xis a constant).\\nRecall our convention of occasionally dropping the parentheses when writ-\\ning expectations, so in the theorem above, f(EX) =f(E[X]).\\nFor an interpretation of the theorem, consider the \\x0cgure below.\\na E[X] bf(a)\\nf(b)\\nf(EX)E[f(X)]f\\nHere,fis a convex function shown by the solid line. Also, Xis a random\\nvariable that has a 0.5 chance of taking the value a, and a 0.5 chance of\",\n",
       " \"152\\ntaking the value b(indicated on the x-axis). Thus, the expected value of X\\nis given by the midpoint between aandb.\\nWe also see the values f(a),f(b) andf(E[X]) indicated on the y-axis.\\nMoreover, the value E[ f(X)] is now the midpoint on the y-axis between f(a)\\nandf(b). From our example, we see that because fis convex, it must be the\\ncase that E[ f(X)]\\x15f(EX).\\nIncidentally, quite a lot of people have trouble remembering which way\\nthe inequality goes, and remembering a picture like this is a good way to\\nquickly \\x0cgure out the answer.\\nRemark. Recall that fis [strictly] concave if and only if \\x00fis [strictly]\\nconvex (i.e., f00(x)\\x140 orH\\x140). Jensen's inequality also holds for concave\\nfunctionsf, but with the direction of all the inequalities reversed (E[ f(X)]\\x14\\nf(EX), etc.).\\n11.3 General EM algorithms\\nSuppose we have an estimation problem in which we have a training set\\nfx(1);:::;x(n)gconsisting of nindependent examples. We have a latent vari-\\nable model p(x;z;\\x12) withzbeing the latent variable (which for simplicity is\\nassumed to take \\x0cnite number of values). The density for xcan be obtained\\nby marginalized over the latent variable z:\\np(x;\\x12) =X\\nzp(x;z;\\x12) (11.1)\\nWe wish to \\x0ct the parameters \\x12by maximizing the log-likelihood of the\\ndata, de\\x0cned by\\n`(\\x12) =nX\\ni=1logp(x(i);\\x12) (11.2)\\nWe can rewrite the objective in terms of the joint density p(x;z;\\x12) by\\n`(\\x12) =nX\\ni=1logp(x(i);\\x12) (11.3)\\n=nX\\ni=1logX\\nz(i)p(x(i);z(i);\\x12): (11.4)\\nBut, explicitly \\x0cnding the maximum likelihood estimates of the parameters\\n\\x12may be hard since it will result in di\\x0ecult non-convex optimization prob-\",\n",
       " \"153\\nlems.3Here, thez(i)'s are the latent random variables; and it is often the case\\nthat if the z(i)'s were observed, then maximum likelihood estimation would\\nbe easy.\\nIn such a setting, the EM algorithm gives an e\\x0ecient method for max-\\nimum likelihood estimation. Maximizing `(\\x12) explicitly might be di\\x0ecult,\\nand our strategy will be to instead repeatedly construct a lower-bound on `\\n(E-step), and then optimize that lower-bound (M-step).4\\nIt turns out that the summationPn\\ni=1is not essential here, and towards a\\nsimpler exposition of the EM algorithm, we will \\x0crst consider optimizing the\\nthe likelihood log p(x) fora single example x. After we derive the algorithm\\nfor optimizing log p(x), we will convert it to an algorithm that works for n\\nexamples by adding back the sum to each of the relevant equations. Thus,\\nnow we aim to optimize log p(x;\\x12) which can be rewritten as\\nlogp(x;\\x12) = logX\\nzp(x;z;\\x12) (11.5)\\nLetQbe a distribution over the possible values of z. That is,P\\nzQ(z) = 1,\\nQ(z)\\x150).\\nConsider the following:5\\nlogp(x;\\x12) = logX\\nzp(x;z;\\x12)\\n= logX\\nzQ(z)p(x;z;\\x12)\\nQ(z)(11.6)\\n\\x15X\\nzQ(z) logp(x;z;\\x12)\\nQ(z)(11.7)\\nThe last step of this derivation used Jensen's inequality. Speci\\x0ccally,\\nf(x) = logxis a concave function, since f00(x) =\\x001=x2<0 over its domain\\n3It's mostly an empirical observation that the optimization problem is di\\x0ecult to op-\\ntimize.\\n4Empirically, the E-step and M-step can often be computed more e\\x0eciently than op-\\ntimizing the function `(\\x01) directly. However, it doesn't necessarily mean that alternating\\nthe two steps can always converge to the global optimum of `(\\x01). Even for mixture of\\nGaussians, the EM algorithm can either converge to a global optimum or get stuck, de-\\npending on the properties of the training data. Empirically, for real-world data, often EM\\ncan converge to a solution with relatively high likelihood (if not the optimum), and the\\ntheory behind it is still largely not understood.\\n5Ifzwere continuous, then Qwould be a density, and the summations over zin our\\ndiscussion are replaced with integrals over z.\",\n",
       " '154\\nx2R+. Also, the term\\nX\\nzQ(z)\\x14p(x;z;\\x12)\\nQ(z)\\x15\\nin the summation is just an expectation of the quantity [ p(x;z;\\x12)=Q(z)] with\\nrespect tozdrawn according to the distribution given by Q.6By Jensen\\'s\\ninequality, we have\\nf\\x12\\nEz\\x18Q\\x14p(x;z;\\x12)\\nQ(z)\\x15\\x13\\n\\x15Ez\\x18Q\\x14\\nf\\x12p(x;z;\\x12)\\nQ(z)\\x13\\x15\\n;\\nwhere the \\\\ z\\x18Q\" subscripts above indicate that the expectations are with\\nrespect tozdrawn from Q. This allowed us to go from Equation (11.6) to\\nEquation (11.7).\\nNow, for any distribution Q, the formula (11.7) gives a lower-bound on\\nlogp(x;\\x12). There are many possible choices for the Q\\'s. Which should we\\nchoose? Well, if we have some current guess \\x12of the parameters, it seems\\nnatural to try to make the lower-bound tight at that value of \\x12. I.e., we will\\nmake the inequality above hold with equality at our particular value of \\x12.\\nTo make the bound tight for a particular value of \\x12, we need for the step\\ninvolving Jensen\\'s inequality in our derivation above to hold with equality.\\nFor this to be true, we know it is su\\x0ecient that the expectation be taken\\nover a \\\\constant\"-valued random variable. I.e., we require that\\np(x;z;\\x12)\\nQ(z)=c\\nfor some constant cthat does not depend on z. This is easily accomplished\\nby choosing\\nQ(z)/p(x;z;\\x12):\\nActually, since we knowP\\nzQ(z) = 1 (because it is a distribution), this\\nfurther tells us that\\nQ(z) =p(x;z;\\x12)P\\nzp(x;z;\\x12)\\n=p(x;z;\\x12)\\np(x;\\x12)\\n=p(zjx;\\x12) (11.8)\\n6We note that the notionp(x;z;\\x12)\\nQ(z)only makes sense if Q(z)6= 0 whenever p(x;z;\\x12)6= 0.\\nHere we implicitly assume that we only consider those Qwith such a property.',\n",
       " \"155\\nThus, we simply set the Q's to be the posterior distribution of the z's given\\nxand the setting of the parameters \\x12.\\nIndeed, we can directly verify that when Q(z) =p(zjx;\\x12), then equa-\\ntion (11.7) is an equality because\\nX\\nzQ(z) logp(x;z;\\x12)\\nQ(z)=X\\nzp(zjx;\\x12) logp(x;z;\\x12)\\np(zjx;\\x12)\\n=X\\nzp(zjx;\\x12) logp(zjx;\\x12)p(x;\\x12)\\np(zjx;\\x12)\\n=X\\nzp(zjx;\\x12) logp(x;\\x12)\\n= logp(x;\\x12)X\\nzp(zjx;\\x12)\\n= logp(x;\\x12) (becauseP\\nzp(zjx;\\x12) = 1)\\nFor convenience, we call the expression in Equation (11.7) the evidence\\nlower bound (ELBO) and we denote it by\\nELBO(x;Q;\\x12) =X\\nzQ(z) logp(x;z;\\x12)\\nQ(z)(11.9)\\nWith this equation, we can re-write equation (11.7) as\\n8Q;\\x12;x; logp(x;\\x12)\\x15ELBO(x;Q;\\x12) (11.10)\\nIntuitively, the EM algorithm alternatively updates Qand\\x12by a) set-\\ntingQ(z) =p(zjx;\\x12) following Equation (11.8) so that ELBO( x;Q;\\x12) =\\nlogp(x;\\x12) forxand the current \\x12, and b) maximizing ELBO( x;Q;\\x12) w.r.t\\x12\\nwhile \\x0cxing the choice of Q.\\nRecall that all the discussion above was under the assumption that we\\naim to optimize the log-likelihood log p(x;\\x12) for a single example x. It turns\\nout that with multiple training examples, the basic idea is the same and we\\nonly needs to take a sum over examples at relevant places. Next, we will\\nbuild the evidence lower bound for multiple training examples and make the\\nEM algorithm formal.\\nRecall we have a training set fx(1);:::;x(n)g. Note that the optimal choice\\nofQisp(zjx;\\x12), and it depends on the particular example x. Therefore here\\nwe will introduce ndistributions Q1;:::;Qn, one for each example x(i). For\\neach example x(i), we can build the evidence lower bound\\nlogp(x(i);\\x12)\\x15ELBO(x(i);Qi;\\x12) =X\\nz(i)Qi(z(i)) logp(x(i);z(i);\\x12)\\nQi(z(i))\",\n",
       " \"156\\nTaking sum over all the examples, we obtain a lower bound for the log-\\nlikelihood\\n`(\\x12)\\x15X\\niELBO(x(i);Qi;\\x12) (11.11)\\n=X\\niX\\nz(i)Qi(z(i)) logp(x(i);z(i);\\x12)\\nQi(z(i))\\nForanyset of distributions Q1;:::;Qn, the formula (11.11) gives a lower-\\nbound on`(\\x12), and analogous to the argument around equation (11.8), the\\nQithat attains equality satis\\x0ces\\nQi(z(i)) =p(z(i)jx(i);\\x12)\\nThus, we simply set the Qi's to be the posterior distribution of the z(i)'s\\ngivenx(i)with the current setting of the parameters \\x12.\\nNow, for this choice of the Qi's, Equation (11.11) gives a lower-bound on\\nthe loglikelihood `that we're trying to maximize. This is the E-step. In the\\nM-step of the algorithm, we then maximize our formula in Equation (11.11)\\nwith respect to the parameters to obtain a new setting of the \\x12's. Repeatedly\\ncarrying out these two steps gives us the EM algorithm, which is as follows:\\nRepeat until convergence f\\n(E-step) For each i, set\\nQi(z(i)) :=p(z(i)jx(i);\\x12):\\n(M-step) Set\\n\\x12:= arg max\\n\\x12nX\\ni=1ELBO(x(i);Qi;\\x12)\\n= arg max\\n\\x12X\\niX\\nz(i)Qi(z(i)) logp(x(i);z(i);\\x12)\\nQi(z(i)): (11.12)\\ng\\nHow do we know if this algorithm will converge? Well, suppose \\x12(t)and\\n\\x12(t+1)are the parameters from two successive iterations of EM. We will now\\nprove that `(\\x12(t))\\x14`(\\x12(t+1)), which shows EM always monotonically im-\\nproves the log-likelihood. The key to showing this result lies in our choice of\",\n",
       " \"157\\ntheQi's. Speci\\x0ccally, on the iteration of EM in which the parameters had\\nstarted out as \\x12(t), we would have chosen Q(t)\\ni(z(i)) :=p(z(i)jx(i);\\x12(t)). We\\nsaw earlier that this choice ensures that Jensen's inequality, as applied to get\\nEquation (11.11), holds with equality, and hence\\n`(\\x12(t)) =nX\\ni=1ELBO(x(i);Q(t)\\ni;\\x12(t)) (11.13)\\nThe parameters \\x12(t+1)are then obtained by maximizing the right hand side\\nof the equation above. Thus,\\n`(\\x12(t+1))\\x15nX\\ni=1ELBO(x(i);Q(t)\\ni;\\x12(t+1))\\n(because ineqaulity (11.11) holds for all Qand\\x12)\\n\\x15nX\\ni=1ELBO(x(i);Q(t)\\ni;\\x12(t)) (see reason below)\\n=`(\\x12(t)) (by equation (11.13))\\nwhere the last inequality follows from that \\x12(t+1)is chosen explicitly to be\\narg max\\n\\x12nX\\ni=1ELBO(x(i);Q(t)\\ni;\\x12)\\nHence, EM causes the likelihood to converge monotonically. In our de-\\nscription of the EM algorithm, we said we'd run it until convergence. Given\\nthe result that we just showed, one reasonable convergence test would be\\nto check if the increase in `(\\x12) between successive iterations is smaller than\\nsome tolerance parameter, and to declare convergence if EM is improving\\n`(\\x12) too slowly.\\nRemark. If we de\\x0cne (by overloading ELBO( \\x01))\\nELBO(Q;\\x12) =nX\\ni=1ELBO(x(i);Qi;\\x12) =X\\niX\\nz(i)Qi(z(i)) logp(x(i);z(i);\\x12)\\nQi(z(i))\\n(11.14)\\nthen we know `(\\x12)\\x15ELBO(Q;\\x12) from our previous derivation. The EM\\ncan also be viewed an alternating maximization algorithm on ELBO( Q;\\x12),\\nin which the E-step maximizes it with respect to Q(check this yourself), and\\nthe M-step maximizes it with respect to \\x12.\",\n",
       " '158\\n11.3.1 Other interpretation of ELBO\\nLet ELBO( x;Q;\\x12) =P\\nzQ(z) logp(x;z;\\x12)\\nQ(z)be de\\x0cned as in equation (11.9).\\nThere are several other forms of ELBO. First, we can rewrite\\nELBO(x;Q;\\x12) = Ez\\x18Q[logp(x;z;\\x12)]\\x00Ez\\x18Q[logQ(z)]\\n= Ez\\x18Q[logp(xjz;\\x12)]\\x00DKL(Qkpz) (11.15)\\nwhere we use pzto denote the marginal distribution of z(under the distri-\\nbutionp(x;z;\\x12)), andDKL() denotes the KL divergence\\nDKL(Qkpz) =X\\nzQ(z) logQ(z)\\np(z)(11.16)\\nIn many cases, the marginal distribution of zdoes not depend on the param-\\neter\\x12. In this case, we can see that maximizing ELBO over \\x12is equivalent\\nto maximizing the \\x0crst term in (11.15). This corresponds to maximizing the\\nconditional likelihood of xconditioned on z, which is often a simpler question\\nthan the original question.\\nAnother form of ELBO( \\x01) is (please verify yourself)\\nELBO(x;Q;\\x12) = logp(x)\\x00DKL(Qkpzjx) (11.17)\\nwherepzjxis the conditional distribution of zgivenxunder the parameter\\n\\x12. This forms shows that the maximizer of ELBO( Q;\\x12) overQis obtained\\nwhenQ=pzjx, which was shown in equation (11.8) before.\\n11.4 Mixture of Gaussians revisited\\nArmed with our general de\\x0cnition of the EM algorithm, let\\'s go back to our\\nold example of \\x0ctting the parameters \\x1e,\\x16and \\x06 in a mixture of Gaussians.\\nFor the sake of brevity, we carry out the derivations for the M-step updates\\nonly for\\x1eand\\x16j, and leave the updates for \\x06 jas an exercise for the reader.\\nThe E-step is easy. Following our algorithm derivation above, we simply\\ncalculate\\nw(i)\\nj=Qi(z(i)=j) =P(z(i)=jjx(i);\\x1e;\\x16; \\x06):\\nHere, \\\\Qi(z(i)=j)\" denotes the probability of z(i)taking the value junder\\nthe distribution Qi.',\n",
       " \"159\\nNext, in the M-step, we need to maximize, with respect to our parameters\\n\\x1e;\\x16; \\x06, the quantity\\nnX\\ni=1X\\nz(i)Qi(z(i)) logp(x(i);z(i);\\x1e;\\x16; \\x06)\\nQi(z(i))\\n=nX\\ni=1kX\\nj=1Qi(z(i)=j) logp(x(i)jz(i)=j;\\x16;\\x06)p(z(i)=j;\\x1e)\\nQi(z(i)=j)\\n=nX\\ni=1kX\\nj=1w(i)\\njlog1\\n(2\\x19)d=2j\\x06jj1=2exp\\x00\\n\\x001\\n2(x(i)\\x00\\x16j)T\\x06\\x001\\nj(x(i)\\x00\\x16j)\\x01\\n\\x01\\x1ej\\nw(i)\\nj\\nLet's maximize this with respect to \\x16l. If we take the derivative with respect\\nto\\x16l, we \\x0cnd\\nr\\x16lnX\\ni=1kX\\nj=1w(i)\\njlog1\\n(2\\x19)d=2j\\x06jj1=2exp\\x00\\n\\x001\\n2(x(i)\\x00\\x16j)T\\x06\\x001\\nj(x(i)\\x00\\x16j)\\x01\\n\\x01\\x1ej\\nw(i)\\nj\\n=\\x00r\\x16lnX\\ni=1kX\\nj=1w(i)\\nj1\\n2(x(i)\\x00\\x16j)T\\x06\\x001\\nj(x(i)\\x00\\x16j)\\n=1\\n2nX\\ni=1w(i)\\nlr\\x16l2\\x16T\\nl\\x06\\x001\\nlx(i)\\x00\\x16T\\nl\\x06\\x001\\nl\\x16l\\n=nX\\ni=1w(i)\\nl\\x00\\n\\x06\\x001\\nlx(i)\\x00\\x06\\x001\\nl\\x16l\\x01\\nSetting this to zero and solving for \\x16ltherefore yields the update rule\\n\\x16l:=Pn\\ni=1w(i)\\nlx(i)\\nPn\\ni=1w(i)\\nl;\\nwhich was what we had in the previous set of notes.\\nLet's do one more example, and derive the M-step update for the param-\\neters\\x1ej. Grouping together only the terms that depend on \\x1ej, we \\x0cnd that\\nwe need to maximize\\nnX\\ni=1kX\\nj=1w(i)\\njlog\\x1ej:\\nHowever, there is an additional constraint that the \\x1ej's sum to 1, since they\\nrepresent the probabilities \\x1ej=p(z(i)=j;\\x1e). To deal with the constraint\",\n",
       " '160\\nthatPk\\nj=1\\x1ej= 1, we construct the Lagrangian\\nL(\\x1e) =nX\\ni=1kX\\nj=1w(i)\\njlog\\x1ej+\\x0c(kX\\nj=1\\x1ej\\x001);\\nwhere\\x0cis the Lagrange multiplier.7Taking derivatives, we \\x0cnd\\n@\\n@\\x1ejL(\\x1e) =nX\\ni=1w(i)\\nj\\n\\x1ej+\\x0c\\nSetting this to zero and solving, we get\\n\\x1ej=Pn\\ni=1w(i)\\nj\\n\\x00\\x0c\\nI.e.,\\x1ej/Pn\\ni=1w(i)\\nj. Using the constraint thatP\\nj\\x1ej= 1, we easily \\x0cnd\\nthat\\x00\\x0c=Pn\\ni=1Pk\\nj=1w(i)\\nj=Pn\\ni=11 =n. (This used the fact that w(i)\\nj=\\nQi(z(i)=j), and since probabilities sum to 1,P\\njw(i)\\nj= 1.) We therefore\\nhave our M-step updates for the parameters \\x1ej:\\n\\x1ej:=1\\nnnX\\ni=1w(i)\\nj:\\nThe derivation for the M-step updates to \\x06 jare also entirely straightfor-\\nward.\\n11.5 Variational inference and variational\\nauto-encoder (optional reading)\\nLoosely speaking, variational auto-encoder Kingma and Welling [2013] gen-\\nerally refers to a family of algorithms that extend the EM algorithms to more\\ncomplex models parameterized by neural networks. It extends the technique\\nof variational inference with the additional \\\\re-parametrization trick\" which\\nwill be introduced below. Variational auto-encoder may not give the best\\nperformance for many datasets, but it contains several central ideas about\\nhow to extend EM algorithms to high-dimensional continuous latent variables\\n7We don\\'t need to worry about the constraint that \\x1ej\\x150, because as we\\'ll shortly see,\\nthe solution we\\'ll \\x0cnd from this derivation will automatically satisfy that anyway.',\n",
       " \"161\\nwith non-linear models. Understanding it will likely give you the language\\nand backgrounds to understand various recent papers related to it.\\nAs a running example, we will consider the following parameterization of\\np(x;z;\\x12) by a neural network. Let \\x12be the collection of the weights of a\\nneural network g(z;\\x12) that maps z2RktoRd. Let\\nz\\x18N(0;Ik\\x02k) (11.18)\\nxjz\\x18N(g(z;\\x12);\\x1b2Id\\x02d) (11.19)\\nHereIk\\x02kdenotes identity matrix of dimension kbyk, and\\x1bis a scalar that\\nwe assume to be known for simplicity.\\nFor the Gaussian mixture models in Section 11.4, the optimal choice of\\nQ(z) =p(zjx;\\x12) for each \\x0cxed \\x12, that is the posterior distribution of z,\\ncan be analytically computed. In many more complex models such as the\\nmodel (11.19), it's intractable to compute the exact the posterior distribution\\np(zjx;\\x12).\\nRecall that from equation (11.10), ELBO is always a lower bound for any\\nchoice ofQ, and therefore, we can also aim for \\x0cnding an approximation of\\nthe true posterior distribution. Often, one has to use some particular form\\nto approximate the true posterior distribution. Let Qbe a family of Q's that\\nwe are considering, and we will aim to \\x0cnd a Qwithin the family of Qthat is\\nclosest to the true posterior distribution. To formalize, recall the de\\x0cnition of\\nthe ELBO lower bound as a function of Qand\\x12de\\x0cned in equation (11.14)\\nELBO(Q;\\x12) =nX\\ni=1ELBO(x(i);Qi;\\x12) =X\\niX\\nz(i)Qi(z(i)) logp(x(i);z(i);\\x12)\\nQi(z(i))\\nRecall that EM can be viewed as alternating maximization of\\nELBO(Q;\\x12). Here instead, we optimize the EBLO over Q2Q\\nmax\\nQ2Qmax\\n\\x12ELBO(Q;\\x12) (11.20)\\nNow the next question is what form of Q(or what structural assumptions\\nto make about Q) allows us to e\\x0eciently maximize the objective above. When\\nthe latent variable zare high-dimensional discrete variables, one popular as-\\nsumption is the mean \\x0celd assumption , which assumes that Qi(z) gives a\\ndistribution with independent coordinates, or in other words, Qican be de-\\ncomposed into Qi(z) =Q1\\ni(z1)\\x01\\x01\\x01Qk\\ni(zk). There are tremendous applications\\nof mean \\x0celd assumptions to learning generative models with discrete latent\\nvariables, and we refer to Blei et al. [2017] for a survey of these models and\",\n",
       " \"162\\ntheir impact to a wide range of applications including computational biology,\\ncomputational neuroscience, social sciences. We will not get into the details\\nabout the discrete latent variable cases, and our main focus is to deal with\\ncontinuous latent variables, which requires not only mean \\x0celd assumptions,\\nbut additional techniques.\\nWhenz2Rkis a continuous latent variable, there are several decisions to\\nmake towards successfully optimizing (11.20). First we need to give a succinct\\nrepresentation of the distribution Qibecause it is over an in\\x0cnite number of\\npoints. A natural choice is to assume Qiis a Gaussian distribution with some\\nmean and variance. We would also like to have more succinct representation\\nof the means of Qiof all the examples. Note that Qi(z(i)) is supposed to\\napproximate p(z(i)jx(i);\\x12). It would make sense let all the means of the Qi's\\nbe some function of x(i). Concretely, let q(\\x01;\\x1e);v(\\x01;\\x1e) be two functions that\\nmap from dimension dtok, which are parameterized by \\x1eand , we assume\\nthat\\nQi=N(q(x(i);\\x1e);diag(v(x(i); ))2) (11.21)\\nHere diag(w) means the k\\x02kmatrix with the entries of w2Rkon the\\ndiagonal. In other words, the distribution Qiis assumed to be a Gaussian\\ndistribution with independent coordinates, and the mean and standard de-\\nviations are governed by qandv. Often in variational auto-encoder, qandv\\nare chosen to be neural networks.8In recent deep learning literature, often\\nq;vare called encoder (in the sense of encoding the data into latent code),\\nwhereasg(z;\\x12) if often referred to as the decoder.\\nWe remark that Qiof such form in many cases are very far from a good ap-\\nproximation of the true posterior distribution. However, some approximation\\nis necessary for feasible optimization. In fact, the form of Qineeds to satisfy\\nother requirements (which happened to be satis\\x0ced by the form (11.21))\\nBefore optimizing the ELBO, let's \\x0crst verify whether we can e\\x0eciently\\nevaluate the value of the ELBO for \\x0cxed Qof the form (11.21) and \\x12. We\\nrewrite the ELBO as a function of \\x1e; ;\\x12 by\\nELBO(\\x1e; ;\\x12 ) =nX\\ni=1Ez(i)\\x18Qi\\x14\\nlogp(x(i);z(i);\\x12)\\nQi(z(i))\\x15\\n; (11.22)\\nwhereQi=N(q(x(i);\\x1e);diag(v(x(i); ))2)\\nNote that to evaluate Qi(z(i)) inside the expectation, we should be able to\\ncompute the density of Qi. To estimate the expectation Ez(i)\\x18Qi, we\\n8qandvcan also share parameters. We sweep this level of details under the rug in this\\nnote.\",\n",
       " \"163\\nshould be able to sample from distribution Qiso that we can build an\\nempirical estimator with samples. It happens that for Gaussian distribution\\nQi=N(q(x(i);\\x1e);diag(v(x(i); ))2), we are able to be both e\\x0eciently.\\nNow let's optimize the ELBO. It turns out that we can run gradient ascent\\nover\\x1e; ;\\x12 instead of alternating maximization. There is no strong need to\\ncompute the maximum over each variable at a much greater cost. (For Gaus-\\nsian mixture model in Section 11.4, computing the maximum is analytically\\nfeasible and relatively cheap, and therefore we did alternating maximization.)\\nMathematically, let \\x11be the learning rate, the gradient ascent step is\\n\\x12:=\\x12+\\x11r\\x12ELBO(\\x1e; ;\\x12 )\\n\\x1e:=\\x1e+\\x11r\\x1eELBO(\\x1e; ;\\x12 )\\n := +\\x11r ELBO(\\x1e; ;\\x12 )\\nComputing the gradient over \\x12is simple because\\nr\\x12ELBO(\\x1e; ;\\x12 ) =r\\x12nX\\ni=1Ez(i)\\x18Qi\\x14\\nlogp(x(i);z(i);\\x12)\\nQi(z(i))\\x15\\n=r\\x12nX\\ni=1Ez(i)\\x18Qi\\x02\\nlogp(x(i);z(i);\\x12)\\x03\\n=nX\\ni=1Ez(i)\\x18Qi\\x02\\nr\\x12logp(x(i);z(i);\\x12)\\x03\\n; (11.23)\\nBut computing the gradient over \\x1eand is tricky because the sam-\\npling distribution Qidepends on \\x1eand . (Abstractly speaking, the is-\\nsue we face can be simpli\\x0ced as the problem of computing the gradi-\\nent Ez\\x18Q\\x1e[f(\\x1e)] with respect to variable \\x1e. We know that in general,\\nrEz\\x18Q\\x1e[f(\\x1e)]6= Ez\\x18Q\\x1e[rf(\\x1e)] because the dependency of Q\\x1eon\\x1ehas to be\\ntaken into account as well. )\\nThe idea that comes to rescue is the so-called re-parameterization\\ntrick : we rewrite z(i)\\x18Qi=N(q(x(i);\\x1e);diag(v(x(i); ))2) in an equivalent\\nway:\\nz(i)=q(x(i);\\x1e) +v(x(i); )\\x0c\\x18(i)where\\x18(i)\\x18N(0;Ik\\x02k) (11.24)\\nHerex\\x0cydenotes the entry-wise product of two vectors of the same\\ndimension. Here we used the fact that x\\x18N(\\x16;\\x1b2) is equivalent to that\\nx=\\x16+\\x18\\x1bwith\\x18\\x18N(0;1). We mostly just used this fact in every dimension\\nsimultaneously for the random variable z(i)\\x18Qi.\",\n",
       " \"164\\nWith this re-parameterization, we have that\\nEz(i)\\x18Qi\\x14\\nlogp(x(i);z(i);\\x12)\\nQi(z(i))\\x15\\n(11.25)\\n= E\\x18(i)\\x18N(0;1)\\x14\\nlogp(x(i);q(x(i);\\x1e) +v(x(i); )\\x0c\\x18(i);\\x12)\\nQi(q(x(i);\\x1e) +v(x(i); )\\x0c\\x18(i))\\x15\\nIt follows that\\nr\\x1eEz(i)\\x18Qi\\x14\\nlogp(x(i);z(i);\\x12)\\nQi(z(i))\\x15\\n=r\\x1eE\\x18(i)\\x18N(0;1)\\x14\\nlogp(x(i);q(x(i);\\x1e) +v(x(i); )\\x0c\\x18(i);\\x12)\\nQi(q(x(i);\\x1e) +v(x(i); )\\x0c\\x18(i))\\x15\\n= E\\x18(i)\\x18N(0;1)\\x14\\nr\\x1elogp(x(i);q(x(i);\\x1e) +v(x(i); )\\x0c\\x18(i);\\x12)\\nQi(q(x(i);\\x1e) +v(x(i); )\\x0c\\x18(i))\\x15\\nWe can now sample multiple copies of \\x18(i)'s to estimate the the expecta-\\ntion in the RHS of the equation above.9We can estimate the gradient with\\nrespect to similarly, and with these, we can implement the gradient ascent\\nalgorithm to optimize the ELBO over \\x1e; ;\\x12:\\nThere are not many high-dimensional distributions with analytically com-\\nputable density function are known to be re-parameterizable. We refer to\\nKingma and Welling [2013] for a few other choices that can replace Gaussian\\ndistribution.\\n9Empirically people sometimes just use one sample to estimate it for maximum com-\\nputational e\\x0eciency.\",\n",
       " 'Chapter 12\\nPrincipal components analysis\\nIn this set of notes, we will develop a method, Principal Components Analysis\\n(PCA), that tries to identify the subspace in which the data approximately\\nlies. PCA is computationally e\\x0ecient: it will require only an eigenvector\\ncalculation (easily done with the eigfunction in Matlab).\\nSuppose we are given a dataset fx(i);i= 1;:::;ngof attributes of ndif-\\nferent types of automobiles, such as their maximum speed, turn radius, and\\nso on. Let x(i)2Rdfor eachi(d\\x1cn). But unknown to us, two di\\x0berent\\nattributes|some xiandxj|respectively give a car\\'s maximum speed mea-\\nsured in miles per hour, and the maximum speed measured in kilometers per\\nhour. These two attributes are therefore almost linearly dependent, up to\\nonly small di\\x0berences introduced by rounding o\\x0b to the nearest mph or kph.\\nThus, the data really lies approximately on an n\\x001 dimensional subspace.\\nHow can we automatically detect, and perhaps remove, this redundancy?\\nFor a less contrived example, consider a dataset resulting from a survey of\\npilots for radio-controlled helicopters, where x(i)\\n1is a measure of the piloting\\nskill of pilot i, andx(i)\\n2captures how much he/she enjoys \\rying. Because\\nRC helicopters are very di\\x0ecult to \\ry, only the most committed students,\\nones that truly enjoy \\rying, become good pilots. So, the two attributes\\nx1andx2are strongly correlated. Indeed, we might posit that that the\\ndata actually likes along some diagonal axis (the u1direction) capturing the\\nintrinsic piloting \\\\karma\" of a person, with only a small amount of noise\\nlying o\\x0b this axis. (See \\x0cgure.) How can we automatically compute this u1\\ndirection?\\n165',\n",
       " '166\\nx1x2(enjoyment)\\n(skill)1\\nuu\\n2\\nWe will shortly develop the PCA algorithm. But prior to running PCA\\nper se, typically we \\x0crst preprocess the data by normalizing each feature\\nto have mean 0 and variance 1. We do this by subtracting the mean and\\ndividing by the empirical standard deviation:\\nx(i)\\nj x(i)\\nj\\x00\\x16j\\n\\x1bj\\nwhere\\x16j=1\\nnPn\\ni=1x(i)\\njand\\x1b2\\nj=1\\nnPn\\ni=1(x(i)\\nj\\x00\\x16j)2are the mean variance of\\nfeaturej, respectively.\\nSubtracting \\x16jzeros out the mean and may be omitted for data known\\nto have zero mean (for instance, time series corresponding to speech or other\\nacoustic signals). Dividing by the standard deviation \\x1bjrescales each coor-\\ndinate to have unit variance, which ensures that di\\x0berent attributes are all\\ntreated on the same \\\\scale.\" For instance, if x1was cars\\' maximum speed in\\nmph (taking values in the high tens or low hundreds) and x2were the num-\\nber of seats (taking values around 2-4), then this renormalization rescales\\nthe di\\x0berent attributes to make them more comparable. This rescaling may\\nbe omitted if we had a priori knowledge that the di\\x0berent attributes are all\\non the same scale. One example of this is if each data point represented a\\ngrayscale image, and each x(i)\\njtook a value inf0;1;:::; 255gcorresponding\\nto the intensity value of pixel jin imagei.\\nNow, having normalized our data, how do we compute the \\\\major axis\\nof variation\" u|that is, the direction on which the data approximately lies?\\nOne way is to pose this problem as \\x0cnding the unit vector uso that when',\n",
       " '167\\nthe data is projected onto the direction corresponding to u, the variance of\\nthe projected data is maximized. Intuitively, the data starts o\\x0b with some\\namount of variance/information in it. We would like to choose a direction u\\nso that if we were to approximate the data as lying in the direction/subspace\\ncorresponding to u, as much as possible of this variance is still retained.\\nConsider the following dataset, on which we have already carried out the\\nnormalization steps:\\nNow, suppose we pick uto correspond the the direction shown in the\\n\\x0cgure below. The circles denote the projections of the original data onto this\\nline.',\n",
       " '168\\n/0/0/1/1/0/0/0/0/1/1/1/1\\n/0 /1\\n/0/0/0/0/1/1/1/1\\n/0 /1 /0/0/0/0/1/1/1/1 /0/0/0/0/0/0/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1\\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\\n/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1\\nWe see that the projected data still has a fairly large variance, and the\\npoints tend to be far from zero. In contrast, suppose had instead picked the\\nfollowing direction:\\n/0/0 /1/1\\n/0/0/0/0/1/1/1/1 /0 /1\\n/0/0/0/0/1/1/1/1/0/0/1/1\\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\\n/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\\n/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\\nHere, the projections have a signi\\x0ccantly smaller variance, and are much\\ncloser to the origin.\\nWe would like to automatically select the direction ucorresponding to\\nthe \\x0crst of the two \\x0cgures shown above. To formalize this, note that given a',\n",
       " \"169\\nunit vector uand a point x, the length of the projection of xontouis given\\nbyxTu. I.e., ifx(i)is a point in our dataset (one of the crosses in the plot),\\nthen its projection onto u(the corresponding circle in the \\x0cgure) is distance\\nxTufrom the origin. Hence, to maximize the variance of the projections, we\\nwould like to choose a unit-length uso as to maximize:\\n1\\nnnX\\ni=1(x(i)Tu)2=1\\nnnX\\ni=1uTx(i)x(i)Tu\\n=uT \\n1\\nnnX\\ni=1x(i)x(i)T!\\nu:\\nWe easily recognize that the maximizing this subject to kuk2= 1 gives the\\nprincipal eigenvector of \\x06 =1\\nnPn\\ni=1x(i)x(i)T, which is just the empirical\\ncovariance matrix of the data (assuming it has zero mean).1\\nTo summarize, we have found that if we wish to \\x0cnd a 1-dimensional\\nsubspace with with to approximate the data, we should choose uto be the\\nprincipal eigenvector of \\x06. More generally, if we wish to project our data\\ninto ak-dimensional subspace ( k<d ), we should choose u1;:::;ukto be the\\ntopkeigenvectors of \\x06. The ui's now form a new, orthogonal basis for the\\ndata.2\\nThen, to represent x(i)in this basis, we need only compute the corre-\\nsponding vector\\ny(i)=2\\n6664uT\\n1x(i)\\nuT\\n2x(i)\\n...\\nuT\\nkx(i)3\\n77752Rk:\\nThus, whereas x(i)2Rd, the vector y(i)now gives a lower, k-dimensional,\\napproximation/representation for x(i). PCA is therefore also referred to as\\nadimensionality reduction algorithm. The vectors u1;:::;ukare called\\nthe \\x0crstkprincipal components of the data.\\nRemark. Although we have shown it formally only for the case of k= 1,\\nusing well-known properties of eigenvectors it is straightforward to show that\\n1If you haven't seen this before, try using the method of Lagrange multipliers to max-\\nimizeuT\\x06usubject to that uTu= 1. You should be able to show that \\x06 u=\\x15u, for some\\n\\x15, which implies uis an eigenvector of \\x06, with eigenvalue \\x15.\\n2Because \\x06 is symmetric, the ui's will (or always can be chosen to be) orthogonal to\\neach other.\",\n",
       " '170\\nof all possible orthogonal bases u1;:::;uk, the one that we have chosen max-\\nimizesP\\niky(i)k2\\n2. Thus, our choice of a basis preserves as much variability\\nas possible in the original data.\\nPCA can also be derived by picking the basis that minimizes the ap-\\nproximation error arising from projecting the data onto the k-dimensional\\nsubspace spanned by them. (See more in homework.)\\nPCA has many applications; we will close our discussion with a few exam-\\nples. First, compression|representing x(i)\\'s with lower dimension y(i)\\'s|is\\nan obvious application. If we reduce high dimensional data to k= 2 or 3 di-\\nmensions, then we can also plot the y(i)\\'s to visualize the data. For instance,\\nif we were to reduce our automobiles data to 2 dimensions, then we can plot\\nit (one point in our plot would correspond to one car type, say) to see what\\ncars are similar to each other and what groups of cars may cluster together.\\nAnother standard application is to preprocess a dataset to reduce its\\ndimension before running a supervised learning learning algorithm with the\\nx(i)\\'s as inputs. Apart from computational bene\\x0cts, reducing the data\\'s\\ndimension can also reduce the complexity of the hypothesis class considered\\nand help avoid over\\x0ctting (e.g., linear classi\\x0cers over lower dimensional input\\nspaces will have smaller VC dimension).\\nLastly, as in our RC pilot example, we can also view PCA as a noise\\nreduction algorithm. In our example it, estimates the intrinsic \\\\piloting\\nkarma\" from the noisy measures of piloting skill and enjoyment. In class, we\\nalso saw the application of this idea to face images, resulting in eigenfaces\\nmethod. Here, each point x(i)2R100\\x02100was a 10000 dimensional vector,\\nwith each coordinate corresponding to a pixel intensity value in a 100x100\\nimage of a face. Using PCA, we represent each image x(i)with a much lower-\\ndimensional y(i). In doing so, we hope that the principal components we\\nfound retain the interesting, systematic variations between faces that capture\\nwhat a person really looks like, but not the \\\\noise\" in the images introduced\\nby minor lighting variations, slightly di\\x0berent imaging conditions, and so on.\\nWe then measure distances between faces iandjby working in the reduced\\ndimension, and computing ky(i)\\x00y(j)k2. This resulted in a surprisingly good\\nface-matching and retrieval algorithm.',\n",
       " 'Chapter 13\\nIndependent components\\nanalysis\\nOur next topic is Independent Components Analysis (ICA). Similar to PCA,\\nthis will \\x0cnd a new basis in which to represent our data. However, the goal\\nis very di\\x0berent.\\nAs a motivating example, consider the \\\\cocktail party problem.\" Here, d\\nspeakers are speaking simultaneously at a party, and any microphone placed\\nin the room records only an overlapping combination of the dspeakers\\' voices.\\nBut lets say we have ddi\\x0berent microphones placed in the room, and because\\neach microphone is a di\\x0berent distance from each of the speakers, it records a\\ndi\\x0berent combination of the speakers\\' voices. Using these microphone record-\\nings, can we separate out the original dspeakers\\' speech signals?\\nTo formalize this problem, we imagine that there is some data s2Rd\\nthat is generated via dindependent sources. What we observe is\\nx=As;\\nwhereAis an unknown square matrix called the mixing matrix . Repeated\\nobservations gives us a dataset fx(i);i= 1;:::;ng, and our goal is to recover\\nthe sources s(i)that had generated our data ( x(i)=As(i)).\\nIn our cocktail party problem, s(i)is and-dimensional vector, and s(i)\\njis\\nthe sound that speaker jwas uttering at time i. Also,x(i)in and-dimensional\\nvector, and x(i)\\njis the acoustic reading recorded by microphone jat timei.\\nLetW=A\\x001be the unmixing matrix. Our goal is to \\x0cnd W, so\\nthat given our microphone recordings x(i), we can recover the sources by\\ncomputing s(i)=Wx(i). For notational convenience, we also let wT\\nidenote\\n171',\n",
       " '172\\nthei-th row ofW, so that\\nW=2\\n64|wT\\n1|\\n...\\n|wT\\nd|3\\n75:\\nThus,wi2Rd, and thej-th source can be recovered as s(i)\\nj=wT\\njx(i).\\n13.1 ICA ambiguities\\nTo what degree can W=A\\x001be recovered? If we have no prior knowledge\\nabout the sources and the mixing matrix, it is easy to see that there are some\\ninherent ambiguities in Athat are impossible to recover, given only the x(i)\\'s.\\nSpeci\\x0ccally, let Pbe anyd-by-dpermutation matrix. This means that\\neach row and each column of Phas exactly one \\\\1.\" Here are some examples\\nof permutation matrices:\\nP=2\\n40 1 0\\n1 0 0\\n0 0 13\\n5;P=\\x140 1\\n1 0\\x15\\n;P=\\x141 0\\n0 1\\x15\\n:\\nIfzis a vector, then Pzis another vector that contains a permuted version\\nofz\\'s coordinates. Given only the x(i)\\'s, there will be no way to distinguish\\nbetweenWandPW. Speci\\x0ccally, the permutation of the original sources is\\nambiguous, which should be no surprise. Fortunately, this does not matter\\nfor most applications.\\nFurther, there is no way to recover the correct scaling of the wi\\'s. For in-\\nstance, ifAwere replaced with 2 A, and every s(i)were replaced with (0 :5)s(i),\\nthen our observed x(i)= 2A\\x01(0:5)s(i)would still be the same. More broadly,\\nif a single column of Awere scaled by a factor of \\x0b, and the corresponding\\nsource were scaled by a factor of 1 =\\x0b, then there is again no way to determine\\nthat this had happened given only the x(i)\\'s. Thus, we cannot recover the\\n\\\\correct\" scaling of the sources. However, for the applications that we are\\nconcerned with|including the cocktail party problem|this ambiguity also\\ndoes not matter. Speci\\x0ccally, scaling a speaker\\'s speech signal s(i)\\njby some\\npositive factor \\x0ba\\x0bects only the volume of that speaker\\'s speech. Also, sign\\nchanges do not matter, and s(i)\\njand\\x00s(i)\\njsound identical when played on a\\nspeaker. Thus, if the wifound by an algorithm is scaled by any non-zero real\\nnumber, the corresponding recovered source si=wT\\nixwill be scaled by the',\n",
       " '173\\nsame factor; but this usually does not matter. (These comments also apply\\nto ICA for the brain/MEG data that we talked about in class.)\\nAre these the only sources of ambiguity in ICA? It turns out that they\\nare, so long as the sources siarenon-Gaussian . To see what the di\\x0eculty is\\nwith Gaussian data, consider an example in which n= 2, ands\\x18N (0;I).\\nHere,Iis the 2x2 identity matrix. Note that the contours of the density of\\nthe standard normal distribution N(0;I) are circles centered on the origin,\\nand the density is rotationally symmetric.\\nNow, suppose we observe some x=As, whereAis our mixing matrix.\\nThen, the distribution of xwill be Gaussian, x\\x18N(0;AAT), since\\nEs\\x18N(0;I)[x] = E[As] =AE[s] = 0\\nCov[x] = Es\\x18N(0;I)[xxT] = E[AssTAT] =AE[ssT]AT=A\\x01Cov[s]\\x01AT=AAT\\nNow, letRbe an arbitrary orthogonal (less formally, a rotation/re\\rection)\\nmatrix, so that RRT=RTR=I, and letA0=AR. Then if the data had\\nbeen mixed according to A0instead ofA, we would have instead observed\\nx0=A0s. The distribution of x0is also Gaussian, x0\\x18N (0;AAT), since\\nEs\\x18N(0;I)[x0(x0)T] = E[A0ssT(A0)T] = E[ARssT(AR)T] =ARRTAT=AAT.\\nHence, whether the mixing matrix is AorA0, we would observe data from\\naN(0;AAT) distribution. Thus, there is no way to tell if the sources were\\nmixed using AandA0. There is an arbitrary rotational component in the\\nmixing matrix that cannot be determined from the data, and we cannot\\nrecover the original sources.\\nOur argument above was based on the fact that the multivariate standard\\nnormal distribution is rotationally symmetric. Despite the bleak picture that\\nthis paints for ICA on Gaussian data, it turns out that, so long as the data is\\nnotGaussian, it is possible, given enough data, to recover the dindependent\\nsources.\\n13.2 Densities and linear transformations\\nBefore moving on to derive the ICA algorithm proper, we \\x0crst digress brie\\ry\\nto talk about the e\\x0bect of linear transformations on densities.\\nSuppose a random variable sis drawn according to some density ps(s).\\nFor simplicity, assume for now that s2Ris a real number. Now, let the\\nrandom variable xbe de\\x0cned according to x=As(here,x2R;A2R). Let\\npxbe the density of x. What ispx?\\nLetW=A\\x001. To calculate the \\\\probability\" of a particular value of x,\\nit is tempting to compute s=Wx, then then evaluate psat that point, and',\n",
       " '174\\nconclude that \\\\ px(x) =ps(Wx).\" However, this is incorrect . For example,\\nlets\\x18Uniform[0;1], sops(s) = 1f0\\x14s\\x141g. Now, let A= 2, sox= 2s.\\nClearly,xis distributed uniformly in the interval [0 ;2]. Thus, its density is\\ngiven bypx(x) = (0:5)1f0\\x14x\\x142g. This does not equal ps(Wx), where\\nW= 0:5 =A\\x001. Instead, the correct formula is px(x) =ps(Wx)jWj.\\nMore generally, if sis a vector-valued distribution with density ps, and\\nx=Asfor a square, invertible matrix A, then the density of xis given by\\npx(x) =ps(Wx)\\x01jWj;\\nwhereW=A\\x001.\\nRemark. If you\\'re seen the result that Amaps [0;1]dto a set of volume jAj,\\nthen here\\'s another way to remember the formula for pxgiven above, that also\\ngeneralizes our previous 1-dimensional example. Speci\\x0ccally, let A2Rd\\x02dbe\\ngiven, and let W=A\\x001as usual. Also let C1= [0;1]dbe thed-dimensional\\nhypercube, and de\\x0cne C2=fAs:s2C1g\\x12Rdto be the image of C1\\nunder the mapping given by A. Then it is a standard result in linear algebra\\n(and, indeed, one of the ways of de\\x0cning determinants) that the volume of\\nC2is given byjAj. Now, suppose sis uniformly distributed in [0 ;1]d, so its\\ndensity isps(s) = 1fs2C1g. Then clearly xwill be uniformly distributed\\ninC2. Its density is therefore found to be px(x) = 1fx2C2g=vol(C2) (since\\nit must integrate over C2to 1). But using the fact that the determinant\\nof the inverse of a matrix is just the inverse of the determinant, we have\\n1=vol(C2) = 1=jAj=jA\\x001j=jWj. Thus,px(x) = 1fx2C2gjWj= 1fWx2\\nC1gjWj=ps(Wx)jWj.\\n13.3 ICA algorithm\\nWe are now ready to derive an ICA algorithm. We describe an algorithm\\nby Bell and Sejnowski, and we give an interpretation of their algorithm as a\\nmethod for maximum likelihood estimation. (This is di\\x0berent from their orig-\\ninal interpretation involving a complicated idea called the infomax principal\\nwhich is no longer necessary given the modern understanding of ICA.)\\nWe suppose that the distribution of each source sjis given by a density\\nps, and that the joint distribution of the sources sis given by\\np(s) =dY\\nj=1ps(sj):',\n",
       " '175\\nNote that by modeling the joint distribution as a product of marginals, we\\ncapture the assumption that the sources are independent. Using our formulas\\nfrom the previous section, this implies the following density on x=As=\\nW\\x001s:\\np(x) =dY\\nj=1ps(wT\\njx)\\x01jWj:\\nAll that remains is to specify a density for the individual sources ps.\\nRecall that, given a real-valued random variable z, its cumulative distri-\\nbution function (cdf) Fis de\\x0cned by F(z0) =P(z\\x14z0) =Rz0\\n\\x001pz(z)dzand\\nthe density is the derivative of the cdf: pz(z) =F0(z).\\nThus, to specify a density for the si\\'s, all we need to do is to specify some\\ncdf for it. A cdf has to be a monotonic function that increases from zero\\nto one. Following our previous discussion, we cannot choose the Gaussian\\ncdf, as ICA doesn\\'t work on Gaussian data. What we\\'ll choose instead as\\na reasonable \\\\default\" cdf that slowly increases from 0 to 1, is the sigmoid\\nfunctiong(s) = 1=(1 +e\\x00s). Hence,ps(s) =g0(s).1\\nThe square matrix Wis the parameter in our model. Given a training\\nsetfx(i);i= 1;:::;ng, the log likelihood is given by\\n`(W) =nX\\ni=1 dX\\nj=1logg0(wT\\njx(i)) + logjWj!\\n:\\nWe would like to maximize this in terms W. By taking derivatives and using\\nthe fact (from the \\x0crst set of notes) that rWjWj=jWj(W\\x001)T, we easily\\nderive a stochastic gradient ascent learning rule. For a training example x(i),\\nthe update rule is:\\nW:=W+\\x0b0\\nBBB@2\\n66641\\x002g(wT\\n1x(i))\\n1\\x002g(wT\\n2x(i))\\n...\\n1\\x002g(wT\\ndx(i))3\\n7775x(i)T+ (WT)\\x0011\\nCCCA;\\n1If you have prior knowledge that the sources\\' densities take a certain form, then it\\nis a good idea to substitute that in here. But in the absence of such knowledge, the\\nsigmoid function can be thought of as a reasonable default that seems to work well for\\nmany problems. Also, the presentation here assumes that either the data x(i)has been\\npreprocessed to have zero mean, or that it can naturally be expected to have zero mean\\n(such as acoustic signals). This is necessary because our assumption that ps(s) =g0(s)\\nimplies E[s] = 0 (the derivative of the logistic function is a symmetric function, and\\nhence gives a density corresponding to a random variable with zero mean), which implies\\nE[x] = E[As] = 0.',\n",
       " \"176\\nwhere\\x0bis the learning rate.\\nAfter the algorithm converges, we then compute s(i)=Wx(i)to recover\\nthe original sources.\\nRemark. When writing down the likelihood of the data, we implicitly as-\\nsumed that the x(i)'s were independent of each other (for di\\x0berent values\\nofi; note this issue is di\\x0berent from whether the di\\x0berent coordinates of\\nx(i)are independent), so that the likelihood of the training set was given\\nbyQ\\nip(x(i);W). This assumption is clearly incorrect for speech data and\\nother time series where the x(i)'s are dependent, but it can be shown that\\nhaving correlated training examples will not hurt the performance of the al-\\ngorithm if we have su\\x0ecient data. However, for problems where successive\\ntraining examples are correlated, when implementing stochastic gradient as-\\ncent, it sometimes helps accelerate convergence if we visit training examples\\nin a randomly permuted order. (I.e., run stochastic gradient ascent on a\\nrandomly shu\\x0fed copy of the training set.)\",\n",
       " 'Chapter 14\\nSelf-supervised learning and\\nfoundation models\\nDespite its huge success, supervised learning with neural networks typically\\nrelies on the availability of a labeled dataset of decent size, which is some-\\ntimes costly to collect. Recently, AI and machine learning are undergoing a\\nparadigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] and\\nGPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale and\\nare adaptable to a wide range of downstream tasks. These models, called\\nfoundation models by Bommasani et al. [2021], oftentimes leverage massive\\nunlabeled data so that much fewer labeled data in the downstream tasks are\\nneeded. Moreover, though foundation models are based on standard deep\\nlearning and transfer learning, their scale results in new emergent capabil-\\nities. These models are typically (pre-)trained by self-supervised learning\\nmethods where the supervisions/labels come from parts of the inputs.\\nThis chapter will introduce the paradigm of foundation models and basic\\nrelated concepts.\\n14.1 Pretraining and adaptation\\nThe foundation models paradigm consists of two phases: pretraining (or sim-\\nply training) and adaptation. We \\x0crst pretrain a large model on a massive\\nunlabeled dataset (e.g., billions of unlabeled images).1Then, we adapt the\\npretrained model to a downstream task (e.g., detecting cancer from scan im-\\nages). These downstream tasks are often prediction tasks with limited or\\n1Sometimes, pretraining can involve large-scale labeled datasets as well (e.g., the Ima-\\ngeNet dataset).\\n177',\n",
       " '178\\neven no labeled data. The intuition is that the pretrained models learn good\\nrepresentations that capture intrinsic semantic structure/ information about\\nthe data, and the adaptation phase customizes the model to a particular\\ndownstream task by, e.g., retrieving the information speci\\x0cc to it. For ex-\\nample, a model pretrained on massive unlabeled image data may learn good\\ngeneral visual representations/features, and we adapt the representations to\\nsolve biomedical imagining tasks.\\nWe formalize the two phases below.\\nPretraining. Suppose we have an unlabeled pretraining dataset\\nfx(1);x(2)\\x01\\x01\\x01;x(n)gthat consists of nexamples in Rd. Let\\x1e\\x12be a model that\\nis parameterized by \\x12and maps the input xto somem-dimensional represen-\\ntation\\x1e\\x12(x). (People also call \\x1e\\x12(x)2Rmthe embedding or features of the\\nexamplex.) We pretrain the model \\x12with a pretraining loss, which is often\\nan average of loss functions on all the examples: Lpre(\\x12) =1\\nnPn\\ni=1`pre(\\x12;x(i)).\\nHere`preis a so-called self-supervised loss on a single datapoint x(i), because\\nas shown later, e.g., in Section 14.3, the \\\\supervision\" comes from the data\\npointx(i)itself. It is also possible that the pretraining loss is not a sum\\nof losses on individual examples. We will discuss two pretraining losses in\\nSection 14.2 and Section 14.3.\\nWe use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,\\n2014]) to minimize Lpre(\\x12). We denote the obtained pretrained model by ^\\x12.\\nAdaptation. For a downstream task, we usually have a labeled dataset\\nf(x(1)\\ntask;y(1)\\ntask);\\x01\\x01\\x01;(x(ntask)\\ntask;y(ntask)\\ntask )gwithntaskexamples. The setting when\\nntask= 0 is called zero-shot learning|the downstream task doesn\\'t have any\\nlabeled examples. When ntaskis relatively small (say, between 1 and 50), the\\nsetting is called few-shot learning. It\\'s also pretty common to have a larger\\nntaskon the order of ranging from hundreds to tens of thousands.\\nAn adaptation algorithm generally takes in a downstream dataset and the\\npretrained model ^\\x12, and outputs a variant of ^\\x12that solves the downstream\\ntask. We will discuss below two popular and general adaptation methods,\\nlinear probe and \\x0cnetuning. In addition, two other methods speci\\x0cc to lan-\\nguage problems are introduced in 14.3.1.\\nThe linear probe approach uses a linear head on top of the representation\\nto predict the downstream labels. Mathematically, the adapted model out-\\nputsw>\\x1e^\\x12(x), wherew2Rmis a parameter to be learned, and ^\\x12is exactly\\nthe pretrained model (\\x0cxed). We can use SGD (or other optimizers) to train',\n",
       " '179\\nwon the downstream task loss to predict the task label\\nmin\\nw2Rm1\\nntaskntaskX\\ni=1`task(y(i)\\ntask;w>\\x1e^\\x12(x(i)\\ntask)) (14.1)\\nE.g., if the downstream task is a regression problem, we will have\\n`task(ytask;w>\\x1e^\\x12(xtask)) = (ytask\\x00w>\\x1e^\\x12(xtask))2.\\nThe \\x0cnetuning algorithm uses a similar structure for the downstream\\nprediction model, but also further \\x0cnetunes the pretrained model (instead\\nof keeping it \\x0cxed). Concretely, the prediction model is w>\\x1e\\x12(x) with pa-\\nrameterswand\\x12:We optimize both wand\\x12to \\x0ct the downstream data,\\nbut initialize \\x12with the pretrained model ^\\x12. The linear head wis usually\\ninitialized randomly.\\nminimize\\nw;\\x121\\nntaskntaskX\\ni=1`task(y(i)\\ntask;w>\\x1e\\x12(x(i)\\ntask)) (14.2)\\nwith initialization w random vector (14.3)\\n\\x12 ^\\x12 (14.4)\\nVarious other adaptation methods exists and are sometimes specialized\\nto the particular pretraining methods. We will discuss one of them in Sec-\\ntion 14.3.1.\\n14.2 Pretraining methods in computer vision\\nThis section introduces two concrete pretraining methods for computer vi-\\nsion: supervised pretraining and contrastive learning.\\nSupervised pretraining. Here, the pretraining dataset is a large-scale\\nlabeled dataset (e.g., ImageNet), and the pretrained models are simply a\\nneural network trained with vanilla supervised learning (with the last layer\\nbeing removed). Concretely, suppose we write the learned neural network as\\nU\\x1e^\\x12(x), whereUis the last (fully-connected) layer parameters, ^\\x12corresponds\\nto the parameters of all the other layers, and \\x1e^\\x12(x) are the penultimate\\nactivations layer (which serves as the representation). We simply discard U\\nand use\\x1e^\\x12(x) as the pretrained model.\\nContrastive learning. Contrastive learning is a self-supervised pretraining\\nmethod that uses only unlabeled data. The main intuition is that a good\\nrepresentation function \\x1e\\x12(\\x01) should map semantically similar images to sim-\\nilar representations, and that random pair of images should generally have',\n",
       " '180\\ndistinct representations. E.g., we may want to map images of two huskies to\\nsimilar representations, but a husky and an elephant should have di\\x0berent\\nrepresentations. One de\\x0cnition of similarity is that images from the same\\nclass are similar. Using this de\\x0cnition will result in the so-called supervised\\ncontrastive algorithms that work well when labeled pretraining datasets are\\navailable.\\nWithout labeled data, we can use data augmentation to generate a pair\\nof \\\\similar\" augmented images given an original image x. Data augmenta-\\ntion typically means that we apply random cropping, \\ripping, and/or color\\ntransformation on the original image xto generate a variant. We can take\\ntwo random augmentations, denoted by ^ xand ~x, of the same original image\\nx, and call them a positive pair. We observe that positive pairs of images\\nare often semantically related because they are augmentations of the same\\nimage. We will design a loss function for \\x12such that the representations of\\na positive pair, \\x1e\\x12(^x);\\x1e\\x12(~x), as close to each other as possible.\\nOn the other hand, we can also take another random image zfrom the\\npretraining dataset and generate an augmentation ^ zfromz. Note that (^ x;^z)\\nare from di\\x0berent images; therefore, with a good chance, they are not seman-\\ntically related. We call (^ x;^z) a negative or random pair.2We will design a\\nloss to push the representation of random pairs, \\x1e\\x12(^x);\\x1e\\x12(^z), far away from\\neach other.\\nThere are many recent algorithms based on the contrastive learning prin-\\nciple, and here we introduce SIMCLR [Chen et al., 2020] as an concrete\\nexample. The loss function is de\\x0cned on a batch of examples ( x1;\\x01\\x01\\x01;x(B))\\nwith batch size B. The algorithm computes two random augmentations for\\neach example x(i)in the batch, denoted by ^ x(i)and ~x(i):As a result, we\\nhave the augmented batch of 2 Bexamples: ^x1;\\x01\\x01\\x01;^x(B), ~x1;\\x01\\x01\\x01;~x(B). The\\nSIMCLR loss is de\\x0cned as3\\nLpre(\\x12) =\\x00BX\\ni=1logexp\\x00\\n\\x1e\\x12(^x(i))>\\x1e\\x12(~x(i))\\x01\\nexp (\\x1e\\x12(^x(i))>\\x1e\\x12(~x(i))) +P\\nj6=iexp (\\x1e\\x12(^x(i))>\\x1e\\x12(~x(j))):\\nThe intuition is as follows. The loss is increasing in \\x1e\\x12(^x(i))>\\x1e\\x12(~x(j)), and\\nthus minimizing the loss encourages \\x1e\\x12(^x(i))>\\x1e\\x12(~x(j)) to be small, making\\n\\x1e\\x12(^x(i)) far away from \\x1e\\x12(~x(j)). On the other hand, the loss is decreasing in\\n2Random pair may be a more accurate term because it\\'s still possible (though not\\nlikely) that xandzare semantically related, so are ^ xand ^z. But in the literature, the\\nterm negative pair seems to be also common.\\n3This is a variant and simpli\\x0ccation of the original loss that does not change the essence\\n(but may change the e\\x0eciency slightly).',\n",
       " '181\\n\\x1e\\x12(^x(i))>\\x1e\\x12(~x(i)), and thus minimizing the loss encourages \\x1e\\x12(^x(i))>\\x1e\\x12(~x(i))\\nto be large, resulting in \\x1e\\x12(^x(i)) and\\x1e\\x12(~x(i)) to be close.4\\n14.3 Pretrained large language models\\nNatural language processing is another area where pretraining models are\\nparticularly successful. In language problems, an examples typically cor-\\nresponds to a document or generally a sequence/trunk of words,5denoted\\nbyx= (x1;\\x01\\x01\\x01;xT) whereTis the length of the document/sequence,\\nxi2f1;\\x01\\x01\\x01;Vgare words in the document, and Vis the vocabulary size.6\\nA language model is a probabilistic model representing the probability of\\na document, denoted by p(x1;\\x01\\x01\\x01;xT):This probability distribution is very\\ncomplex because its support size is VT| exponential in the length of the\\ndocument. Instead of modeling the distribution of a document itself, we can\\napply the chain rule of conditional probability to decompose it as follows:\\np(x1;\\x01\\x01\\x01;xT) =p(x1)p(x2jx1)\\x01\\x01\\x01p(xTjx1;\\x01\\x01\\x01;xT\\x001): (14.5)\\nNow the support of each of the conditional probability p(xtjx1;\\x01\\x01\\x01;xt\\x001) is\\nV.\\nWe will model the conditional probability p(xtjx1;\\x01\\x01\\x01;xt\\x001) with some\\nparameterized form. To this end, we \\x0crst turn the discrete words into word\\nembeddings.\\nLetei2Rdbe the embedding of the word i2f1;2;\\x01\\x01\\x01;Vg:We call\\n[e1;\\x01\\x01\\x01;eV]2Rd\\x02Vthe embedding matrix. The most commonly used model\\nis transformer [Vaswani et al., 2017]. We will introduce the basic concepts\\nregarding the inputs and outputs of a transformer, but treat the interme-\\ndiate computation in transformer as a blackbox. We refer the students to\\nmore advanced courses or the original paper for more details. The high-level\\npipeline is visualized in Figure 14.1. Given a document ( x1;\\x01\\x01\\x01;xT), we \\x0crst\\ncompute the corresponding word embeddings ( ex1;\\x01\\x01\\x01;exT). Then, the word\\nembeddings is passed to a transformer model, which takes in a sequence of\\n4To see this, you can verify that the function \\x00logp\\np+qis decreasing in p, and increasing\\ninqwhenp;q> 0:\\n5In the practical implementations, typically all the data are concatenated into a single\\nsequence in some order, and each example typically corresponds a sub-sequence of consec-\\nutive words which may corresponds to a subset of a document or may span across multiple\\ndocuments.\\n6Technically, words may be decomposed into tokens which could be words or sub-words\\n(combinations of letters), but this note omits this technicality. In fact most commons words\\nare a single token themselves.',\n",
       " \"182\\nvectors and outputs a sequence of vectors ( c2;\\x01\\x01\\x01;cT+1):In particular, here\\nwe use the autoregressive version of the transformers which makes sure that\\nctonly depends on x1;\\x01\\x01\\x01;xt\\x001.7Thect's are often called the representations\\nor the contextualized embeddings, and we write ct=\\x1e\\x12(x1;:::;xt\\x001) where\\n\\x1e\\x12denotes the mapping from the input to the representations.\\nFigure 14.1: The input-output interface of a transformer.\\nTo learn the parameters \\x12in the transformer, we use ctto predict\\nthe conditional probability p(xtjx1;\\x01\\x01\\x01;xt\\x001).8We parameterize p(xtj\\nx1;\\x01\\x01\\x01;xt\\x001) by\\n2\\n6664p(xt= 1jx1\\x01\\x01\\x01;xt\\x001)\\np(xt= 2jx1\\x01\\x01\\x01;xt\\x001)\\n...\\np(xt=Vjx1\\x01\\x01\\x01;xt\\x001)3\\n7775= softmax( Wtct)2RV(14.6)\\n= softmax( Wt\\x1e\\x12(x1;\\x01\\x01\\x01;xt\\x001)); (14.7)\\nwhereW2RV\\x02dis a weight matrix that maps the contextualized embedding\\nctto the logits. In other words, Wtis an additional linear layer for the\\nprediction of the conditional probability. Recall that softmax( \\x01) :RV7!RV\\nmaps the logits to the probabilities:\\nsoftmax(u) =2\\n664exp(u1)PV\\ni=1exp(ui)\\n...\\nexp(uV)PV\\ni=1exp(ui)3\\n775(14.8)\\n7This property no longer holds in masked language models [Devlin et al., 2019] where\\nthe losses are also di\\x0berent.\\n8Heret\\x152 and we omit the loss for predicting p(x1) for simplicity (which also doesn't\\na\\x0bect the performance much). To formally model p(x1), an option is to prepend a special\\ntokenx0=?to the sequence, and then ask the language model to predict p(x1jx0=?).\",\n",
       " '183\\nWe train all the parameters \\x12in the transformers as well as the\\nparameters W= (W1;:::;WT) by the cross entropy loss. Let pt=\\nsoftmax(Wt\\x1e\\x12(x1;\\x01\\x01\\x01;xt\\x001))2RVbe the predicted conditional probability\\nat position t. LetWbe the concatenation of W1;\\x01\\x01\\x01;WT. The loss function\\nis often called language modeling loss and de\\x0cned as\\nL(W;\\x12) =TX\\nt=2(cross entropy loss at position t)\\n=TX\\nt=2\\x00logpt;xt; (14.9)\\nwherept;jdenotes the j-th entry of the probability vector pt.\\n14.3.1 Zero-shot learning and in-context learning\\nFor language models, there are many ways to adapt a pretrained model to\\ndownstream tasks. In this notes, we discuss three of them: \\x0cnetuning, zero-\\nshot learning, and in-context learning.\\nFinetuning is not very common for the autoregressive language models that\\nwe introduced in Section 14.3 but much more common for other variants\\nsuch as masked language models which has similar input-output interfaces\\nbut are pretrained di\\x0berently [Devlin et al., 2019]. The \\x0cnetuning method is\\nthe same as introduced generally in Section 14.1|the only question is how\\nwe de\\x0cne the prediction task with an additional linear head. One option\\nis to treat cT+1=\\x1e\\x12(x1;\\x01\\x01\\x01;xT) as the representation and use w>cT+1=\\nw>\\x1e\\x12(x1;\\x01\\x01\\x01;xT) to predict task label. As described in Section 14.1, we\\ninitialize\\x12to the pretrained model ^\\x12and then optimize both wand\\x12.\\nZero-shot adaptation or zero-shot learning is the setting where there is no\\ninput-output pairs from the downstream tasks. For language problems tasks,\\ntypically the task is formatted as a question or a cloze test form via natural\\nlanguage. For example, we can format an example as a question:\\nxtask= (xtask;1;\\x01\\x01\\x01;xtask;T) = \\\\Is the speed of light a universal constant?\"\\nThen, we compute the most likely next word predicted by the lan-\\nguage model given this question, that is, computing argmaxxT+1p(xT+1j\\nxtask;1;\\x01\\x01\\x01;xtask;T). In this case, if the most likely next word xT+1is \\\\No\",\\nthen we solve the task. (The speed of light is only a constant in vacuum).\\nWe note that there are many ways to decode the answer from the language',\n",
       " '184\\nmodels, e.g., instead of computing the argmax, we may use the language\\nmodel to generate a few words word. It is an active research question to \\x0cnd\\nthe best way to utilize the language models.\\nIn-context learning is mostly used for few-shot settings where we have a\\nfew labeled examples ( x(1)\\ntask;y(1)\\ntask);\\x01\\x01\\x01;(x(ntask)\\ntask;y(ntask)\\ntask ). Given a test example\\nxtest, we construct a document ( x1;\\x01\\x01\\x01;xT), which is more commonly called\\na \\\\prompt\" in this context, by concatenating the labeled examples and the\\ntext example in some format. For example, we may construct the prompt as\\nfollows\\nx1;\\x01\\x01\\x01;xT= \\\\Q: 2\\x183 = ? x(1)\\ntask\\nA: 5 y(1)\\ntask\\nQ: 6\\x187 = ? x(2)\\ntask\\nA: 13 y(2)\\ntask\\n\\x01\\x01\\x01\\nQ: 15\\x182 = ?\" xtest\\nThen, we let the pretrained model generate the most likely xT+1;xT+2;\\x01\\x01\\x01:\\nIn this case, if the model can \\\\learn\" that the symbol \\x18means addition from\\nthe few examples, we will obtain the following which suggests the answer is\\n17.\\nxT+1;xT+2;\\x01\\x01\\x01=\\\\A: 17\":\\nThe area of foundation models is very new and quickly growing. The notes\\nhere only attempt to introduce these models on a conceptual level with a\\nsigni\\x0ccant amount of simpli\\x0ccation. We refer the readers to other materials,\\ne.g., Bommasani et al. [2021], for more details.',\n",
       " 'Part V\\nReinforcement Learning and\\nControl\\n185',\n",
       " 'Chapter 15\\nReinforcement learning\\nWe now begin our study of reinforcement learning and adaptive control.\\nIn supervised learning, we saw algorithms that tried to make their outputs\\nmimic the labels ygiven in the training set. In that setting, the labels gave\\nan unambiguous \\\\right answer\" for each of the inputs x. In contrast, for\\nmany sequential decision making and control problems, it is very di\\x0ecult to\\nprovide this type of explicit supervision to a learning algorithm. For example,\\nif we have just built a four-legged robot and are trying to program it to walk,\\nthen initially we have no idea what the \\\\correct\" actions to take are to make\\nit walk, and so do not know how to provide explicit supervision for a learning\\nalgorithm to try to mimic.\\nIn the reinforcement learning framework, we will instead provide our al-\\ngorithms only a reward function, which indicates to the learning agent when\\nit is doing well, and when it is doing poorly. In the four-legged walking ex-\\nample, the reward function might give the robot positive rewards for moving\\nforwards, and negative rewards for either moving backwards or falling over.\\nIt will then be the learning algorithm\\'s job to \\x0cgure out how to choose actions\\nover time so as to obtain large rewards.\\nReinforcement learning has been successful in applications as diverse as\\nautonomous helicopter \\right, robot legged locomotion, cell-phone network\\nrouting, marketing strategy selection, factory control, and e\\x0ecient web-page\\nindexing. Our study of reinforcement learning will begin with a de\\x0cnition of\\ntheMarkov decision processes (MDP) , which provides the formalism in\\nwhich RL problems are usually posed.\\n186',\n",
       " \"187\\n15.1 Markov decision processes\\nA Markov decision process is a tuple ( S;A;fPsag;\\r;R ), where:\\n•Sis a set of states . (For example, in autonomous helicopter \\right, S\\nmight be the set of all possible positions and orientations of the heli-\\ncopter.)\\n•Ais a set of actions . (For example, the set of all possible directions in\\nwhich you can push the helicopter's control sticks.)\\n•Psaare the state transition probabilities. For each state s2Sand\\nactiona2A,Psais a distribution over the state space. We'll say more\\nabout this later, but brie\\ry, Psagives the distribution over what states\\nwe will transition to if we take action ain states.\\n•\\r2[0;1) is called the discount factor .\\n•R:S\\x02A7!Ris the reward function . (Rewards are sometimes also\\nwritten as a function of a state Sonly, in which case we would have\\nR:S7!R).\\nThe dynamics of an MDP proceeds as follows: We start in some state s0,\\nand get to choose some action a02Ato take in the MDP. As a result of our\\nchoice, the state of the MDP randomly transitions to some successor state\\ns1, drawn according to s1\\x18Ps0a0. Then, we get to pick another action a1.\\nAs a result of this action, the state transitions again, now to some s2\\x18Ps1a1.\\nWe then pick a2, and so on. . . . Pictorially, we can represent this process as\\nfollows:\\ns0a0\\x00!s1a1\\x00!s2a2\\x00!s3a3\\x00!:::\\nUpon visiting the sequence of states s0;s1;:::with actions a0;a1;:::, our\\ntotal payo\\x0b is given by\\nR(s0;a0) +\\rR(s1;a1) +\\r2R(s2;a2) +\\x01\\x01\\x01:\\nOr, when we are writing rewards as a function of the states only, this becomes\\nR(s0) +\\rR(s1) +\\r2R(s2) +\\x01\\x01\\x01:\\nFor most of our development, we will use the simpler state-rewards R(s),\\nthough the generalization to state-action rewards R(s;a) o\\x0bers no special\\ndi\\x0eculties.\",\n",
       " \"188\\nOur goal in reinforcement learning is to choose actions over time so as to\\nmaximize the expected value of the total payo\\x0b:\\nE\\x02\\nR(s0) +\\rR(s1) +\\r2R(s2) +\\x01\\x01\\x01\\x03\\nNote that the reward at timestep tisdiscounted by a factor of \\rt. Thus, to\\nmake this expectation large, we would like to accrue positive rewards as soon\\nas possible (and postpone negative rewards as long as possible). In economic\\napplications where R(\\x01) is the amount of money made, \\ralso has a natural\\ninterpretation in terms of the interest rate (where a dollar today is worth\\nmore than a dollar tomorrow).\\nApolicy is any function \\x19:S7!Amapping from the states to the\\nactions. We say that we are executing some policy \\x19if, whenever we are\\nin states, we take action a=\\x19(s). We also de\\x0cne the value function for\\na policy\\x19according to\\nV\\x19(s) = E\\x02\\nR(s0) +\\rR(s1) +\\r2R(s2) +\\x01\\x01\\x01\\x0c\\x0cs0=s;\\x19]:\\nV\\x19(s) is simply the expected sum of discounted rewards upon starting in\\nstates, and taking actions according to \\x19.1\\nGiven a \\x0cxed policy \\x19, its value function V\\x19satis\\x0ces the Bellman equa-\\ntions :\\nV\\x19(s) =R(s) +\\rX\\ns02SPs\\x19(s)(s0)V\\x19(s0):\\nThis says that the expected sum of discounted rewards V\\x19(s) for starting\\ninsconsists of two terms: First, the immediate reward R(s) that we get\\nright away simply for starting in state s, and second, the expected sum of\\nfuture discounted rewards. Examining the second term in more detail, we\\nsee that the summation term above can be rewritten E s0\\x18Ps\\x19(s)[V\\x19(s0)]. This\\nis the expected sum of discounted rewards for starting in state s0, wheres0\\nis distributed according Ps\\x19(s), which is the distribution over where we will\\nend up after taking the \\x0crst action \\x19(s) in the MDP from state s. Thus, the\\nsecond term above gives the expected sum of discounted rewards obtained\\nafter the \\x0crst step in the MDP.\\nBellman's equations can be used to e\\x0eciently solve for V\\x19. Speci\\x0ccally,\\nin a \\x0cnite-state MDP ( jSj<1), we can write down one such equation for\\nV\\x19(s) for every state s. This gives us a set of jSjlinear equations in jSj\\nvariables (the unknown V\\x19(s)'s, one for each state), which can be e\\x0eciently\\nsolved for the V\\x19(s)'s.\\n1This notation in which we condition on \\x19isn't technically correct because \\x19isn't a\\nrandom variable, but this is quite standard in the literature.\",\n",
       " '189\\nWe also de\\x0cne the optimal value function according to\\nV\\x03(s) = max\\n\\x19V\\x19(s): (15.1)\\nIn other words, this is the best possible expected sum of discounted rewards\\nthat can be attained using any policy. There is also a version of Bellman\\'s\\nequations for the optimal value function:\\nV\\x03(s) =R(s) + max\\na2A\\rX\\ns02SPsa(s0)V\\x03(s0): (15.2)\\nThe \\x0crst term above is the immediate reward as before. The second term\\nis the maximum over all actions aof the expected future sum of discounted\\nrewards we\\'ll get upon after action a. You should make sure you understand\\nthis equation and see why it makes sense.\\nWe also de\\x0cne a policy \\x19\\x03:S7!Aas follows:\\n\\x19\\x03(s) = arg max\\na2AX\\ns02SPsa(s0)V\\x03(s0): (15.3)\\nNote that\\x19\\x03(s) gives the action athat attains the maximum in the \\\\max\"\\nin Equation (15.2).\\nIt is a fact that for every state sand every policy \\x19, we have\\nV\\x03(s) =V\\x19\\x03(s)\\x15V\\x19(s):\\nThe \\x0crst equality says that the V\\x19\\x03, the value function for \\x19\\x03, is equal to the\\noptimal value function V\\x03for every state s. Further, the inequality above\\nsays that\\x19\\x03\\'s value is at least a large as the value of any other other policy.\\nIn other words, \\x19\\x03as de\\x0cned in Equation (15.3) is the optimal policy.\\nNote that\\x19\\x03has the interesting property that it is the optimal policy\\nforallstatess. Speci\\x0ccally, it is not the case that if we were starting in\\nsome state sthen there\\'d be some optimal policy for that state, and if we\\nwere starting in some other state s0then there\\'d be some other policy that\\'s\\noptimal policy for s0. The same policy \\x19\\x03attains the maximum in Equa-\\ntion (15.1) for allstatess. This means that we can use the same policy \\x19\\x03\\nno matter what the initial state of our MDP is.\\n15.2 Value iteration and policy iteration\\nWe now describe two e\\x0ecient algorithms for solving \\x0cnite-state MDPs. For\\nnow, we will consider only MDPs with \\x0cnite state and action spaces ( jSj<',\n",
       " '190\\n1;jAj<1). In this section, we will also assume that we know the state\\ntransition probabilities fPsagand the reward function R.\\nThe \\x0crst algorithm, value iteration , is as follows:\\nAlgorithm 4 Value Iteration\\n1:For each state s, initializeV(s) := 0.\\n2:foruntil convergence do\\n3: For every state, update\\nV(s) :=R(s) + max\\na2A\\rX\\ns0Psa(s0)V(s0): (15.4)\\nThis algorithm can be thought of as repeatedly trying to update the\\nestimated value function using Bellman Equations (15.2).\\nThere are two possible ways of performing the updates in the inner loop of\\nthe algorithm. In the \\x0crst, we can \\x0crst compute the new values for V(s) for\\nevery state s, and then overwrite all the old values with the new values. This\\nis called a synchronous update. In this case, the algorithm can be viewed as\\nimplementing a \\\\Bellman backup operator\" that takes a current estimate of\\nthe value function, and maps it to a new estimate. (See homework problem\\nfor details.) Alternatively, we can also perform asynchronous updates.\\nHere, we would loop over the states (in some order), updating the values one\\nat a time.\\nUnder either synchronous or asynchronous updates, it can be shown that\\nvalue iteration will cause Vto converge to V\\x03. Having found V\\x03, we can\\nthen use Equation (15.3) to \\x0cnd the optimal policy.\\nApart from value iteration, there is a second standard algorithm for \\x0cnd-\\ning an optimal policy for an MDP. The policy iteration algorithm proceeds\\nas follows:\\nThus, the inner-loop repeatedly computes the value function for the cur-\\nrent policy, and then updates the policy using the current value function.\\n(The policy \\x19found in step (b) is also called the policy that is greedy with\\nrespect to V.) Note that step (a) can be done via solving Bellman\\'s equa-\\ntions as described earlier, which in the case of a \\x0cxed policy, is just a set of\\njSjlinear equations in jSjvariables.\\nAfter at most a \\x0cnite number of iterations of this algorithm, Vwill con-\\nverge toV\\x03, and\\x19will converge to \\x19\\x03.2\\n2Note that value iteration cannot reach the exact V\\x03in a \\x0cnite number of iterations,',\n",
       " \"191\\nAlgorithm 5 Policy Iteration\\n1:Initialize\\x19randomly.\\n2:foruntil convergence do\\n3: LetV:=V\\x19. .typically by linear system solver\\n4: For each state s, let\\n\\x19(s) := arg max\\na2AX\\ns0Psa(s0)V(s0):\\nBoth value iteration and policy iteration are standard algorithms for solv-\\ning MDPs, and there isn't currently universal agreement over which algo-\\nrithm is better. For small MDPs, policy iteration is often very fats and\\nconverges with very few iterations. However, for MDPs with large state\\nspaces, solving for V\\x19explicitly would involve solving a large system of lin-\\near equations, and could be di\\x0ecult (and note that one has to solve the\\nlinear system multiple times in policy iteration). In these problems, value\\niteration may be preferred. For this reason, in practice value iteration seems\\nto be used more often than policy iteration. For some more discussions on\\nthe comparison and connection of value iteration and policy iteration, please\\nsee Section 15.5.\\n15.3 Learning a model for an MDP\\nSo far, we have discussed MDPs and algorithms for MDPs assuming that the\\nstate transition probabilities and rewards are known. In many realistic prob-\\nlems, we are not given state transition probabilities and rewards explicitly,\\nbut must instead estimate them from data. (Usually, S;A and\\rare known.)\\nFor example, suppose that, for the inverted pendulum problem (see prob-\\nwhereas policy iteration with an exact linear system solver, can. This is because when\\nthe actions space and policy space are discrete and \\x0cnite, and once the policy reaches the\\noptimal policy in policy iteration, then it will not change at all. On the other hand, even\\nthough value iteration will converge to the V\\x03, but there is always some non-zero error in\\nthe learned value function.\",\n",
       " '192\\nlem set 4), we had a number of trials in the MDP, that proceeded as follows:\\ns(1)\\n0a(1)\\n0\\x00!s(1)\\n1a(1)\\n1\\x00!s(1)\\n2a(1)\\n2\\x00!s(1)\\n3a(1)\\n3\\x00!:::\\ns(2)\\n0a(2)\\n0\\x00!s(2)\\n1a(2)\\n1\\x00!s(2)\\n2a(2)\\n2\\x00!s(2)\\n3a(2)\\n3\\x00!:::\\n:::\\nHere,s(j)\\niis the state we were at time iof trialj, anda(j)\\niis the cor-\\nresponding action that was taken from that state. In practice, each of the\\ntrials above might be run until the MDP terminates (such as if the pole falls\\nover in the inverted pendulum problem), or it might be run for some large\\nbut \\x0cnite number of timesteps.\\nGiven this \\\\experience\" in the MDP consisting of a number of trials,\\nwe can then easily derive the maximum likelihood estimates for the state\\ntransition probabilities:\\nPsa(s0) =#times took we action ain statesand got to s0\\n#times we took action a in state s(15.5)\\nOr, if the ratio above is \\\\0/0\"|corresponding to the case of never having\\ntaken action ain statesbefore|the we might simply estimate Psa(s0) to be\\n1=jSj. (I.e., estimate Psato be the uniform distribution over all states.)\\nNote that, if we gain more experience (observe more trials) in the MDP,\\nthere is an e\\x0ecient way to update our estimated state transition probabilities\\nusing the new experience. Speci\\x0ccally, if we keep around the counts for both\\nthe numerator and denominator terms of (15.5), then as we observe more\\ntrials, we can simply keep accumulating those counts. Computing the ratio\\nof these counts then given our estimate of Psa.\\nUsing a similar procedure, if Ris unknown, we can also pick our estimate\\nof the expected immediate reward R(s) in statesto be the average reward\\nobserved in state s.\\nHaving learned a model for the MDP, we can then use either value it-\\neration or policy iteration to solve the MDP using the estimated transition\\nprobabilities and rewards. For example, putting together model learning and\\nvalue iteration, here is one possible algorithm for learning in an MDP with\\nunknown state transition probabilities:\\n1. Initialize \\x19randomly.\\n2. Repeatf\\n(a) Execute \\x19in the MDP for some number of trials.',\n",
       " \"193\\n(b) Using the accumulated experience in the MDP, update our esti-\\nmates forPsa(andR, if applicable).\\n(c) Apply value iteration with the estimated state transition probabil-\\nities and rewards to get a new estimated value function V.\\n(d) Update \\x19to be the greedy policy with respect to V.\\ng\\nWe note that, for this particular algorithm, there is one simple optimiza-\\ntion that can make it run much more quickly. Speci\\x0ccally, in the inner loop\\nof the algorithm where we apply value iteration, if instead of initializing value\\niteration with V= 0, we initialize it with the solution found during the pre-\\nvious iteration of our algorithm, then that will provide value iteration with\\na much better initial starting point and make it converge more quickly.\\n15.4 Continuous state MDPs\\nSo far, we've focused our attention on MDPs with a \\x0cnite number of states.\\nWe now discuss algorithms for MDPs that may have an in\\x0cnite number of\\nstates. For example, for a car, we might represent the state as ( x;y;\\x12; _x;_y;_\\x12),\\ncomprising its position ( x;y); orientation \\x12; velocity in the xandydirections\\n_xand _y; and angular velocity _\\x12. Hence,S=R6is an in\\x0cnite set of states,\\nbecause there is an in\\x0cnite number of possible positions and orientations\\nfor the car.3Similarly, the inverted pendulum you saw in PS4 has states\\n(x;\\x12; _x;_\\x12), where\\x12is the angle of the pole. And, a helicopter \\rying in 3d\\nspace has states of the form ( x;y;z;\\x1e;\\x12; ; _x;_y;_z;_\\x1e;_\\x12;_ ), where here the roll\\n\\x1e, pitch\\x12, and yaw angles specify the 3d orientation of the helicopter.\\nIn this section, we will consider settings where the state space is S=Rd,\\nand describe ways for solving such MDPs.\\n15.4.1 Discretization\\nPerhaps the simplest way to solve a continuous-state MDP is to discretize\\nthe state space, and then to use an algorithm like value iteration or policy\\niteration, as described previously.\\nFor example, if we have 2d states ( s1;s2), we can use a grid to discretize\\nthe state space:\\n3Technically, \\x12is an orientation and so the range of \\x12is better written \\x122[\\x00\\x19;\\x19) than\\n\\x122R; but for our purposes, this distinction is not important.\",\n",
       " '194\\n[t]\\nHere, each grid cell represents a separate discrete state \\x16 s. We can\\nthen approximate the continuous-state MDP via a discrete-state one\\n(\\x16S;A;fP\\x16sag;\\r;R ), where \\x16Sis the set of discrete states, fP\\x16sagare our state\\ntransition probabilities over the discrete states, and so on. We can then use\\nvalue iteration or policy iteration to solve for the V\\x03(\\x16s) and\\x19\\x03(\\x16s) in the\\ndiscrete state MDP ( \\x16S;A;fP\\x16sag;\\r;R ). When our actual system is in some\\ncontinuous-valued state s2Sand we need to pick an action to execute, we\\ncompute the corresponding discretized state \\x16 s, and execute action \\x19\\x03(\\x16s).\\nThis discretization approach can work well for many problems. However,\\nthere are two downsides. First, it uses a fairly naive representation for V\\x03\\n(and\\x19\\x03). Speci\\x0ccally, it assumes that the value function is takes a constant\\nvalue over each of the discretization intervals (i.e., that the value function is\\npiecewise constant in each of the gridcells).\\nTo better understand the limitations of such a representation, consider a\\nsupervised learning problem of \\x0ctting a function to this dataset:\\n[t]\\n1 2 3 4 5 6 7 81.522.533.544.555.5\\nxy',\n",
       " \"195\\nClearly, linear regression would do \\x0cne on this problem. However, if we\\ninstead discretize the x-axis, and then use a representation that is piecewise\\nconstant in each of the discretization intervals, then our \\x0ct to the data would\\nlook like this:\\n[t]\\n1 2 3 4 5 6 7 81.522.533.544.555.5\\nxy\\nThis piecewise constant representation just isn't a good representation for\\nmany smooth functions. It results in little smoothing over the inputs, and no\\ngeneralization over the di\\x0berent grid cells. Using this sort of representation,\\nwe would also need a very \\x0cne discretization (very small grid cells) to get a\\ngood approximation.\\nA second downside of this representation is called the curse of dimen-\\nsionality . SupposeS=Rd, and we discretize each of the ddimensions of the\\nstate intokvalues. Then the total number of discrete states we have is kd.\\nThis grows exponentially quickly in the dimension of the state space d, and\\nthus does not scale well to large problems. For example, with a 10d state, if\\nwe discretize each state variable into 100 values, we would have 10010= 1020\\ndiscrete states, which is far too many to represent even on a modern desktop\\ncomputer.\\nAs a rule of thumb, discretization usually works extremely well for 1d\\nand 2d problems (and has the advantage of being simple and quick to im-\\nplement). Perhaps with a little bit of cleverness and some care in choosing\\nthe discretization method, it often works well for problems with up to 4d\\nstates. If you're extremely clever, and somewhat lucky, you may even get it\\nto work for some 6d problems. But it very rarely works for problems any\\nhigher dimensional than that.\",\n",
       " '196\\n15.4.2 Value function approximation\\nWe now describe an alternative method for \\x0cnding policies in continuous-\\nstate MDPs, in which we approximate V\\x03directly, without resorting to dis-\\ncretization. This approach, called value function approximation, has been\\nsuccessfully applied to many RL problems.\\nUsing a model or simulator\\nTo develop a value function approximation algorithm, we will assume that\\nwe have a model , orsimulator , for the MDP. Informally, a simulator is\\na black-box that takes as input any (continuous-valued) state stand action\\nat, and outputs a next-state st+1sampled according to the state transition\\nprobabilities Pstat:\\n[t]\\nThere are several ways that one can get such a model. One is to use\\nphysics simulation. For example, the simulator for the inverted pendulum\\nin PS4 was obtained by using the laws of physics to calculate what position\\nand orientation the cart/pole will be in at time t+ 1, given the current state\\nat timetand the action ataken, assuming that we know all the parameters\\nof the system such as the length of the pole, the mass of the pole, and so\\non. Alternatively, one can also use an o\\x0b-the-shelf physics simulation software\\npackage which takes as input a complete physical description of a mechanical\\nsystem, the current state stand actionat, and computes the state st+1of the\\nsystem a small fraction of a second into the future.4\\nAn alternative way to get a model is to learn one from data collected in\\nthe MDP. For example, suppose we execute ntrials in which we repeatedly\\ntake actions in an MDP, each trial for Ttimesteps. This can be done picking\\nactions at random, executing some speci\\x0cc policy, or via some other way of\\n4Open Dynamics Engine (http://www.ode.com) is one example of a free/open-source\\nphysics simulator that can be used to simulate systems like the inverted pendulum, and\\nthat has been a reasonably popular choice among RL researchers.',\n",
       " \"197\\nchoosing actions. We would then observe nstate sequences like the following:\\ns(1)\\n0a(1)\\n0\\x00!s(1)\\n1a(1)\\n1\\x00!s(1)\\n2a(1)\\n2\\x00!\\x01\\x01\\x01a(1)\\nT\\x001\\x00!s(1)\\nT\\ns(2)\\n0a(2)\\n0\\x00!s(2)\\n1a(2)\\n1\\x00!s(2)\\n2a(2)\\n2\\x00!\\x01\\x01\\x01a(2)\\nT\\x001\\x00!s(2)\\nT\\n\\x01\\x01\\x01\\ns(n)\\n0a(n)\\n0\\x00!s(n)\\n1a(n)\\n1\\x00!s(n)\\n2a(n)\\n2\\x00!\\x01\\x01\\x01a(n)\\nT\\x001\\x00!s(n)\\nT\\nWe can then apply a learning algorithm to predict st+1as a function of st\\nandat.\\nFor example, one may choose to learn a linear model of the form\\nst+1=Ast+Bat; (15.6)\\nusing an algorithm similar to linear regression. Here, the parameters of the\\nmodel are the matrices AandB, and we can estimate them using the data\\ncollected from our ntrials, by picking\\narg min\\nA;BnX\\ni=1T\\x001X\\nt=0\\r\\r\\rs(i)\\nt+1\\x00\\x10\\nAs(i)\\nt+Ba(i)\\nt\\x11\\r\\r\\r2\\n2:\\nWe could also potentially use other loss functions for learning the model.\\nFor example, it has been found in recent work Luo et al. [2018] that using\\nk\\x01k 2norm (without the square) may be helpful in certain cases.\\nHaving learned AandB, one option is to build a deterministic model,\\nin which given an input standat, the output st+1is exactly determined.\\nSpeci\\x0ccally, we always compute st+1according to Equation (15.6). Alter-\\nnatively, we may also build a stochastic model, in which st+1is a random\\nfunction of the inputs, by modeling it as\\nst+1=Ast+Bat+\\x0ft;\\nwhere here \\x0ftis a noise term, usually modeled as \\x0ft\\x18N(0;\\x06). (The covari-\\nance matrix \\x06 can also be estimated from data in a straightforward way.)\\nHere, we've written the next-state st+1as a linear function of the current\\nstate and action; but of course, non-linear functions are also possible. Specif-\\nically, one can learn a model st+1=A\\x1es(st) +B\\x1ea(at), where\\x1esand\\x1eaare\\nsome non-linear feature mappings of the states and actions. Alternatively,\\none can also use non-linear learning algorithms, such as locally weighted lin-\\near regression, to learn to estimate st+1as a function of standat. These\\napproaches can also be used to build either deterministic or stochastic sim-\\nulators of an MDP.\",\n",
       " '198\\nFitted value iteration\\nWe now describe the \\x0ctted value iteration algorithm for approximating\\nthe value function of a continuous state MDP. In the sequel, we will assume\\nthat the problem has a continuous state space S=Rd, but that the action\\nspaceAis small and discrete.5\\nRecall that in value iteration, we would like to perform the update\\nV(s) :=R(s) +\\rmax\\naZ\\ns0Psa(s0)V(s0)ds0(15.7)\\n=R(s) +\\rmax\\naEs0\\x18Psa[V(s0)] (15.8)\\n(In Section 15.2, we had written the value iteration update with a summation\\nV(s) :=R(s) +\\rmaxaP\\ns0Psa(s0)V(s0) rather than an integral over states;\\nthe new notation re\\rects that we are now working in continuous states rather\\nthan discrete states.)\\nThe main idea of \\x0ctted value iteration is that we are going to approxi-\\nmately carry out this step, over a \\x0cnite sample of states s(1);:::;s(n). Specif-\\nically, we will use a supervised learning algorithm|linear regression in our\\ndescription below|to approximate the value function as a linear or non-linear\\nfunction of the states:\\nV(s) =\\x12T\\x1e(s):\\nHere,\\x1eis some appropriate feature mapping of the states.\\nFor each state sin our \\x0cnite sample of nstates, \\x0ctted value iteration\\nwill \\x0crst compute a quantity y(i), which will be our approximation to R(s) +\\n\\rmaxaEs0\\x18Psa[V(s0)] (the right hand side of Equation 15.8). Then, it will\\napply a supervised learning algorithm to try to get V(s) close toR(s) +\\n\\rmaxaEs0\\x18Psa[V(s0)] (or, in other words, to try to get V(s) close toy(i)).\\nIn detail, the algorithm is as follows:\\n1. Randomly sample nstatess(1);s(2);:::s(n)2S.\\n2. Initialize \\x12:= 0.\\n3. Repeatf\\nFori= 1;:::;nf\\n5In practice, most MDPs have much smaller action spaces than state spaces. E.g., a car\\nhas a 6d state space, and a 2d action space (steering and velocity controls); the inverted\\npendulum has a 4d state space, and a 1d action space; a helicopter has a 12d state space,\\nand a 4d action space. So, discretizing this set of actions is usually less of a problem than\\ndiscretizing the state space would have been.',\n",
       " \"199\\nFor each action a2Af\\nSamples0\\n1;:::;s0\\nk\\x18Ps(i)a(using a model of the MDP).\\nSetq(a) =1\\nkPk\\nj=1R(s(i)) +\\rV(s0\\nj)\\n==Hence,q(a) is an estimate of R(s(i)) +\\n\\rEs0\\x18Ps(i)a[V(s0)].\\ng\\nSety(i)= maxaq(a).\\n==Hence,y(i)is an estimate of R(s(i)) +\\n\\rmaxaEs0\\x18Ps(i)a[V(s0)].\\ng\\n==In the original value iteration algorithm (over discrete states)\\n==we updated the value function according to V(s(i)) :=y(i).\\n==In this algorithm, we want V(s(i))\\x19y(i), which we'll achieve\\n==using supervised learning (linear regression).\\nSet\\x12:= arg min \\x121\\n2Pn\\ni=1\\x00\\n\\x12T\\x1e(s(i))\\x00y(i)\\x012\\ng\\nAbove, we had written out \\x0ctted value iteration using linear regression\\nas the algorithm to try to make V(s(i)) close toy(i). That step of the algo-\\nrithm is completely analogous to a standard supervised learning (regression)\\nproblem in which we have a training set ( x(1);y(1));(x(2);y(2));:::; (x(n);y(n)),\\nand want to learn a function mapping from xtoy; the only di\\x0berence is that\\nheresplays the role of x. Even though our description above used linear re-\\ngression, clearly other regression algorithms (such as locally weighted linear\\nregression) can also be used.\\nUnlike value iteration over a discrete set of states, \\x0ctted value iteration\\ncannot be proved to always to converge. However, in practice, it often does\\nconverge (or approximately converge), and works well for many problems.\\nNote also that if we are using a deterministic simulator/model of the MDP,\\nthen \\x0ctted value iteration can be simpli\\x0ced by setting k= 1 in the algorithm.\\nThis is because the expectation in Equation (15.8) becomes an expectation\\nover a deterministic distribution, and so a single example is su\\x0ecient to\\nexactly compute that expectation. Otherwise, in the algorithm above, we\\nhad to draw ksamples, and average to try to approximate that expectation\\n(see the de\\x0cnition of q(a), in the algorithm pseudo-code).\",\n",
       " \"200\\nFinally, \\x0ctted value iteration outputs V, which is an approximation to\\nV\\x03. This implicitly de\\x0cnes our policy. Speci\\x0ccally, when our system is in\\nsome state s, and we need to choose an action, we would like to choose the\\naction\\narg max\\naEs0\\x18Psa[V(s0)] (15.9)\\nThe process for computing/approximating this is similar to the inner-loop of\\n\\x0ctted value iteration, where for each action, we sample s0\\n1;:::;s0\\nk\\x18Psato\\napproximate the expectation. (And again, if the simulator is deterministic,\\nwe can setk= 1.)\\nIn practice, there are often other ways to approximate this step as well.\\nFor example, one very common case is if the simulator is of the form st+1=\\nf(st;at) +\\x0ft, wherefis some deterministic function of the states (such as\\nf(st;at) =Ast+Bat), and\\x0fis zero-mean Gaussian noise. In this case, we\\ncan pick the action given by\\narg max\\naV(f(s;a)):\\nIn other words, here we are just setting \\x0ft= 0 (i.e., ignoring the noise in\\nthe simulator), and setting k= 1. Equivalent, this can be derived from\\nEquation (15.9) using the approximation\\nEs0[V(s0)]\\x19V(Es0[s0]) (15.10)\\n=V(f(s;a)); (15.11)\\nwhere here the expectation is over the random s0\\x18Psa. So long as the noise\\nterms\\x0ftare small, this will usually be a reasonable approximation.\\nHowever, for problems that don't lend themselves to such approximations,\\nhaving to sample kjAjstates using the model, in order to approximate the\\nexpectation above, can be computationally expensive.\\n15.5 Connections between Policy and Value\\nIteration (Optional)\\nIn the policy iteration, line 3 of Algorithm 5, we typically use linear system\\nsolver to compute V\\x19. Alternatively, one can also the iterative Bellman\\nupdates, similarly to the value iteration, to evaluate V\\x19, as in the Procedure\\nVE(\\x01) in Line 1 of Algorithm 6 below. Here if we take option 1 in Line 2 of\\nthe Procedure VE, then the di\\x0berence between the Procedure VE from the\",\n",
       " '201\\nAlgorithm 6 Variant of Policy Iteration\\n1:procedure VE(\\x19,k) .To evaluate V\\x19\\n2: Option 1: initialize V(s) := 0; Option 2: Initialize from the current\\nVin the main algorithm.\\n3: fori= 0 tok\\x001do\\n4: For every state s, update\\nV(s) :=R(s) +\\rX\\ns0Ps\\x19(s)(s0)V(s0): (15.12)\\nreturnV\\n5:\\nRequire: hyperparameter k.\\n6:Initialize\\x19randomly.\\n7:foruntil convergence do\\n8: LetV= VE(\\x19;k).\\n9: For each state s, let\\n\\x19(s) := arg max\\na2AX\\ns0Psa(s0)V(s0): (15.13)',\n",
       " '202\\nvalue iteration (Algorithm 4) is that on line 4, the procedure is using the\\naction from \\x19instead of the greedy action.\\nUsing the Procedure VE, we can build Algorithm 6, which is a variant\\nof policy iteration that serves an intermediate algorithm that connects pol-\\nicy iteration and value iteration. Here we are going to use option 2 in VE\\nto maximize the re-use of knowledge learned before. One can verify indeed\\nthat if we take k= 1 and use option 2 in Line 2 in Algorithm 6, then Algo-\\nrithm 6 is semantically equivalent to value iteration (Algorithm 4). In other\\nwords, both Algorithm 6 and value iteration interleave the updates in (15.13)\\nand (15.12). Algorithm 6 alternate between ksteps of update (15.12) and\\none step of (15.13), whereas value iteration alternates between 1 steps of up-\\ndate (15.12) and one step of (15.13). Therefore generally Algorithm 6 should\\nnot be faster than value iteration, because assuming that update (15.12)\\nand (15.13) are equally useful and time-consuming, then the optimal balance\\nof the update frequencies could be just k= 1 ork\\x191.\\nOn the other hand, if ksteps of update (15.12) can be done much faster\\nthanktimes a single step of (15.12), then taking additional steps of equa-\\ntion (15.12) in group might be useful. This is what policy iteration is lever-\\naging | the linear system solver can give us the result of Procedure VE with\\nk=1much faster than using the Procedure VE for a large k. On the \\rip\\nside, when such a speeding-up e\\x0bect no longer exists, e.g.,, when the state\\nspace is large and linear system solver is also not fast, then value iteration is\\nmore preferable.',\n",
       " \"Chapter 16\\nLQR, DDP and LQG\\n16.1 Finite-horizon MDPs\\nIn Chapter 15, we de\\x0cned Markov Decision Processes (MDPs) and covered\\nValue Iteration / Policy Iteration in a simpli\\x0ced setting. More speci\\x0ccally we\\nintroduced the optimal Bellman equation that de\\x0cnes the optimal value\\nfunctionV\\x19\\x03of the optimal policy \\x19\\x03.\\nV\\x19\\x03(s) =R(s) + max\\na2A\\rX\\ns02SPsa(s0)V\\x19\\x03(s0)\\nRecall that from the optimal value function, we were able to recover the\\noptimal policy \\x19\\x03with\\n\\x19\\x03(s) = argmaxa2AX\\ns02SPsa(s0)V\\x03(s0)\\nIn this chapter, we'll place ourselves in a more general setting:\\n1. We want to write equations that make sense for both the discrete and\\nthe continuous case. We'll therefore write\\nEs0\\x18Psa\\x02\\nV\\x19\\x03(s0)\\x03\\ninstead ofX\\ns02SPsa(s0)V\\x19\\x03(s0)\\nmeaning that we take the expectation of the value function at the next\\nstate. In the \\x0cnite case, we can rewrite the expectation as a sum over\\n203\",\n",
       " \"204\\nstates. In the continuous case, we can rewrite the expectation as an\\nintegral. The notation s0\\x18Psameans that the state s0is sampled from\\nthe distribution Psa.\\n2. We'll assume that the rewards depend on both states and actions . In\\nother words, R:S\\x02A! R. This implies that the previous mechanism\\nfor computing the optimal action is changed into\\n\\x19\\x03(s) = argmaxa2AR(s;a) +\\rEs0\\x18Psa\\x02\\nV\\x19\\x03(s0)\\x03\\n3. Instead of considering an in\\x0cnite horizon MDP, we'll assume that we\\nhave a \\x0cnite horizon MDP that will be de\\x0cned as a tuple\\n(S;A;Psa;T;R )\\nwithT > 0 the time horizon (for instance T= 100). In this setting,\\nour de\\x0cnition of payo\\x0b is going to be (slightly) di\\x0berent:\\nR(s0;a0) +R(s1;a1) +\\x01\\x01\\x01+R(sT;aT)\\ninstead of (in\\x0cnite horizon case)\\nR(s0;a0) +\\rR(s1;a1) +\\r2R(s2;a2) +:::\\n1X\\nt=0R(st;at)\\rt\\nWhat happened to the discount factor \\r?Remember that the intro-\\nduction of\\rwas (partly) justi\\x0ced by the necessity of making sure that\\nthe in\\x0cnite sum would be \\x0cnite and well-de\\x0cned. If the rewards are\\nbounded by a constant \\x16R, the payo\\x0b is indeed bounded by\\nj1X\\nt=0R(st)\\rtj\\x14\\x16R1X\\nt=0\\rt\\nand we recognize a geometric sum! Here, as the payo\\x0b is a \\x0cnite sum,\\nthe discount factor \\ris not necessary anymore.\",\n",
       " \"205\\nIn this new setting, things behave quite di\\x0berently. First, the optimal\\npolicy\\x19\\x03might be non-stationary, meaning that it changes over time .\\nIn other words, now we have\\n\\x19(t):S!A\\nwhere the superscript ( t) denotes the policy at time step t. The dynam-\\nics of the \\x0cnite horizon MDP following policy \\x19(t)proceeds as follows:\\nwe start in some state s0, take some action a0:=\\x19(0)(s0) according to\\nour policy at time step 0. The MDP transitions to a successor s1, drawn\\naccording to Ps0a0. Then, we get to pick another action a1:=\\x19(1)(s1)\\nfollowing our new policy at time step 1 and so on...\\nWhy does the optimal policy happen to be non-stationary in the \\x0cnite-\\nhorizon setting? Intuitively, as we have a \\x0cnite numbers of actions to\\ntake, we might want to adopt di\\x0berent strategies depending on where\\nwe are in the environment and how much time we have left. Imagine\\na grid with 2 goals with rewards +1 and +10. At the beginning, we\\nmight want to take actions to aim for the +10 goal. But if after some\\nsteps, dynamics somehow pushed us closer to the +1 goal and we don't\\nhave enough steps left to be able to reach the +10 goal, then a better\\nstrategy would be to aim for the +1 goal...\\n4. This observation allows us to use time dependent dynamics\\nst+1\\x18P(t)\\nst;at\\nmeaning that the transition's distribution P(t)\\nst;atchanges over time. The\\nsame thing can be said about R(t). Note that this setting is a better\\nmodel for real life. In a car, the gas tank empties, tra\\x0ec changes,\\netc. Combining the previous remarks, we'll use the following general\\nformulation for our \\x0cnite horizon MDP\\n\\x00\\nS;A;P(t)\\nsa;T;R(t)\\x01\\nRemark : notice that the above formulation would be equivalent to\\nadding the time into the state.\",\n",
       " \"206\\nThe value function at time tfor a policy \\x19is then de\\x0cned in the same\\nway as before, as an expectation over trajectories generated following\\npolicy\\x19starting in state s.\\nVt(s) =E\\x02\\nR(t)(st;at) +\\x01\\x01\\x01+R(T)(sT;aT)jst=s;\\x19\\x03\\nNow, the question is\\nIn this \\x0cnite-horizon setting, how do we \\x0cnd the optimal value function\\nV\\x03\\nt(s) = max\\n\\x19V\\x19\\nt(s)\\nIt turns out that Bellman's equation for Value Iteration is made for Dy-\\nnamic Programming . This may come as no surprise as Bellman is one of\\nthe fathers of dynamic programming and the Bellman equation is strongly\\nrelated to the \\x0celd. To understand how we can simplify the problem by\\nadopting an iteration-based approach, we make the following observations:\\n1. Notice that at the end of the game (for time step T), the optimal value\\nis obvious\\n8s2S:V\\x03\\nT(s) := max\\na2AR(T)(s;a) (16.1)\\n2. For another time step 0 \\x14t < T , if we suppose that we know the\\noptimal value function for the next time step V\\x03\\nt+1, then we have\\n8t<T;s2S:V\\x03\\nt(s) := max\\na2Ah\\nR(t)(s;a) +Es0\\x18P(t)\\nsa\\x02\\nV\\x03\\nt+1(s0)\\x03i\\n(16.2)\\nWith these observations in mind, we can come up with a clever algorithm\\nto solve for the optimal value function:\\n1. compute V\\x03\\nTusing equation (16.1).\\n2. fort=T\\x001;:::; 0:\\ncomputeV\\x03\\ntusingV\\x03\\nt+1using equation (16.2)\",\n",
       " \"207\\nSide note We can interpret standard value iteration as a special case\\nof this general case, but without keeping track of time. It turns out that\\nin the standard setting, if we run value iteration for T steps, we get a \\rT\\napproximation of the optimal value iteration (geometric convergence). See\\nproblem set 4 for a proof of the following result:\\nTheorem LetBdenote the Bellman update and jjf(x)jj1:= supxjf(x)j.\\nIfVtdenotes the value function at the t-th step, then\\njjVt+1\\x00V\\x03jj1=jjB(Vt)\\x00V\\x03jj1\\n\\x14\\rjjVt\\x00V\\x03jj1\\n\\x14\\rtjjV1\\x00V\\x03jj1\\nIn other words, the Bellman operator Bis a\\r-contracting operator.\\n16.2 Linear Quadratic Regulation (LQR)\\nIn this section, we'll cover a special case of the \\x0cnite-horizon setting described\\nin Section 16.1, for which the exact solution is (easily) tractable. This\\nmodel is widely used in robotics, and a common technique in many problems\\nis to reduce the formulation to this framework.\\nFirst, let's describe the model's assumptions. We place ourselves in the\\ncontinuous setting, with\\nS=Rd;A=Rd\\nand we'll assume linear transitions (with noise)\\nst+1=Atst+Btat+wt\\nwhereAt2Rd\\x02d;Bt2Rd\\x02dare matrices and wt\\x18N (0;\\x06t) is some\\ngaussian noise (with zero mean). As we'll show in the following paragraphs,\\nit turns out that the noise, as long as it has zero mean, does not impact the\\noptimal policy!\\nWe'll also assume quadratic rewards\\nR(t)(st;at) =\\x00s>\\ntUtst\\x00a>\\ntWtat\",\n",
       " \"208\\nwhereUt2Rd\\x02n;Wt2Rd\\x02dare positive de\\x0cnite matrices (meaning that\\nthe reward is always negative ).\\nRemark Note that the quadratic formulation of the reward is equivalent\\nto saying that we want our state to be close to the origin (where the reward\\nis higher). For example, if Ut=Id(the identity matrix) and Wt=Id, then\\nRt=\\x00jjstjj2\\x00jjatjj2, meaning that we want to take smooth actions (small\\nnorm ofat) to go back to the origin (small norm of st). This could model a\\ncar trying to stay in the middle of lane without making impulsive moves...\\nNow that we have de\\x0cned the assumptions of our LQR model, let's cover\\nthe 2 steps of the LQR algorithm\\nstep 1 suppose that we don't know the matrices A;B; \\x06. To esti-\\nmate them, we can follow the ideas outlined in the Value Ap-\\nproximation section of the RL notes. First, collect transitions\\nfrom an arbitrary policy. Then, use linear regression to \\x0cnd\\nargminA;BPn\\ni=1PT\\x001\\nt=0\\r\\r\\rs(i)\\nt+1\\x00\\x10\\nAs(i)\\nt+Ba(i)\\nt\\x11\\r\\r\\r2\\n. Finally, use a tech-\\nnique seen in Gaussian Discriminant Analysis to learn \\x06.\\nstep 2 assuming that the parameters of our model are known (given or esti-\\nmated with step 1), we can derive the optimal policy using dynamic\\nprogramming.\\nIn other words, given\\n(\\nst+1 =Atst+Btat+wtAt;Bt;Ut;Wt;\\x06tknown\\nR(t)(st;at) =\\x00s>\\ntUtst\\x00a>\\ntWtat\\nwe want to compute V\\x03\\nt. If we go back to section 16.1, we can apply\\ndynamic programming, which yields\\n1.Initialization step\\nFor the last time step T,\\nV\\x03\\nT(sT) = max\\naT2ART(sT;aT)\\n= max\\naT2A\\x00s>\\nTUTsT\\x00a>\\nTWtaT\\n=\\x00s>\\nTUtsT (maximized for aT= 0)\",\n",
       " '209\\n2.Recurrence step\\nLett<T . Suppose we know V\\x03\\nt+1.\\nFact 1: It can be shown that if V\\x03\\nt+1is a quadratic function in st, thenV\\x03\\nt\\nis also a quadratic function. In other words, there exists some matrix \\x08\\nand some scalar \\t such that\\nifV\\x03\\nt+1(st+1) =s>\\nt+1\\x08t+1st+1+ \\tt+1\\nthenV\\x03\\nt(st) =s>\\nt\\x08tst+ \\tt\\nFor time step t=T, we had \\x08 t=\\x00UTand \\tT= 0.\\nFact 2: We can show that the optimal policy is just a linear function of\\nthe state.\\nKnowingV\\x03\\nt+1is equivalent to knowing \\x08 t+1and \\tt+1, so we just need\\nto explain how we compute \\x08 tand \\ttfrom \\x08t+1and \\tt+1and the other\\nparameters of the problem.\\nV\\x03\\nt(st) =s>\\nt\\x08tst+ \\tt\\n= max\\nath\\nR(t)(st;at) +Est+1\\x18P(t)\\nst;at[V\\x03\\nt+1(st+1)]i\\n= max\\nat\\x02\\n\\x00s>\\ntUtst\\x00a>\\ntVtat+Est+1\\x18N(Atst+Btat;\\x06t)[s>\\nt+1\\x08t+1st+1+ \\tt+1]\\x03\\nwhere the second line is just the de\\x0cnition of the optimal value function\\nand the third line is obtained by plugging in the dynamics of our model\\nalong with the quadratic assumption. Notice that the last expression is\\na quadratic function in atand can thus be (easily) optimized1. We get\\nthe optimal action a\\x03\\nt\\na\\x03\\nt=\\x02\\n(B>\\nt\\x08t+1Bt\\x00Vt)\\x001Bt\\x08t+1At\\x03\\n\\x01st\\n=Lt\\x01st\\nwhere\\nLt:=\\x02\\n(B>\\nt\\x08t+1Bt\\x00Wt)\\x001Bt\\x08t+1At\\x03\\n1Use the identity E\\x02\\nw>\\nt\\x08t+1wt\\x03\\n= Tr(\\x06t\\x08t+1) withwt\\x18N (0;\\x06t)',\n",
       " \"210\\nwhich is an impressive result: our optimal policy is linear inst. Given\\na\\x03\\ntwe can solve for \\x08 tand \\tt. We \\x0cnally get the Discrete Ricatti\\nequations\\n\\x08t=A>\\nt\\x10\\n\\x08t+1\\x00\\x08t+1Bt\\x00\\nB>\\nt\\x08t+1Bt\\x00Wt\\x01\\x001Bt\\x08t+1\\x11\\nAt\\x00Ut\\n\\tt=\\x00tr (\\x06t\\x08t+1) + \\tt+1\\nFact 3: we notice that \\x08 tdepends on neither \\t nor the noise \\x06 t! AsLt\\nis a function of At;Btand \\x08t+1, it implies that the optimal policy also\\ndoes not depend on the noise ! (But \\t tdoes depend on \\x06 t, which\\nimplies that V\\x03\\ntdepends on \\x06 t.)\\nThen, to summarize, the LQR algorithm works as follows\\n1. (if necessary) estimate parameters At;Bt;\\x06t\\n2. initialize \\x08 T:=\\x00UTand \\tT:= 0.\\n3. iterate from t=T\\x001:::0 to update \\x08 tand \\ttusing \\x08t+1and \\tt+1\\nusing the discrete Ricatti equations. If there exists a policy that drives\\nthe state towards zero, then convergence is guaranteed!\\nUsing Fact 3 , we can be even more clever and make our algorithm run\\n(slightly) faster! As the optimal policy does not depend on \\t t, and the\\nupdate of \\x08 tonly depends on \\x08 t, it is su\\x0ecient to update only \\x08t!\\n16.3 From non-linear dynamics to LQR\\nIt turns out that a lot of problems can be reduced to LQR, even if dynamics\\nare non-linear. While LQR is a nice formulation because we are able to come\\nup with a nice exact solution, it is far from being general. Let's take for\\ninstance the case of the inverted pendulum. The transitions between states\\nlook like\\n0\\nBB@xt+1\\n_xt+1\\n\\x12t+1\\n_\\x12t+11\\nCCA=F0\\nBB@0\\nBB@xt\\n_xt\\n\\x12t\\n_\\x12t1\\nCCA;at1\\nCCA\\nwhere the function Fdepends on the cos of the angle etc. Now, the\\nquestion we may ask is\\nCan we linearize this system?\",\n",
       " \"211\\n16.3.1 Linearization of dynamics\\nLet's suppose that at time t, the system spends most of its time in some state\\n\\x16stand the actions we perform are around \\x16 at. For the inverted pendulum, if\\nwe reached some kind of optimal, this is true: our actions are small and we\\ndon't deviate much from the vertical.\\nWe are going to use Taylor expansion to linearize the dynamics. In the\\nsimple case where the state is one-dimensional and the transition function F\\ndoes not depend on the action, we would write something like\\nst+1=F(st)\\x19F(\\x16st) +F0(\\x16st)\\x01(st\\x00\\x16st)\\nIn the more general setting, the formula looks the same, with gradients\\ninstead of simple derivatives\\nst+1\\x19F(\\x16st;\\x16at) +rsF(\\x16st;\\x16at)\\x01(st\\x00\\x16st) +raF(\\x16st;\\x16at)\\x01(at\\x00\\x16at) (16.3)\\nand now,st+1is linear instandat, because we can rewrite equation (16.3)\\nas\\nst+1\\x19Ast+Bst+\\x14\\nwhere\\x14is some constant and A;B are matrices. Now, this writing looks\\nawfully similar to the assumptions made for LQR. We just have to get rid\\nof the constant term \\x14! It turns out that the constant term can be absorbed\\nintostby arti\\x0ccially increasing the dimension by one. This is the same trick\\nthat we used at the beginning of the class for linear regression...\\n16.3.2 Di\\x0berential Dynamic Programming (DDP)\\nThe previous method works well for cases where the goal is to stay around\\nsome state s\\x03(think about the inverted pendulum, or a car having to stay\\nin the middle of a lane). However, in some cases, the goal can be more\\ncomplicated.\\nWe'll cover a method that applies when our system has to follow some\\ntrajectory (think about a rocket). This method is going to discretize the\\ntrajectory into discrete time steps, and create intermediary goals around\\nwhich we will be able to use the previous technique! This method is called\\nDi\\x0berential Dynamic Programming . The main steps are\",\n",
       " '212\\nstep 1 come up with a nominal trajectory using a naive controller, that approx-\\nimate the trajectory we want to follow. In other words, our controller\\nis able to approximate the gold trajectory with\\ns\\x03\\n0;a\\x03\\n0!s\\x03\\n1;a\\x03\\n1!:::\\nstep 2 linearize the dynamics around each trajectory point s\\x03\\nt, in other words\\nst+1\\x19F(s\\x03\\nt;a\\x03\\nt) +rsF(s\\x03\\nt;a\\x03\\nt)(st\\x00s\\x03\\nt) +raF(s\\x03\\nt;a\\x03\\nt)(at\\x00a\\x03\\nt)\\nwherest;atwould be our current state and action. Now that we have\\na linear approximation around each of these points, we can use the\\nprevious section and rewrite\\nst+1=At\\x01st+Bt\\x01at\\n(notice that in that case, we use the non-stationary dynamics setting\\nthat we mentioned at the beginning of these lecture notes)\\nNote We can apply a similar derivation for the reward R(t), with a\\nsecond-order Taylor expansion.\\nR(st;at)\\x19R(s\\x03\\nt;a\\x03\\nt) +rsR(s\\x03\\nt;a\\x03\\nt)(st\\x00s\\x03\\nt) +raR(s\\x03\\nt;a\\x03\\nt)(at\\x00a\\x03\\nt)\\n+1\\n2(st\\x00s\\x03\\nt)>Hss(st\\x00s\\x03\\nt) + (st\\x00s\\x03\\nt)>Hsa(at\\x00a\\x03\\nt)\\n+1\\n2(at\\x00a\\x03\\nt)>Haa(at\\x00a\\x03\\nt)\\nwhereHxyrefers to the entry of the Hessian of Rwith respect to xand\\nyevaluated in ( s\\x03\\nt;a\\x03\\nt) (omitted for readability). This expression can be\\nre-written as\\nRt(st;at) =\\x00s>\\ntUtst\\x00a>\\ntWtat\\nfor some matrices Ut;Wt, with the same trick of adding an extra dimen-\\nsion of ones. To convince yourself, notice that\\n\\x00\\n1x\\x01\\n\\x01\\x12a b\\nb c\\x13\\n\\x01\\x121\\nx\\x13\\n=a+ 2bx+cx2',\n",
       " \"213\\nstep 3 Now, you can convince yourself that our problem is strictly re-written\\nin the LQR framework. Let's just use LQR to \\x0cnd the optimal policy\\n\\x19t. As a result, our new controller will (hopefully) be better!\\nNote: Some problems might arise if the LQR trajectory deviates too\\nmuch from the linearized approximation of the trajectory, but that can\\nbe \\x0cxed with reward-shaping...\\nstep 4 Now that we get a new controller (our new policy \\x19t), we use it to\\nproduce a new trajectory\\ns\\x03\\n0;\\x190(s\\x03\\n0)!s\\x03\\n1;\\x191(s\\x03\\n1)!:::!s\\x03\\nT\\nnote that when we generate this new trajectory, we use the real Fand\\nnot its linear approximation to compute transitions, meaning that\\ns\\x03\\nt+1=F(s\\x03\\nt;a\\x03\\nt)\\nthen, go back to step 2 and repeat until some stopping criterion.\\n16.4 Linear Quadratic Gaussian (LQG)\\nOften, in the real word, we don't get to observe the full state st. For example,\\nan autonomous car could receive an image from a camera, which is merely\\nanobservation , and not the full state of the world. So far, we assumed\\nthat the state was available. As this might not hold true for most of the\\nreal-world problems, we need a new tool to model this situation: Partially\\nObservable MDPs .\\nA POMDP is an MDP with an extra observation layer. In other words,\\nwe introduce a new variable ot, that follows some conditional distribution\\ngiven the current state st\\notjst\\x18O(ojs)\\nFormally, a \\x0cnite-horizon POMDP is given by a tuple\\n(S;O;A;Psa;T;R )\\nWithin this framework, the general strategy is to maintain a belief state\\n(distribution over states) based on the observation o1;:::;ot. Then, a policy\\nin a POMDP maps this belief states to actions.\",\n",
       " \"214\\nIn this section, we'll present a extension of LQR to this new setting.\\nAssume that we observe yt2Rnwithm<n such that\\n(\\nyt =C\\x01st+vt\\nst+1=A\\x01st+B\\x01at+wt\\nwhereC2Rn\\x02dis a compression matrix and vtis the sensor noise (also\\ngaussian, like wt). Note that the reward function R(t)is left unchanged, as a\\nfunction of the state (not the observation) and action. Also, as distributions\\nare gaussian, the belief state is also going to be gaussian. In this new frame-\\nwork, let's give an overview of the strategy we are going to adopt to \\x0cnd the\\noptimal policy:\\nstep 1 \\x0crst, compute the distribution on the possible states (the belief state),\\nbased on the observations we have. In other words, we want to compute\\nthe meanstjtand the covariance \\x06 tjtof\\nstjy1;:::;yt\\x18N\\x00\\nstjt;\\x06tjt\\x01\\nto perform the computation e\\x0eciently over time, we'll use the Kalman\\nFilter algorithm (used on-board Apollo Lunar Module!).\\nstep 2 now that we have the distribution, we'll use the mean stjtas the best\\napproximation for st\\nstep 3 then set the action at:=LtstjtwhereLtcomes from the regular LQR\\nalgorithm.\\nIntuitively, to understand why this works, notice that stjtis a noisy ap-\\nproximation of st(equivalent to adding more noise to LQR) but we proved\\nthat LQR is independent of the noise!\\nStep 1 needs to be explicated. We'll cover a simple case where there is\\nno action dependence in our dynamics (but the general case follows the same\\nidea). Suppose that\\n(\\nst+1=A\\x01st+wt; wt\\x18N(0;\\x06s)\\nyt =C\\x01st+vt; vt\\x18N(0;\\x06y)\\nAs noises are Gaussians, we can easily prove that the joint distribution is\\nalso Gaussian\",\n",
       " '215\\n0\\nBBBBBBB@s1\\n...\\nst\\ny1\\n...\\nyt1\\nCCCCCCCA\\x18N (\\x16;\\x06) for some \\x16;\\x06\\nthen, using the marginal formulas of gaussians (see Factor Analysis notes),\\nwe would get\\nstjy1;:::;yt\\x18N\\x00\\nstjt;\\x06tjt\\x01\\nHowever, computing the marginal distribution parameters using these\\nformulas would be computationally expensive! It would require manipulating\\nmatrices of shape t\\x02t. Recall that inverting a matrix can be done in O(t3),\\nand it would then have to be repeated over the time steps, yielding a cost in\\nO(t4)!\\nTheKalman \\x0clter algorithm provides a much better way of computing\\nthe mean and variance, by updating them over time in constant time in\\nt! The kalman \\x0clter is based on two basics steps. Assume that we know the\\ndistribution of stjy1;:::;yt:\\npredict step computest+1jy1;:::;yt\\nupdate step computest+1jy1;:::;yt+1\\nand iterate over time steps! The combination of the predict and update\\nsteps updates our belief states. In other words, the process looks like\\n(stjy1;:::;yt)predict\\x00\\x00\\x00\\x00! (st+1jy1;:::;yt)update\\x00\\x00\\x00\\x00! (st+1jy1;:::;yt+1)predict\\x00\\x00\\x00\\x00!:::\\npredict step Suppose that we know the distribution of\\nstjy1;:::;yt\\x18N\\x00\\nstjt;\\x06tjt\\x01\\nthen, the distribution over the next state is also a gaussian distribution\\nst+1jy1;:::;yt\\x18N\\x00\\nst+1jt;\\x06t+1jt\\x01\\nwhere',\n",
       " \"216\\n(\\nst+1jt=A\\x01stjt\\n\\x06t+1jt=A\\x01\\x06tjt\\x01A>+ \\x06s\\nupdate step givenst+1jtand \\x06t+1jtsuch that\\nst+1jy1;:::;yt\\x18N\\x00\\nst+1jt;\\x06t+1jt\\x01\\nwe can prove that\\nst+1jy1;:::;yt+1\\x18N\\x00\\nst+1jt+1;\\x06t+1jt+1\\x01\\nwhere\\n(\\nst+1jt+1 =st+1jt+Kt(yt+1\\x00Cst+1jt)\\n\\x06t+1jt+1= \\x06t+1jt\\x00Kt\\x01C\\x01\\x06t+1jt\\nwith\\nKt:= \\x06t+1jtC>(C\\x06t+1jtC>+ \\x06y)\\x001\\nThe matrix Ktis called the Kalman gain .\\nNow, if we have a closer look at the formulas, we notice that we don't\\nneed the observations prior to time step t! The update steps only depends\\non the previous distribution. Putting it all together, the algorithm \\x0crst runs\\na forward pass to compute the Kt, \\x06tjtandstjt(sometimes referred to as\\n^sin the literature). Then, it runs a backward pass (the LQR updates) to\\ncompute the quantities \\t t;\\ttandLt. Finally, we recover the optimal policy\\nwitha\\x03\\nt=Ltstjt.\",\n",
       " 'Chapter 17\\nPolicy Gradient\\n(REINFORCE)\\nWe will present a model-free algorithm called REINFORCE that does not\\nrequire the notion of value functions and Qfunctions. It turns out to be more\\nconvenient to introduce REINFORCE in the \\x0cnite horizon case, which will\\nbe assumed throughout this note: we use \\x1c= (s0;a0;:::;sT\\x001;aT\\x001;sT) to\\ndenote a trajectory, where T <1is the length of the trajectory. Moreover,\\nREINFORCE only applies to learning a randomized policy . We use\\x19\\x12(ajs)\\nto denote the probability of the policy \\x19\\x12outputting the action aat states.\\nThe other notations will be the same as in previous lecture notes.\\nThe advantage of applying REINFORCE is that we only need to assume\\nthat we can sample from the transition probabilities fPsagand can query the\\nreward function R(s;a) at statesand actiona,1but we do not need to know\\nthe analytical form of the transition probabilities or the reward function.\\nWe do not explicitly learn the transition probabilities or the reward function\\neither.\\nLets0be sampled from some distribution \\x16. We consider optimizing the\\nexpected total payo\\x0b of the policy \\x19\\x12over the parameter \\x12de\\x0cned as.\\n\\x11(\\x12),E\"T\\x001X\\nt=0\\rtR(st;at)#\\n(17.1)\\nRecall that st\\x18Pst\\x001at\\x001andat\\x18\\x19\\x12(\\x01jst). Also note that \\x11(\\x12) =\\nEs0\\x18P[V\\x19\\x12(s0)] if we ignore the di\\x0berence between \\x0cnite and in\\x0cnite hori-\\nzon.\\n1In this notes we will work with the general setting where the reward depends on both\\nthe state and the action.\\n217',\n",
       " '218\\nWe aim to use gradient ascent to maximize \\x11(\\x12). The main challenge\\nwe face here is to compute (or estimate) the gradient of \\x11(\\x12) without the\\nknowledge of the form of the reward function and the transition probabilities.\\nLetP\\x12(\\x1c) denote the distribution of \\x1c(generated by the policy \\x19\\x12), and\\nletf(\\x1c) =PT\\x001\\nt=0\\rtR(st;at). We can rewrite \\x11(\\x12) as\\n\\x11(\\x12) = E\\x1c\\x18P\\x12[f(\\x1c)] (17.2)\\nWe face a similar situations in the variational auto-encoder (VAE) setting\\ncovered in the previous lectures, where the we need to take the gradient w.r.t\\nto a variable that shows up under the expectation | the distribution P\\x12\\ndepends on \\x12. Recall that in VAE, we used the re-parametrization techniques\\nto address this problem. However it does not apply here because we do\\nknow not how to compute the gradient of the function f. (We only have\\nan e\\x0ecient way to evaluate the function fby taking a weighted sum of the\\nobserved rewards, but we do not necessarily know the reward function itself\\nto compute the gradient.)\\nThe REINFORCE algorithm uses an another approach to estimate the\\ngradient of \\x11(\\x12). We start with the following derivation:\\nr\\x12E\\x1c\\x18P\\x12[f(\\x1c)] =r\\x12Z\\nP\\x12(\\x1c)f(\\x1c)d\\x1c\\n=Z\\nr\\x12(P\\x12(\\x1c)f(\\x1c))d\\x1c (swap integration with gradient)\\n=Z\\n(r\\x12P\\x12(\\x1c))f(\\x1c)d\\x1c (becauefdoes not depend on \\x12)\\n=Z\\nP\\x12(\\x1c)(r\\x12logP\\x12(\\x1c))f(\\x1c)d\\x1c\\n(becauserlogP\\x12(\\x1c) =rP\\x12(\\x1c)\\nP\\x12(\\x1c))\\n= E\\x1c\\x18P\\x12[(r\\x12logP\\x12(\\x1c))f(\\x1c)] (17.3)\\nNow we have a sample-based estimator for r\\x12E\\x1c\\x18P\\x12[f(\\x1c)]. Let\\x1c(1);:::;\\x1c(n)\\nbenempirical samples from P\\x12(which are obtained by running the policy\\n\\x19\\x12forntimes, with Tsteps for each run). We can estimate the gradient of\\n\\x11(\\x12) by\\nr\\x12E\\x1c\\x18P\\x12[f(\\x1c)] = E\\x1c\\x18P\\x12[(r\\x12logP\\x12(\\x1c))f(\\x1c)] (17.4)\\n\\x191\\nnnX\\ni=1(r\\x12logP\\x12(\\x1c(i)))f(\\x1c(i)) (17.5)',\n",
       " '219\\nThe next question is how to compute log P\\x12(\\x1c). We derive an analyt-\\nical formula for log P\\x12(\\x1c) and compute its gradient w.r.t \\x12(using auto-\\ndi\\x0berentiation). Using the de\\x0cnition of \\x1c, we have\\nP\\x12(\\x1c) =\\x16(s0)\\x19\\x12(a0js0)Ps0a0(s1)\\x19\\x12(a1js1)Ps1a1(s2)\\x01\\x01\\x01PsT\\x001aT\\x001(sT) (17.6)\\nHere recall that \\x16to used to denote the density of the distribution of s0. It\\nfollows that\\nlogP\\x12(\\x1c) = log\\x16(s0) + log\\x19\\x12(a0js0) + logPs0a0(s1) + log\\x19\\x12(a1js1)\\n+ logPs1a1(s2) +\\x01\\x01\\x01+ logPsT\\x001aT\\x001(sT) (17.7)\\nTaking gradient w.r.t to \\x12, we obtain\\nr\\x12logP\\x12(\\x1c) =r\\x12log\\x19\\x12(a0js0) +r\\x12log\\x19\\x12(a1js1) +\\x01\\x01\\x01+r\\x12log\\x19\\x12(aT\\x001jsT\\x001)\\nNote that many of the terms disappear because they don\\'t depend on \\x12and\\nthus have zero gradients. (This is somewhat important | we don\\'t know how\\nto evaluate those terms such as log Ps0a0(s1) because we don\\'t have access to\\nthe transition probabilities, but luckily those terms have zero gradients!)\\nPlugging the equation above into equation (17.4), we conclude that\\nr\\x12\\x11(\\x12) =r\\x12E\\x1c\\x18P\\x12[f(\\x1c)] = E\\x1c\\x18P\\x12\" T\\x001X\\nt=0r\\x12log\\x19\\x12(atjst)!\\n\\x01f(\\x1c)#\\n= E\\x1c\\x18P\\x12\" T\\x001X\\nt=0r\\x12log\\x19\\x12(atjst)!\\n\\x01 T\\x001X\\nt=0\\rtR(st;at)!#\\n(17.8)\\nWe estimate the RHS of the equation above by empirical sample trajectories,\\nand the estimate is unbiased. The vanilla REINFORCE algorithm iteratively\\nupdates the parameter by gradient ascent using the estimated gradients.\\nInterpretation of the policy gradient formula (17.8) .The quantity\\nr\\x12P\\x12(\\x1c) =PT\\x001\\nt=0r\\x12log\\x19\\x12(atjst) is intuitively the direction of the change\\nof\\x12that will make the trajectory \\x1cmore likely to occur (or increase the\\nprobability of choosing action a0;:::;at\\x001), andf(\\x1c) is the total payo\\x0b of\\nthis trajectory. Thus, by taking a gradient step, intuitively we are trying to\\nimprove the likelihood of all the trajectories, but with a di\\x0berent emphasis\\nor weight for each \\x1c(or for each set of actions a0;a1;:::;at\\x001). If\\x1cis very\\nrewarding (that is, f(\\x1c) is large), we try very hard to move in the direction',\n",
       " '220\\nthat can increase the probability of the trajectory \\x1c(or the direction that\\nincreases the probability of choosing a0;:::;at\\x001), and if\\x1chas low payo\\x0b,\\nwe try less hard with a smaller weight.\\nAn interesting fact that follows from formula (17.3) is that\\nE\\x1c\\x18P\\x12\"T\\x001X\\nt=0r\\x12log\\x19\\x12(atjst)#\\n= 0 (17.9)\\nTo see this, we take f(\\x1c) = 1 (that is, the reward is always a constant),\\nthen the LHS of (17.8) is zero because the payo\\x0b is always a \\x0cxed constantPT\\nt=0\\rt. Thus the RHS of (17.8) is also zero, which implies (17.9).\\nIn fact, one can verify that E at\\x18\\x19\\x12(\\x01jst)r\\x12log\\x19\\x12(atjst) = 0 for any \\x0cxed t\\nandst.2This fact has two consequences. First, we can simplify formula (17.8)\\nto\\nr\\x12\\x11(\\x12) =T\\x001X\\nt=0E\\x1c\\x18P\\x12\"\\nr\\x12log\\x19\\x12(atjst)\\x01 T\\x001X\\nj=0\\rjR(sj;aj)!#\\n=T\\x001X\\nt=0E\\x1c\\x18P\\x12\"\\nr\\x12log\\x19\\x12(atjst)\\x01 T\\x001X\\nj\\x15t\\rjR(sj;aj)!#\\n(17.10)\\nwhere the second equality follows from\\nE\\x1c\\x18P\\x12\"\\nr\\x12log\\x19\\x12(atjst)\\x01 X\\n0\\x14j<t\\rjR(sj;aj)!#\\n= E\"\\nE [r\\x12log\\x19\\x12(atjst)js0;a0;:::;st\\x001;at\\x001;st]\\x01 X\\n0\\x14j<t\\rjR(sj;aj)!#\\n= 0 (because E [ r\\x12log\\x19\\x12(atjst)js0;a0;:::;st\\x001;at\\x001;st] = 0)\\nNote that here we used the law of total expectation. The outer expecta-\\ntion in the second line above is over the randomness of s0;a0;:::;at\\x001;st,\\nwhereas the inner expectation is over the randomness of at(conditioned on\\ns0;a0;:::;at\\x001;st.) We see that we\\'ve made the estimator slightly simpler.\\nThe second consequence of E at\\x18\\x19\\x12(\\x01jst)r\\x12log\\x19\\x12(atjst) = 0 is the following: for\\nany valueB(st) that only depends on st, it holds that\\nE\\x1c\\x18P\\x12[r\\x12log\\x19\\x12(atjst)\\x01B(st)]\\n= E [E [r\\x12log\\x19\\x12(atjst)js0;a0;:::;st\\x001;at\\x001;st]B(st)]\\n= 0 (because E [ r\\x12log\\x19\\x12(atjst)js0;a0;:::;st\\x001;at\\x001;st] = 0)\\n2In general, it\\'s true that E x\\x18p\\x12[rlogp\\x12(x)] = 0.',\n",
       " '221\\nAgain here we used the law of total expectation. The outer expecta-\\ntion in the second line above is over the randomness of s0;a0;:::;at\\x001;st,\\nwhereas the inner expectation is over the randomness of at(conditioned on\\ns0;a0;:::;at\\x001;st.) It follows from equation (17.10) and the equation above\\nthat\\nr\\x12\\x11(\\x12) =T\\x001X\\nt=0E\\x1c\\x18P\\x12\"\\nr\\x12log\\x19\\x12(atjst)\\x01 T\\x001X\\nj\\x15t\\rjR(sj;aj)\\x00\\rtB(st)!#\\n=T\\x001X\\nt=0E\\x1c\\x18P\\x12\"\\nr\\x12log\\x19\\x12(atjst)\\x01\\rt T\\x001X\\nj\\x15t\\rj\\x00tR(sj;aj)\\x00B(st)!#\\n(17.11)\\nTherefore, we will get a di\\x0berent estimator for estimating the r\\x11(\\x12) with a\\ndi\\x0berence choice of B(\\x01). The bene\\x0ct of introducing a proper B(\\x01) | which\\nis often referred to as a baseline | is that it helps reduce the variance of the\\nestimator.3It turns out that a near optimal estimator would be the expected\\nfuture payo\\x0b EhPT\\x001\\nj\\x15t\\rj\\x00tR(sj;aj)jsti\\n, which is pretty much the same as the\\nvalue function V\\x19\\x12(st) (if we ignore the di\\x0berence between \\x0cnite and in\\x0cnite\\nhorizon.) Here one could estimate the value function V\\x19\\x12(\\x01) in a crude way,\\nbecause its precise value doesn\\'t in\\ruence the mean of the estimator but only\\nthe variance. This leads to a policy gradient algorithm with baselines stated\\nin Algorithm 7.4\\n3As a heuristic but illustrating example, suppose for a \\x0cxed t, the future rewardPT\\x001\\nj\\x15t\\rj\\x00tR(sj;aj) randomly takes two values 1000 + 1 and 1000 \\x002 with equal proba-\\nbility, and the corresponding values for r\\x12log\\x19\\x12(atjst) are vector zand\\x00z. (Note that\\nbecause E [r\\x12log\\x19\\x12(atjst)] = 0, ifr\\x12log\\x19\\x12(atjst) can only take two values uniformly,\\nthen the two values have to two vectors in an opposite direction.) In this case, without\\nsubtracting the baseline, the estimators take two values (1000 + 1) zand\\x00(1000\\x002)z,\\nwhereas after subtracting a baseline of 1000, the estimator has two values zand 2z. The\\nlatter estimator has much lower variance compared to the original estimator.\\n4We note that the estimator of the gradient in the algorithm does not exactly match\\nthe equation 17.11. If we multiply \\rtin the summand of equation (17.13), then they will\\nexactly match. Removing such discount factors empirically works well because it gives a\\nlarge update.',\n",
       " '222\\nAlgorithm 7 Vanilla policy gradient with baseline\\nfori= 1;\\x01\\x01\\x01do\\nCollect a set of trajectories by executing the current policy. Use R\\x15t\\nas a shorthand forPT\\x001\\nj\\x15t\\rj\\x00tR(sj;aj)\\nFit the baseline by \\x0cnding a function Bthat minimizes\\nX\\n\\x1cX\\nt(R\\x15t\\x00B(st))2(17.12)\\nUpdate the policy parameter \\x12with the gradient estimator\\nX\\n\\x1cX\\ntr\\x12log\\x19\\x12(atjst)\\x01(R\\x15t\\x00B(st)) (17.13)',\n",
       " 'Bibliography\\nMikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling\\nmodern machine-learning practice and the classical bias{variance trade-\\no\\x0b.Proceedings of the National Academy of Sciences , 116(32):15849{15854,\\n2019.\\nMikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for\\nweak features. SIAM Journal on Mathematics of Data Science , 2(4):1167{\\n1180, 2020.\\nDavid M Blei, Alp Kucukelbir, and Jon D McAuli\\x0be. Variational inference:\\nA review for statisticians. Journal of the American Statistical Association ,\\n112(518):859{877, 2017.\\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran\\nArora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\\nBosselut, Emma Brunskill, et al. On the opportunities and risks of foun-\\ndation models. arXiv preprint arXiv:2108.07258 , 2021.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas-\\ntry, Amanda Askell, et al. Language models are few-shot learners. Advances\\nin neural information processing systems , 33:1877{1901, 2020.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geo\\x0brey Hinton.\\nA simple framework for contrastive learning of visual representations. In\\nInternational Conference on Machine Learning , pages 1597{1607. PMLR,\\n2020.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\\nPre-training of deep bidirectional transformers for language understand-\\ning. In Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers) , pages 4171{4186, 2019.\\n223',\n",
       " '224\\nJe\\x0b Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters:\\nUnderstanding the implicit bias of the noise covariance. arXiv preprint\\narXiv:2006.08680 , 2020.\\nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.\\nSurprises in high-dimensional ridgeless least squares interpolation. 2019.\\nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.\\nSurprises in high-dimensional ridgeless least squares interpolation. The\\nAnnals of Statistics , 50(2):949{986, 2022.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition , pages 770{778, 2016.\\nSergey Io\\x0be and Christian Szegedy. Batch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift. In Proceedings of the\\n32nd International Conference on Machine Learning, ICML 2015, Lille,\\nFrance, 6-11 July 2015 , pages 448{456, 2015. URL http://jmlr.org/\\nproceedings/papers/v37/ioffe15.html .\\nGareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An\\nintroduction to statistical learning, second edition , volume 112. Springer,\\n2021.\\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic opti-\\nmization. arXiv preprint arXiv:1412.6980 , 2014.\\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv\\npreprint arXiv:1312.6114 , 2013.\\nYuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and\\nTengyu Ma. Algorithmic framework for model-based deep reinforcement\\nlearning with theoretical guarantees. In International Conference on Learn-\\ning Representations , 2018.\\nSong Mei and Andrea Montanari. The generalization error of random features\\nregression: Precise asymptotics and the double descent curve. Communi-\\ncations on Pure and Applied Mathematics , 75(4):667{766, 2022.\\nPreetum Nakkiran. More data can hurt for linear regression: Sample-wise\\ndouble descent. 2019.\\nPreetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal\\nregularization can mitigate double descent. 2020.',\n",
       " '225\\nManfred Opper. Statistical mechanics of learning: Generalization. The hand-\\nbook of brain theory and neural networks , pages 922{925, 1995.\\nManfred Opper. Learning to generalize. Frontiers of Life , 3(part 2):763{775,\\n2001.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\\nneed. arXiv preprint arXiv:1706.03762 , 2017.\\nBlake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-\\ndro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and\\nrich regimes in overparametrized models. arXiv preprint arXiv:2002.09277 ,\\n2020.\\nYuxin Wu and Kaiming He. Group normalization. In Proceedings of the\\nEuropean conference on computer vision (ECCV) , pages 3{19, 2018.']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "49868199-5503-429b-af5d-dab06906c966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CS229 Lecture Notes\n",
      "Andrew Ng and Tengyu Ma\n",
      "April 30, 2023\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "I Supervised learning 5\n",
      "1 Linear regression 8\n",
      "1.1 LMS algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "1.2 The normal equations . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "1.2.1 Matrix derivatives . . . . . . . . . . . . . . . . . . . . . 13\n",
      "1.2.2 Least squares revisited . . . . . . . . . . . . . . . . . . 14\n",
      "1.3 Probabilistic interpretation . . . . . . . . . . . . . . . . . . . . 15\n",
      "1.4 Locally weighted linear regression (optional reading) . . . . . . 17\n",
      "2 Classi\f",
      "cation and logistic regression 20\n",
      "2.1 Logistic regression . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "2.2 Digression: the perceptron learning algorithm . . . . . . . . . 23\n",
      "2.3 Multi-class classi\f",
      "cation . . . . . . . . . . . . . . . . . . . . . 24\n",
      "2.4 Another algorithm for maximizing `(\u0012) . . . . . . . . . . . . . 27\n",
      "3 Generalized linear models 29\n",
      "3.1 The exponential family . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "3.2 Constructing GLMs . . . . . . . . . . . . . . . . . . . . . . . . 31\n",
      "3.2.1 Ordinary least squares . . . . . . . . . . . . . . . . . . 32\n",
      "3.2.2 Logistic regression . . . . . . . . . . . . . . . . . . . . 33\n",
      "4 Generative learning algorithms 34\n",
      "4.1 Gaussian discriminant analysis . . . . . . . . . . . . . . . . . . 35\n",
      "4.1.1 The multivariate normal distribution . . . . . . . . . . 35\n",
      "4.1.2 The Gaussian discriminant analysis model . . . . . . . 38\n",
      "4.1.3 Discussion: GDA and logistic regression . . . . . . . . 40\n",
      "4.2 Naive bayes (Option Reading) . . . . . . . . . . . . . . . . . . 41\n",
      "4.2.1 Laplace smoothing . . . . . . . . . . . . . . . . . . . . 44\n",
      "4.2.2 Event models for text classi\f",
      "cation . . . . . . . . . . . 46\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CS229 Spring 20223 2\n",
      "5 Kernel methods 48\n",
      "5.1 Feature maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n",
      "5.2 LMS (least mean squares) with features . . . . . . . . . . . . . 49\n",
      "5.3 LMS with the kernel trick . . . . . . . . . . . . . . . . . . . . 49\n",
      "5.4 Properties of kernels . . . . . . . . . . . . . . . . . . . . . . . 53\n",
      "6 Support vector machines 59\n",
      "6.1 Margins: intuition . . . . . . . . . . . . . . . . . . . . . . . . . 59\n",
      "6.2 Notation (option reading) . . . . . . . . . . . . . . . . . . . . 61\n",
      "6.3 Functional and geometric margins (option reading) . . . . . . 61\n",
      "6.4 The optimal margin classi\f",
      "er (option reading) . . . . . . . . . 63\n",
      "6.5 Lagrange duality (optional reading) . . . . . . . . . . . . . . . 65\n",
      "6.6 Optimal margin classi\f",
      "ers: the dual form (option reading) . . 68\n",
      "6.7 Regularization and the non-separable case (optional reading) . 72\n",
      "6.8 The SMO algorithm (optional reading) . . . . . . . . . . . . . 73\n",
      "6.8.1 Coordinate ascent . . . . . . . . . . . . . . . . . . . . . 74\n",
      "6.8.2 SMO . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n",
      "II Deep learning 79\n",
      "7 Deep learning 80\n",
      "7.1 Supervised learning with non-linear models . . . . . . . . . . . 80\n",
      "7.2 Neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n",
      "7.3 Modules in Modern Neural Networks . . . . . . . . . . . . . . 92\n",
      "7.4 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . 98\n",
      "7.4.1 Preliminaries on partial derivatives . . . . . . . . . . . 99\n",
      "7.4.2 General strategy of backpropagation . . . . . . . . . . 102\n",
      "7.4.3 Backward functions for basic modules . . . . . . . . . . 105\n",
      "7.4.4 Back-propagation for MLPs . . . . . . . . . . . . . . . 107\n",
      "7.5 Vectorization over training examples . . . . . . . . . . . . . . 109\n",
      "III Generalization and regularization 112\n",
      "8 Generalization 113\n",
      "8.1 Bias-variance tradeo\u000b",
      " . . . . . . . . . . . . . . . . . . . . . . . 115\n",
      "8.1.1 A mathematical decomposition (for regression) . . . . . 120\n",
      "8.2 The double descent phenomenon . . . . . . . . . . . . . . . . . 121\n",
      "8.3 Sample complexity bounds (optional readings) . . . . . . . . . 126\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CS229 Spring 20223 3\n",
      "8.3.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . 126\n",
      "8.3.2 The case of \f",
      "nite H. . . . . . . . . . . . . . . . . . . . 128\n",
      "8.3.3 The case of in\f",
      "nite H. . . . . . . . . . . . . . . . . . 131\n",
      "9 Regularization and model selection 135\n",
      "9.1 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "9.2 Implicit regularization e\u000b",
      "ect . . . . . . . . . . . . . . . . . . . 137\n",
      "9.3 Model selection via cross validation . . . . . . . . . . . . . . . 139\n",
      "9.4 Bayesian statistics and regularization . . . . . . . . . . . . . . 142\n",
      "IV Unsupervised learning 144\n",
      "10 Clustering and the k-means algorithm 145\n",
      "11 EM algorithms 148\n",
      "11.1 EM for mixture of Gaussians . . . . . . . . . . . . . . . . . . . 148\n",
      "11.2 Jensen's inequality . . . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "11.3 General EM algorithms . . . . . . . . . . . . . . . . . . . . . . 152\n",
      "11.3.1 Other interpretation of ELBO . . . . . . . . . . . . . . 158\n",
      "11.4 Mixture of Gaussians revisited . . . . . . . . . . . . . . . . . . 158\n",
      "11.5 Variational inference and variational auto-encoder (optional\n",
      "reading) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n",
      "12 Principal components analysis 165\n",
      "13 Independent components analysis 171\n",
      "13.1 ICA ambiguities . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n",
      "13.2 Densities and linear transformations . . . . . . . . . . . . . . . 173\n",
      "13.3 ICA algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\n",
      "14 Self-supervised learning and foundation models 177\n",
      "14.1 Pretraining and adaptation . . . . . . . . . . . . . . . . . . . . 177\n",
      "14.2 Pretraining methods in computer vision . . . . . . . . . . . . . 179\n",
      "14.3 Pretrained large language models . . . . . . . . . . . . . . . . 181\n",
      "14.3.1 Zero-shot learning and in-context learning . . . . . . . 183\n",
      "V Reinforcement Learning and Control 185\n",
      "15 Reinforcement learning 186\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CS229 Spring 20223 4\n",
      "15.1 Markov decision processes . . . . . . . . . . . . . . . . . . . . 187\n",
      "15.2 Value iteration and policy iteration . . . . . . . . . . . . . . . 189\n",
      "15.3 Learning a model for an MDP . . . . . . . . . . . . . . . . . . 191\n",
      "15.4 Continuous state MDPs . . . . . . . . . . . . . . . . . . . . . 193\n",
      "15.4.1 Discretization . . . . . . . . . . . . . . . . . . . . . . . 193\n",
      "15.4.2 Value function approximation . . . . . . . . . . . . . . 196\n",
      "15.5 Connections between Policy and Value Iteration (Optional) . . 200\n",
      "16 LQR, DDP and LQG 203\n",
      "16.1 Finite-horizon MDPs . . . . . . . . . . . . . . . . . . . . . . . 203\n",
      "16.2 Linear Quadratic Regulation (LQR) . . . . . . . . . . . . . . . 207\n",
      "16.3 From non-linear dynamics to LQR . . . . . . . . . . . . . . . 210\n",
      "16.3.1 Linearization of dynamics . . . . . . . . . . . . . . . . 211\n",
      "16.3.2 Di\u000b",
      "erential Dynamic Programming (DDP) . . . . . . . 211\n",
      "16.4 Linear Quadratic Gaussian (LQG) . . . . . . . . . . . . . . . . 213\n",
      "17 Policy Gradient (REINFORCE) 217\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Part I\n",
      "Supervised learning\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "Let's start by talking about a few examples of supervised learning prob-\n",
      "lems. Suppose we have a dataset giving the living areas and prices of 47\n",
      "houses from Portland, Oregon:\n",
      "Living area (feet2)Price (1000 $s)\n",
      "2104 400\n",
      "1600 330\n",
      "2400 369\n",
      "1416 232\n",
      "3000 540\n",
      "......\n",
      "We can plot this data:\n",
      "500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices\n",
      "square feetprice (in $1000)\n",
      "Given data like this, how can we learn to predict the prices of other houses\n",
      "in Portland, as a function of the size of their living areas?\n",
      "To establish notation for future use, we'll use x(i)to denote the \\input\"\n",
      "variables (living area in this example), also called input features , andy(i)\n",
      "to denote the \\output\" or target variable that we are trying to predict\n",
      "(price). A pair ( x(i);y(i)) is called a training example , and the dataset\n",
      "that we'll be using to learn|a list of ntraining examples f(x(i);y(i));i=\n",
      "1;:::;ng|is called a training set . Note that the superscript \\( i)\" in the\n",
      "notation is simply an index into the training set, and has nothing to do with\n",
      "exponentiation. We will also use Xdenote the space of input values, and Y\n",
      "the space of output values. In this example, X=Y=R.\n",
      "To describe the supervised learning problem slightly more formally, our\n",
      "goal is, given a training set, to learn a function h:X7!Y so thath(x) is a\n",
      "\\good\" predictor for the corresponding value of y. For historical reasons, this\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "functionhis called a hypothesis . Seen pictorially, the process is therefore\n",
      "like this:\n",
      "Training \n",
      "    set\n",
      " house.)(living area ofLearning \n",
      "algorithm\n",
      "h predicted y x\n",
      "(predicted price)\n",
      "of house)\n",
      "When the target variable that we're trying to predict is continuous, such\n",
      "as in our housing example, we call the learning problem a regression prob-\n",
      "lem. When ycan take on only a small number of discrete values (such as\n",
      "if, given the living area, we wanted to predict if a dwelling is a house or an\n",
      "apartment, say), we call it a classi\f",
      "cation problem.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 1\n",
      "Linear regression\n",
      "To make our housing example more interesting, let's consider a slightly richer\n",
      "dataset in which we also know the number of bedrooms in each house:\n",
      "Living area (feet2)#bedrooms Price (1000 $s)\n",
      "2104 3 400\n",
      "1600 3 330\n",
      "2400 3 369\n",
      "1416 2 232\n",
      "3000 4 540\n",
      ".........\n",
      "Here, thex's are two-dimensional vectors in R2. For instance, x(i)\n",
      "1is the\n",
      "living area of the i-th house in the training set, and x(i)\n",
      "2is its number of\n",
      "bedrooms. (In general, when designing a learning problem, it will be up to\n",
      "you to decide what features to choose, so if you are out in Portland gathering\n",
      "housing data, you might also decide to include other features such as whether\n",
      "each house has a \f",
      "replace, the number of bathrooms, and so on. We'll say\n",
      "more about feature selection later, but for now let's take the features as\n",
      "given.)\n",
      "To perform supervised learning, we must decide how we're going to rep-\n",
      "resent functions/hypotheses hin a computer. As an initial choice, let's say\n",
      "we decide to approximate yas a linear function of x:\n",
      "h\u0012(x) =\u00120+\u00121x1+\u00122x2\n",
      "Here, the\u0012i's are the parameters (also called weights ) parameterizing the\n",
      "space of linear functions mapping from XtoY. When there is no risk of\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9\n",
      "confusion, we will drop the \u0012subscript in h\u0012(x), and write it more simply as\n",
      "h(x). To simplify our notation, we also introduce the convention of letting\n",
      "x0= 1 (this is the intercept term ), so that\n",
      "h(x) =dX\n",
      "i=0\u0012ixi=\u0012Tx;\n",
      "where on the right-hand side above we are viewing \u0012andxboth as vectors,\n",
      "and heredis the number of input variables (not counting x0).\n",
      "Now, given a training set, how do we pick, or learn, the parameters \u0012?\n",
      "One reasonable method seems to be to make h(x) close toy, at least for\n",
      "the training examples we have. To formalize this, we will de\f",
      "ne a function\n",
      "that measures, for each value of the \u0012's, how close the h(x(i))'s are to the\n",
      "corresponding y(i)'s. We de\f",
      "ne the cost function :\n",
      "J(\u0012) =1\n",
      "2nX\n",
      "i=1(h\u0012(x(i))\u0000y(i))2:\n",
      "If you've seen linear regression before, you may recognize this as the familiar\n",
      "least-squares cost function that gives rise to the ordinary least squares\n",
      "regression model. Whether or not you have seen it previously, let's keep\n",
      "going, and we'll eventually show this to be a special case of a much broader\n",
      "family of algorithms.\n",
      "1.1 LMS algorithm\n",
      "We want to choose \u0012so as to minimize J(\u0012). To do so, let's use a search\n",
      "algorithm that starts with some \\initial guess\" for \u0012, and that repeatedly\n",
      "changes\u0012to makeJ(\u0012) smaller, until hopefully we converge to a value of\n",
      "\u0012that minimizes J(\u0012). Speci\f",
      "cally, let's consider the gradient descent\n",
      "algorithm, which starts with some initial \u0012, and repeatedly performs the\n",
      "update:\n",
      "\u0012j:=\u0012j\u0000\u000b",
      "@\n",
      "@\u0012jJ(\u0012):\n",
      "(This update is simultaneously performed for all values of j= 0;:::;d .)\n",
      "Here,\u000b",
      "is called the learning rate . This is a very natural algorithm that\n",
      "repeatedly takes a step in the direction of steepest decrease of J.\n",
      "In order to implement this algorithm, we have to work out what is the\n",
      "partial derivative term on the right hand side. Let's \f",
      "rst work it out for the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "case of if we have only one training example ( x;y), so that we can neglect\n",
      "the sum in the de\f",
      "nition of J. We have:\n",
      "@\n",
      "@\u0012jJ(\u0012) =@\n",
      "@\u0012j1\n",
      "2(h\u0012(x)\u0000y)2\n",
      "= 2\u00011\n",
      "2(h\u0012(x)\u0000y)\u0001@\n",
      "@\u0012j(h\u0012(x)\u0000y)\n",
      "= (h\u0012(x)\u0000y)\u0001@\n",
      "@\u0012j dX\n",
      "i=0\u0012ixi\u0000y!\n",
      "= (h\u0012(x)\u0000y)xj\n",
      "For a single training example, this gives the update rule:1\n",
      "\u0012j:=\u0012j+\u000b",
      "\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      "j:\n",
      "The rule is called the LMS update rule (LMS stands for \\least mean squares\"),\n",
      "and is also known as the Widrow-Ho\u000b",
      " learning rule. This rule has several\n",
      "properties that seem natural and intuitive. For instance, the magnitude of\n",
      "the update is proportional to the error term (y(i)\u0000h\u0012(x(i))); thus, for in-\n",
      "stance, if we are encountering a training example on which our prediction\n",
      "nearly matches the actual value of y(i), then we \f",
      "nd that there is little need\n",
      "to change the parameters; in contrast, a larger change to the parameters will\n",
      "be made if our prediction h\u0012(x(i)) has a large error (i.e., if it is very far from\n",
      "y(i)).\n",
      "We'd derived the LMS rule for when there was only a single training\n",
      "example. There are two ways to modify this method for a training set of\n",
      "more than one example. The \f",
      "rst is replace it with the following algorithm:\n",
      "Repeat until convergence f\n",
      "\u0012j:=\u0012j+\u000b",
      "nX\n",
      "i=1\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      "j;(for everyj) (1.1)\n",
      "g\n",
      "1We use the notation \\ a:=b\" to denote an operation (in a computer program) in\n",
      "which we setthe value of a variable ato be equal to the value of b. In other words, this\n",
      "operation overwrites awith the value of b. In contrast, we will write \\ a=b\" when we are\n",
      "asserting a statement of fact, that the value of ais equal to the value of b.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11\n",
      "By grouping the updates of the coordinates into an update of the vector\n",
      "\u0012, we can rewrite update (1.1) in a slightly more succinct way:\n",
      "\u0012:=\u0012+\u000b",
      "nX\n",
      "i=1\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      "The reader can easily verify that the quantity in the summation in the\n",
      "update rule above is just @J(\u0012)=@\u0012j(for the original de\f",
      "nition of J). So, this\n",
      "is simply gradient descent on the original cost function J. This method looks\n",
      "at every example in the entire training set on every step, and is called batch\n",
      "gradient descent . Note that, while gradient descent can be susceptible\n",
      "to local minima in general, the optimization problem we have posed here\n",
      "for linear regression has only one global, and no other local, optima; thus\n",
      "gradient descent always converges (assuming the learning rate \u000b",
      "is not too\n",
      "large) to the global minimum. Indeed, Jis a convex quadratic function.\n",
      "Here is an example of gradient descent as it is run to minimize a quadratic\n",
      "function.\n",
      "5 10 15 20 25 30 35 40 45 505101520253035404550\n",
      "The ellipses shown above are the contours of a quadratic function. Also\n",
      "shown is the trajectory taken by gradient descent, which was initialized at\n",
      "(48,30). The x's in the \f",
      "gure (joined by straight lines) mark the successive\n",
      "values of\u0012that gradient descent went through.\n",
      "When we run batch gradient descent to \f",
      "t \u0012on our previous dataset,\n",
      "to learn to predict housing price as a function of living area, we obtain\n",
      "\u00120= 71:27,\u00121= 0:1345. If we plot h\u0012(x) as a function of x(area), along\n",
      "with the training data, we obtain the following \f",
      "gure:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "12\n",
      "500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices\n",
      "square feetprice (in $1000)\n",
      "If the number of bedrooms were included as one of the input features as well,\n",
      "we get\u00120= 89:60;\u00121= 0:1392,\u00122=\u00008:738.\n",
      "The above results were obtained with batch gradient descent. There is\n",
      "an alternative to batch gradient descent that also works very well. Consider\n",
      "the following algorithm:\n",
      "Loopf\n",
      "fori= 1 ton,f\n",
      "\u0012j:=\u0012j+\u000b",
      "\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      "j;(for everyj) (1.2)\n",
      "g\n",
      "g\n",
      "By grouping the updates of the coordinates into an update of the vector\n",
      "\u0012, we can rewrite update (1.2) in a slightly more succinct way:\n",
      "\u0012:=\u0012+\u000b",
      "\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      "In this algorithm, we repeatedly run through the training set, and each\n",
      "time we encounter a training example, we update the parameters according\n",
      "to the gradient of the error with respect to that single training example only.\n",
      "This algorithm is called stochastic gradient descent (also incremental\n",
      "gradient descent ). Whereas batch gradient descent has to scan through\n",
      "the entire training set before taking a single step|a costly operation if nis\n",
      "large|stochastic gradient descent can start making progress right away, and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "13\n",
      "continues to make progress with each example it looks at. Often, stochastic\n",
      "gradient descent gets \u0012\\close\" to the minimum much faster than batch gra-\n",
      "dient descent. (Note however that it may never \\converge\" to the minimum,\n",
      "and the parameters \u0012will keep oscillating around the minimum of J(\u0012); but\n",
      "in practice most of the values near the minimum will be reasonably good\n",
      "approximations to the true minimum.2) For these reasons, particularly when\n",
      "the training set is large, stochastic gradient descent is often preferred over\n",
      "batch gradient descent.\n",
      "1.2 The normal equations\n",
      "Gradient descent gives one way of minimizing J. Let's discuss a second way\n",
      "of doing so, this time performing the minimization explicitly and without\n",
      "resorting to an iterative algorithm. In this method, we will minimize Jby\n",
      "explicitly taking its derivatives with respect to the \u0012j's, and setting them to\n",
      "zero. To enable us to do this without having to write reams of algebra and\n",
      "pages full of matrices of derivatives, let's introduce some notation for doing\n",
      "calculus with matrices.\n",
      "1.2.1 Matrix derivatives\n",
      "For a function f:Rn\u0002d7!Rmapping from n-by-dmatrices to the real\n",
      "numbers, we de\f",
      "ne the derivative of fwith respect to Ato be:\n",
      "rAf(A) =2\n",
      "64@f\n",
      "@A11\u0001\u0001\u0001@f\n",
      "@A1d.........\n",
      "@f\n",
      "@An1\u0001\u0001\u0001@f\n",
      "@And3\n",
      "75\n",
      "Thus, the gradient rAf(A) is itself an n-by-dmatrix, whose ( i;j)-element is\n",
      "@f=@Aij. For example, suppose A=\u0014\n",
      "A11A12\n",
      "A21A22\u0015\n",
      "is a 2-by-2 matrix, and\n",
      "the function f:R2\u000227!Ris given by\n",
      "f(A) =3\n",
      "2A11+ 5A2\n",
      "12+A21A22:\n",
      "2By slowly letting the learning rate \u000b",
      "decrease to zero as the algorithm runs, it is also\n",
      "possible to ensure that the parameters will converge to the global minimum rather than\n",
      "merely oscillate around the minimum.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "14\n",
      "Here,Aijdenotes the ( i;j) entry of the matrix A. We then have\n",
      "rAf(A) =\u00143\n",
      "210A12\n",
      "A22A21\u0015\n",
      ":\n",
      "1.2.2 Least squares revisited\n",
      "Armed with the tools of matrix derivatives, let us now proceed to \f",
      "nd in\n",
      "closed-form the value of \u0012that minimizes J(\u0012). We begin by re-writing Jin\n",
      "matrix-vectorial notation.\n",
      "Given a training set, de\f",
      "ne the design matrix Xto be then-by-dmatrix\n",
      "(actuallyn-by-d+ 1, if we include the intercept term) that contains the\n",
      "training examples' input values in its rows:\n",
      "X=2\n",
      "6664| (x(1))T|\n",
      "| (x(2))T|\n",
      "...\n",
      "| (x(n))T|3\n",
      "7775:\n",
      "Also, let~ ybe then-dimensional vector containing all the target values from\n",
      "the training set:\n",
      "~ y=2\n",
      "6664y(1)\n",
      "y(2)\n",
      "...\n",
      "y(n)3\n",
      "7775:\n",
      "Now, since h\u0012(x(i)) = (x(i))T\u0012, we can easily verify that\n",
      "X\u0012\u0000~ y=2\n",
      "64(x(1))T\u0012\n",
      "...\n",
      "(x(n))T\u00123\n",
      "75\u00002\n",
      "64y(1)\n",
      "...\n",
      "y(n)3\n",
      "75\n",
      "=2\n",
      "64h\u0012(x(1))\u0000y(1)\n",
      "...\n",
      "h\u0012(x(n))\u0000y(n)3\n",
      "75:\n",
      "Thus, using the fact that for a vector z, we have that zTz=P\n",
      "iz2\n",
      "i:\n",
      "1\n",
      "2(X\u0012\u0000~ y)T(X\u0012\u0000~ y) =1\n",
      "2nX\n",
      "i=1(h\u0012(x(i))\u0000y(i))2\n",
      "=J(\u0012)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15\n",
      "Finally, to minimize J, let's \f",
      "nd its derivatives with respect to \u0012. Hence,\n",
      "r\u0012J(\u0012) =r\u00121\n",
      "2(X\u0012\u0000~ y)T(X\u0012\u0000~ y)\n",
      "=1\n",
      "2r\u0012\u0000\n",
      "(X\u0012)TX\u0012\u0000(X\u0012)T~ y\u0000~ yT(X\u0012) +~ yT~ y\u0001\n",
      "=1\n",
      "2r\u0012\u0000\n",
      "\u0012T(XTX)\u0012\u0000~ yT(X\u0012)\u0000~ yT(X\u0012)\u0001\n",
      "=1\n",
      "2r\u0012\u0000\n",
      "\u0012T(XTX)\u0012\u00002(XT~ y)T\u0012\u0001\n",
      "=1\n",
      "2\u0000\n",
      "2XTX\u0012\u00002XT~ y\u0001\n",
      "=XTX\u0012\u0000XT~ y\n",
      "In the third step, we used the fact that aTb=bTa, and in the \f",
      "fth step\n",
      "used the factsrxbTx=bandrxxTAx= 2Axfor symmetric matrix A(for\n",
      "more details, see Section 4.3 of \\Linear Algebra Review and Reference\"). To\n",
      "minimizeJ, we set its derivatives to zero, and obtain the normal equations :\n",
      "XTX\u0012=XT~ y\n",
      "Thus, the value of \u0012that minimizes J(\u0012) is given in closed form by the\n",
      "equation\n",
      "\u0012= (XTX)\u00001XT~ y:3\n",
      "1.3 Probabilistic interpretation\n",
      "When faced with a regression problem, why might linear regression, and\n",
      "speci\f",
      "cally why might the least-squares cost function J, be a reasonable\n",
      "choice? In this section, we will give a set of probabilistic assumptions, under\n",
      "which least-squares regression is derived as a very natural algorithm.\n",
      "Let us assume that the target variables and the inputs are related via the\n",
      "equation\n",
      "y(i)=\u0012Tx(i)+\u000f(i);\n",
      "3Note that in the above step, we are implicitly assuming that XTXis an invertible\n",
      "matrix. This can be checked before calculating the inverse. If either the number of\n",
      "linearly independent examples is fewer than the number of features, or if the features\n",
      "are not linearly independent, then XTXwill not be invertible. Even in such cases, it is\n",
      "possible to \\\f",
      "x\" the situation with additional techniques, which we skip here for the sake\n",
      "of simplicty.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "16\n",
      "where\u000f(i)is an error term that captures either unmodeled e\u000b",
      "ects (such as\n",
      "if there are some features very pertinent to predicting housing price, but\n",
      "that we'd left out of the regression), or random noise. Let us further assume\n",
      "that the\u000f(i)are distributed IID (independently and identically distributed)\n",
      "according to a Gaussian distribution (also called a Normal distribution) with\n",
      "mean zero and some variance \u001b2. We can write this assumption as \\ \u000f(i)\u0018\n",
      "N(0;\u001b2).\" I.e., the density of \u000f(i)is given by\n",
      "p(\u000f(i)) =1p\n",
      "2\u0019\u001bexp\u0012\n",
      "\u0000(\u000f(i))2\n",
      "2\u001b2\u0013\n",
      ":\n",
      "This implies that\n",
      "p(y(i)jx(i);\u0012) =1p\n",
      "2\u0019\u001bexp\u0012\n",
      "\u0000(y(i)\u0000\u0012Tx(i))2\n",
      "2\u001b2\u0013\n",
      ":\n",
      "The notation \\ p(y(i)jx(i);\u0012)\" indicates that this is the distribution of y(i)\n",
      "givenx(i)and parameterized by \u0012. Note that we should not condition on \u0012\n",
      "(\\p(y(i)jx(i);\u0012)\"), since\u0012is not a random variable. We can also write the\n",
      "distribution of y(i)asy(i)jx(i);\u0012\u0018N(\u0012Tx(i);\u001b2).\n",
      "GivenX(the design matrix, which contains all the x(i)'s) and\u0012, what\n",
      "is the distribution of the y(i)'s? The probability of the data is given by\n",
      "p(~ yjX;\u0012). This quantity is typically viewed a function of ~ y(and perhaps X),\n",
      "for a \f",
      "xed value of \u0012. When we wish to explicitly view this as a function of\n",
      "\u0012, we will instead call it the likelihood function:\n",
      "L(\u0012) =L(\u0012;X;~ y) =p(~ yjX;\u0012):\n",
      "Note that by the independence assumption on the \u000f(i)'s (and hence also the\n",
      "y(i)'s given the x(i)'s), this can also be written\n",
      "L(\u0012) =nY\n",
      "i=1p(y(i)jx(i);\u0012)\n",
      "=nY\n",
      "i=11p\n",
      "2\u0019\u001bexp\u0012\n",
      "\u0000(y(i)\u0000\u0012Tx(i))2\n",
      "2\u001b2\u0013\n",
      ":\n",
      "Now, given this probabilistic model relating the y(i)'s and thex(i)'s, what\n",
      "is a reasonable way of choosing our best guess of the parameters \u0012? The\n",
      "principal of maximum likelihood says that we should choose \u0012so as to\n",
      "make the data as high probability as possible. I.e., we should choose \u0012to\n",
      "maximizeL(\u0012).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "17\n",
      "Instead of maximizing L(\u0012), we can also maximize any strictly increasing\n",
      "function of L(\u0012). In particular, the derivations will be a bit simpler if we\n",
      "instead maximize the log likelihood `(\u0012):\n",
      "`(\u0012) = logL(\u0012)\n",
      "= lognY\n",
      "i=11p\n",
      "2\u0019\u001bexp\u0012\n",
      "\u0000(y(i)\u0000\u0012Tx(i))2\n",
      "2\u001b2\u0013\n",
      "=nX\n",
      "i=1log1p\n",
      "2\u0019\u001bexp\u0012\n",
      "\u0000(y(i)\u0000\u0012Tx(i))2\n",
      "2\u001b2\u0013\n",
      "=nlog1p\n",
      "2\u0019\u001b\u00001\n",
      "\u001b2\u00011\n",
      "2nX\n",
      "i=1(y(i)\u0000\u0012Tx(i))2:\n",
      "Hence, maximizing `(\u0012) gives the same answer as minimizing\n",
      "1\n",
      "2nX\n",
      "i=1(y(i)\u0000\u0012Tx(i))2;\n",
      "which we recognize to be J(\u0012), our original least-squares cost function.\n",
      "To summarize: Under the previous probabilistic assumptions on the data,\n",
      "least-squares regression corresponds to \f",
      "nding the maximum likelihood esti-\n",
      "mate of\u0012. This is thus one set of assumptions under which least-squares re-\n",
      "gression can be justi\f",
      "ed as a very natural method that's just doing maximum\n",
      "likelihood estimation. (Note however that the probabilistic assumptions are\n",
      "by no means necessary for least-squares to be a perfectly good and rational\n",
      "procedure, and there may|and indeed there are|other natural assumptions\n",
      "that can also be used to justify it.)\n",
      "Note also that, in our previous discussion, our \f",
      "nal choice of \u0012did not\n",
      "depend on what was \u001b2, and indeed we'd have arrived at the same result\n",
      "even if\u001b2were unknown. We will use this fact again later, when we talk\n",
      "about the exponential family and generalized linear models.\n",
      "1.4 Locally weighted linear regression (optional\n",
      "reading)\n",
      "Consider the problem of predicting yfromx2R. The leftmost \f",
      "gure below\n",
      "shows the result of \f",
      "tting a y=\u00120+\u00121xto a dataset. We see that the data\n",
      "doesn't really lie on straight line, and so the \f",
      "t is not very good.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "18\n",
      "0 1 2 3 4 5 6 700.511.522.533.544.5\n",
      "xy\n",
      "0 1 2 3 4 5 6 700.511.522.533.544.5\n",
      "xy\n",
      "0 1 2 3 4 5 6 700.511.522.533.544.5\n",
      "xy\n",
      "Instead, if we had added an extra feature x2, and \f",
      "ty=\u00120+\u00121x+\u00122x2,\n",
      "then we obtain a slightly better \f",
      "t to the data. (See middle \f",
      "gure) Naively, it\n",
      "might seem that the more features we add, the better. However, there is also\n",
      "a danger in adding too many features: The rightmost \f",
      "gure is the result of\n",
      "\f",
      "tting a 5-th order polynomial y=P5\n",
      "j=0\u0012jxj. We see that even though the\n",
      "\f",
      "tted curve passes through the data perfectly, we would not expect this to\n",
      "be a very good predictor of, say, housing prices ( y) for di\u000b",
      "erent living areas\n",
      "(x). Without formally de\f",
      "ning what these terms mean, we'll say the \f",
      "gure\n",
      "on the left shows an instance of under\f",
      "tting |in which the data clearly\n",
      "shows structure not captured by the model|and the \f",
      "gure on the right is\n",
      "an example of over\f",
      "tting . (Later in this class, when we talk about learning\n",
      "theory we'll formalize some of these notions, and also de\f",
      "ne more carefully\n",
      "just what it means for a hypothesis to be good or bad.)\n",
      "As discussed previously, and as shown in the example above, the choice of\n",
      "features is important to ensuring good performance of a learning algorithm.\n",
      "(When we talk about model selection, we'll also see algorithms for automat-\n",
      "y talk choosing a good set of features.) In this section, let us brie\n",
      "about the locally weighted linear regression (LWR) algorithm which, assum-\n",
      "ing there is su\u000ecient training data, makes the choice of features less critical.\n",
      "This treatment will be brief, since you'll get a chance to explore some of the\n",
      "properties of the LWR algorithm yourself in the homework.\n",
      "In the original linear regression algorithm, to make a prediction at a query\n",
      "pointx(i.e., to evaluate h(x)), we would:\n",
      "1. Fit\u0012to minimizeP\n",
      "i(y(i)\u0000\u0012Tx(i))2.\n",
      "2. Output\u0012Tx.\n",
      "In contrast, the locally weighted linear regression algorithm does the fol-\n",
      "lowing:\n",
      "1. Fit\u0012to minimizeP\n",
      "iw(i)(y(i)\u0000\u0012Tx(i))2.\n",
      "2. Output\u0012Tx.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "19\n",
      "Here, thew(i)'s are non-negative valued weights . Intuitively, if w(i)is large\n",
      "for a particular value of i, then in picking \u0012, we'll try hard to make ( y(i)\u0000\n",
      "\u0012Tx(i))2small. Ifw(i)is small, then the ( y(i)\u0000\u0012Tx(i))2error term will be\n",
      "pretty much ignored in the \f",
      "t.\n",
      "A fairly standard choice for the weights is4\n",
      "w(i)= exp\u0012\n",
      "\u0000(x(i)\u0000x)2\n",
      "2\u001c",
      "2\u0013\n",
      "Note that the weights depend on the particular point xat which we're trying\n",
      "to evaluate x. Moreover, ifjx(i)\u0000xjis small, then w(i)is close to 1; and\n",
      "ifjx(i)\u0000xjis large, then w(i)is small. Hence, \u0012is chosen giving a much\n",
      "higher \\weight\" to the (errors on) training examples close to the query point\n",
      "x. (Note also that while the formula for the weights takes a form that is\n",
      "cosmetically similar to the density of a Gaussian distribution, the w(i)'s do\n",
      "not directly have anything to do with Gaussians, and in particular the w(i)\n",
      "are not random variables, normally distributed or otherwise.) The parameter\n",
      "\u001c",
      "controls how quickly the weight of a training example falls o\u000b",
      " with distance\n",
      "of itsx(i)from the query point x;\u001c",
      "is called the bandwidth parameter, and\n",
      "is also something that you'll get to experiment with in your homework.\n",
      "Locally weighted linear regression is the \f",
      "rst example we're seeing of a\n",
      "non-parametric algorithm. The (unweighted) linear regression algorithm\n",
      "that we saw earlier is known as a parametric learning algorithm, because\n",
      "it has a \f",
      "xed, \f",
      "nite number of parameters (the \u0012i's), which are \f",
      "t to the\n",
      "data. Once we've \f",
      "t the \u0012i's and stored them away, we no longer need to\n",
      "keep the training data around to make future predictions. In contrast, to\n",
      "make predictions using locally weighted linear regression, we need to keep\n",
      "the entire training set around. The term \\non-parametric\" (roughly) refers\n",
      "to the fact that the amount of stu\u000b",
      " we need to keep in order to represent the\n",
      "hypothesis hgrows linearly with the size of the training set.\n",
      "4Ifxis vector-valued, this is generalized to be w(i)= exp(\u0000(x(i)\u0000x)T(x(i)\u0000x)=(2\u001c",
      "2)),\n",
      "orw(i)= exp(\u0000(x(i)\u0000x)T\u0006\u00001(x(i)\u0000x)=(2\u001c",
      "2)), for an appropriate choice of \u001c",
      "or \u0006.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 2\n",
      "Classi\f",
      "cation and logistic\n",
      "regression\n",
      "Let's now talk about the classi\f",
      "cation problem. This is just like the regression\n",
      "problem, except that the values ywe now want to predict take on only\n",
      "a small number of discrete values. For now, we will focus on the binary\n",
      "classi\f",
      "cation problem in which ycan take on only two values, 0 and 1.\n",
      "(Most of what we say here will also generalize to the multiple-class case.)\n",
      "For instance, if we are trying to build a spam classi\f",
      "er for email, then x(i)\n",
      "may be some features of a piece of email, and ymay be 1 if it is a piece\n",
      "of spam mail, and 0 otherwise. 0 is also called the negative class , and 1\n",
      "thepositive class , and they are sometimes also denoted by the symbols \\-\"\n",
      "and \\+.\" Given x(i), the corresponding y(i)is also called the label for the\n",
      "training example.\n",
      "2.1 Logistic regression\n",
      "We could approach the classi\f",
      "cation problem ignoring the fact that yis\n",
      "discrete-valued, and use our old linear regression algorithm to try to predict\n",
      "ygivenx. However, it is easy to construct examples where this method\n",
      "performs very poorly. Intuitively, it also doesn't make sense for h\u0012(x) to take\n",
      "values larger than 1 or smaller than 0 when we know that y2f0;1g.\n",
      "To \f",
      "x this, let's change the form for our hypotheses h\u0012(x). We will choose\n",
      "h\u0012(x) =g(\u0012Tx) =1\n",
      "1 +e\u0000\u0012Tx;\n",
      "where\n",
      "g(z) =1\n",
      "1 +e\u0000z\n",
      "20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "21\n",
      "is called the logistic function or the sigmoid function . Here is a plot\n",
      "showingg(z):\n",
      "−5 −4 −3 −2 −1 0 1 2 3 4 500.10.20.30.40.50.60.70.80.91\n",
      "zg(z)\n",
      "Notice that g(z) tends towards 1 as z!1 , andg(z) tends towards 0 as\n",
      "z!\u00001 . Moreover, g(z), and hence also h(x), is always bounded between\n",
      "0 and 1. As before, we are keeping the convention of letting x0= 1, so that\n",
      "\u0012Tx=\u00120+Pd\n",
      "j=1\u0012jxj.\n",
      "For now, let's take the choice of gas given. Other functions that smoothly\n",
      "increase from 0 to 1 can also be used, but for a couple of reasons that we'll see\n",
      "later (when we talk about GLMs, and when we talk about generative learning\n",
      "algorithms), the choice of the logistic function is a fairly natural one. Before\n",
      "moving on, here's a useful property of the derivative of the sigmoid function,\n",
      "which we write as g0:\n",
      "g0(z) =d\n",
      "dz1\n",
      "1 +e\u0000z\n",
      "=1\n",
      "(1 +e\u0000z)2\u0000\n",
      "e\u0000z\u0001\n",
      "=1\n",
      "(1 +e\u0000z)\u0001\u0012\n",
      "1\u00001\n",
      "(1 +e\u0000z)\u0013\n",
      "=g(z)(1\u0000g(z)):\n",
      "So, given the logistic regression model, how do we \f",
      "t \u0012for it? Following\n",
      "how we saw least squares regression could be derived as the maximum like-\n",
      "lihood estimator under a set of assumptions, let's endow our classi\f",
      "cation\n",
      "model with a set of probabilistic assumptions, and then \f",
      "t the parameters\n",
      "via maximum likelihood.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "22\n",
      "Let us assume that\n",
      "P(y= 1jx;\u0012) =h\u0012(x)\n",
      "P(y= 0jx;\u0012) = 1\u0000h\u0012(x)\n",
      "Note that this can be written more compactly as\n",
      "p(yjx;\u0012) = (h\u0012(x))y(1\u0000h\u0012(x))1\u0000y\n",
      "Assuming that the ntraining examples were generated independently, we\n",
      "can then write down the likelihood of the parameters as\n",
      "L(\u0012) =p(~ yjX;\u0012)\n",
      "=nY\n",
      "i=1p(y(i)jx(i);\u0012)\n",
      "=nY\n",
      "i=1\u0000\n",
      "h\u0012(x(i))\u0001y(i)\u0000\n",
      "1\u0000h\u0012(x(i))\u00011\u0000y(i)\n",
      "As before, it will be easier to maximize the log likelihood:\n",
      "`(\u0012) = logL(\u0012) =nX\n",
      "i=1y(i)logh(x(i)) + (1\u0000y(i)) log(1\u0000h(x(i))) (2.1)\n",
      "How do we maximize the likelihood? Similar to our derivation in the case\n",
      "of linear regression, we can use gradient ascent. Written in vectorial notation,\n",
      "our updates will therefore be given by \u0012:=\u0012+\u000b",
      "r\u0012`(\u0012). (Note the positive\n",
      "rather than negative sign in the update formula, since we're maximizing,\n",
      "rather than minimizing, a function now.) Let's start by working with just\n",
      "one training example ( x;y), and take derivatives to derive the stochastic\n",
      "gradient ascent rule:\n",
      "@\n",
      "@\u0012j`(\u0012) =\u0012\n",
      "y1\n",
      "g(\u0012Tx)\u0000(1\u0000y)1\n",
      "1\u0000g(\u0012Tx)\u0013@\n",
      "@\u0012jg(\u0012Tx)\n",
      "=\u0012\n",
      "y1\n",
      "g(\u0012Tx)\u0000(1\u0000y)1\n",
      "1\u0000g(\u0012Tx)\u0013\n",
      "g(\u0012Tx)(1\u0000g(\u0012Tx))@\n",
      "@\u0012j\u0012Tx\n",
      "=\u0000\n",
      "y(1\u0000g(\u0012Tx))\u0000(1\u0000y)g(\u0012Tx)\u0001\n",
      "xj\n",
      "= (y\u0000h\u0012(x))xj (2.2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "23\n",
      "Above, we used the fact that g0(z) =g(z)(1\u0000g(z)). This therefore gives us\n",
      "the stochastic gradient ascent rule\n",
      "\u0012j:=\u0012j+\u000b",
      "\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      "j\n",
      "If we compare this to the LMS update rule, we see that it looks identical; but\n",
      "this is notthe same algorithm, because h\u0012(x(i)) is now de\f",
      "ned as a non-linear\n",
      "function of \u0012Tx(i). Nonetheless, it's a little surprising that we end up with\n",
      "the same update rule for a rather di\u000b",
      "erent algorithm and learning problem.\n",
      "Is this coincidence, or is there a deeper reason behind this? We'll answer this\n",
      "when we get to GLM models.\n",
      "Remark 2.1.1: An alternative notational viewpoint of the same loss func-\n",
      "tion is also useful, especially for Section 7.1 where we study nonlinear models.\n",
      "Let`logistic :R\u0002f0;1g!R\u00150be the logistic loss de\f",
      "ned as\n",
      "`logistic (t;y),ylog(1 + exp(\u0000t)) + (1\u0000y) log(1 + exp( t)): (2.3)\n",
      "One can verify by plugging in h\u0012(x) = 1=(1 +e\u0000\u0012>x) that the negative log-\n",
      "likelihood (the negation of `(\u0012) in equation (2.1)) can be re-written as\n",
      "\u0000`(\u0012) =`logistic (\u0012>x;y): (2.4)\n",
      "Oftentimes \u0012>xortis called the logit. Basic calculus gives us that\n",
      "@`logistic (t;y)\n",
      "@t=y\u0000exp(\u0000t)\n",
      "1 + exp(\u0000t)+ (1\u0000y)1\n",
      "1 + exp(\u0000t)(2.5)\n",
      "= 1=(1 + exp(\u0000t))\u0000y: (2.6)\n",
      "Then, using the chain rule, we have that\n",
      "@\n",
      "@\u0012j`(\u0012) =\u0000@`logistic (t;y)\n",
      "@t\u0001@t\n",
      "@\u0012j(2.7)\n",
      "= (y\u00001=(1 + exp(\u0000t)))\u0001xj= (y\u0000h\u0012(x))xj; (2.8)\n",
      "which is consistent with the derivation in equation (2.2). We will see this\n",
      "viewpoint can be extended nonlinear models in Section 7.1.\n",
      "2.2 Digression: the perceptron learning algo-\n",
      "rithm\n",
      "y about an algorithm that's of some historical\n",
      "interest, and that we will also return to later when we talk about learning\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "24\n",
      "theory. Consider modifying the logistic regression method to \\force\" it to\n",
      "output values that are either 0 or 1 or exactly. To do so, it seems natural to\n",
      "change the de\f",
      "nition of gto be the threshold function:\n",
      "g(z) =\u001a1 ifz\u00150\n",
      "0 ifz <0\n",
      "If we then let h\u0012(x) =g(\u0012Tx) as before but using this modi\f",
      "ed de\f",
      "nition of\n",
      "g, and if we use the update rule\n",
      "\u0012j:=\u0012j+\u000b",
      "\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      "j:\n",
      "then we have the perceptron learning algorithn .\n",
      "In the 1960s, this \\perceptron\" was argued to be a rough model for how\n",
      "individual neurons in the brain work. Given how simple the algorithm is, it\n",
      "will also provide a starting point for our analysis when we talk about learning\n",
      "theory later in this class. Note however that even though the perceptron may\n",
      "be cosmetically similar to the other algorithms we talked about, it is actually\n",
      "a very di\u000b",
      "erent type of algorithm than logistic regression and least squares\n",
      "linear regression; in particular, it is di\u000ecult to endow the perceptron's predic-\n",
      "tions with meaningful probabilistic interpretations, or derive the perceptron\n",
      "as a maximum likelihood estimation algorithm.\n",
      "2.3 Multi-class classi\f",
      "cation\n",
      "Consider a classi\f",
      "cation problem in which the response variable ycan take on\n",
      "any one ofkvalues, soy2f1;2;:::;kg. For example, rather than classifying\n",
      "emails into the two classes spam or not-spam|which would have been a\n",
      "binary classi\f",
      "cation problem|we might want to classify them into three\n",
      "classes, such as spam, personal mails, and work-related mails. The label /\n",
      "response variable is still discrete, but can now take on more than two values.\n",
      "We will thus model it as distributed according to a multinomial distribution.\n",
      "In this case, p(yjx;\u0012) is a distribution over kpossible discrete outcomes\n",
      "and is thus a multinomial distribution. Recall that a multinomial distribu-\n",
      "tion involves knumbers\u001e",
      "1;:::;\u001e",
      "kspecifying the probability of each of the\n",
      "outcomes. Note that these numbers must satisfyPk\n",
      "i=1\u001e",
      "i= 1. We will de-\n",
      "sign a parameterized model that outputs \u001e",
      "1;:::;\u001e",
      "ksatisfying this constraint\n",
      "given the input x.\n",
      "We introduce kgroups of parameters \u00121;:::;\u0012k, each of them being a\n",
      "vector in Rd. Intuitively, we would like to use \u0012>\n",
      "1x;:::;\u0012>\n",
      "kxto represent\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "25\n",
      "\u001e",
      "1;:::;\u001e",
      "k, the probabilities P(y= 1jx;\u0012);:::;P (y=kjx;\u0012). However,\n",
      "there are two issues with such a direct approach. First, \u0012>\n",
      "jxis not neces-\n",
      "sarily within [0 ;1]. Second, the summation of \u0012>\n",
      "jx's is not necessarily 1.\n",
      "Thus, instead, we will use the softmax function to turn ( \u0012>\n",
      "1x;\u0001\u0001\u0001;\u0012>\n",
      "kx) into\n",
      "a probability vector with nonnegative entries that sum up to 1.\n",
      "De\f",
      "ne the softmax function softmax : Rk!Rkas\n",
      "softmax(t1;:::;tk) =2\n",
      "664exp(t1)Pk\n",
      "j=1exp(tj)\n",
      "...\n",
      "exp(tk)Pk\n",
      "j=1exp(tj)3\n",
      "775: (2.9)\n",
      "The inputs to the softmax function, the vector there, are often called log-\n",
      "its. Note that by de\f",
      "nition, the output of the softmax function is always a\n",
      "probability vector whose entries are nonnegative and sum up to 1.\n",
      "Let (t1;:::;tk) = (\u0012>\n",
      "1x;\u0001\u0001\u0001;\u0012>\n",
      "kx). We apply the softmax function to\n",
      "(t1;:::;tk), and use the output as the probabilities P(y= 1jx;\u0012);:::;P (y=\n",
      "kjx;\u0012). We obtain the following probabilistic model:\n",
      "2\n",
      "64P(y= 1jx;\u0012)\n",
      "...\n",
      "P(y=kjx;\u0012)3\n",
      "75= softmax( t1;\u0001\u0001\u0001;tk) =2\n",
      "6664exp(\u0012>\n",
      "1x)Pk\n",
      "j=1exp(\u0012>\n",
      "jx)\n",
      "...\n",
      "exp(\u0012>\n",
      "kx)Pk\n",
      "j=1exp(\u0012>\n",
      "jx)3\n",
      "7775: (2.10)\n",
      "For notational convenience, we will let \u001e",
      "i=exp(ti)Pk\n",
      "j=1exp(tj). More succinctly, the\n",
      "equation above can be written as:\n",
      "P(y=ijx;\u0012) =\u001e",
      "i=exp(ti)Pk\n",
      "j=1exp(tj)=exp(\u0012>\n",
      "ix)Pk\n",
      "j=1exp(\u0012>\n",
      "jx): (2.11)\n",
      "Next, we compute the negative log-likelihood of a single example ( x;y).\n",
      "\u0000logp(yjx;\u0012) =\u0000log \n",
      "exp(ty)Pk\n",
      "j=1exp(tj)!\n",
      "=\u0000log \n",
      "exp(\u0012>\n",
      "yx)\n",
      "Pk\n",
      "j=1exp(\u0012>\n",
      "jx)!\n",
      "(2.12)\n",
      "Thus, the loss function, the negative log-likelihood of the training data, is\n",
      "given as\n",
      "`(\u0012) =nX\n",
      "i=1\u0000log \n",
      "exp(\u0012>\n",
      "y(i)x(i))\n",
      "Pk\n",
      "j=1exp(\u0012>\n",
      "jx(i))!\n",
      ": (2.13)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "26\n",
      "It's convenient to de\f",
      "ne the cross-entropy loss `ce:Rk\u0002f1;:::;kg!R\u00150,\n",
      "which modularizes in the complex equation above:1\n",
      "`ce((t1;:::;tk);y) =\u0000log \n",
      "exp(ty)Pk\n",
      "j=1exp(tj)!\n",
      ": (2.14)\n",
      "With this notation, we can simply rewrite equation (2.13) as\n",
      "`(\u0012) =nX\n",
      "i=1`ce((\u0012>\n",
      "1x(i);:::;\u0012>\n",
      "kx(i));y(i)): (2.15)\n",
      "Moreover, conveniently, the cross-entropy loss also has a simple gradient. Let\n",
      "t= (t1;:::;tk), and recall \u001e",
      "i=exp(ti)Pk\n",
      "j=1exp(tj). By basic calculus, we can derive\n",
      "@`ce(t;y)\n",
      "@ti=\u001e",
      "i\u00001fy=ig; (2.16)\n",
      "where 1f\u0001gis the indicator function, that is, 1 fy=ig= 1 ify=i, and\n",
      "1fy=ig= 0 ify6=i. Alternatively, in vectorized notations, we have the\n",
      "following form which will be useful for Chapter 7:\n",
      "@`ce(t;y)\n",
      "@t=\u001e",
      "\u0000ey; (2.17)\n",
      "wherees2Rkis thes-th natural basis vector (where the s-th entry is 1 and\n",
      "all other entries are zeros.) Using Chain rule, we have that\n",
      "@`ce((\u0012>\n",
      "1x;:::;\u0012>\n",
      "kx);y)\n",
      "@\u0012i=@`(t;y)\n",
      "@ti\u0001@ti\n",
      "@\u0012i= (\u001e",
      "i\u00001fy=ig)\u0001x: (2.18)\n",
      "Therefore, the gradient of the loss with respect to the part of parameter \u0012iis\n",
      "@`(\u0012)\n",
      "@\u0012i=nX\n",
      "j=1(\u001e",
      "(j)\n",
      "i\u00001fy(j)=ig)\u0001x(j); (2.19)\n",
      "where\u001e",
      "(j)\n",
      "i=exp(\u0012>\n",
      "ix(j))Pk\n",
      "s=1exp(\u0012>sx(j))is the probability that the model predicts item i\n",
      "for example x(j). With the gradients above, one can implement (stochastic)\n",
      "gradient descent to minimize the loss function `(\u0012).\n",
      "1There are some ambiguity in the naming here. Some people call the cross-entropy loss\n",
      "the function that maps the probability vector (the \u001e",
      "in our language) and label yto the\n",
      "\f",
      "nal real number, and call our version of cross-entropy loss softmax-cross-entropy loss.\n",
      "We choose our current naming convention because it's consistent with the naming of most\n",
      "modern deep learning library such as PyTorch and Jax.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "27\n",
      "1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060\n",
      "xf(x)\n",
      "1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060\n",
      "xf(x)\n",
      "1 1.5 2 2.5 3 3.5 4 4.5 5−100102030405060\n",
      "xf(x)\n",
      "2.4 Another algorithm for maximizing `(\u0012)\n",
      "Returning to logistic regression with g(z) being the sigmoid function, let's\n",
      "now talk about a di\u000b",
      "erent algorithm for maximizing `(\u0012).\n",
      "To get us started, let's consider Newton's method for \f",
      "nding a zero of a\n",
      "function. Speci\f",
      "cally, suppose we have some function f:R7!R, and we\n",
      "wish to \f",
      "nd a value of \u0012so thatf(\u0012) = 0. Here, \u00122Ris a real number.\n",
      "Newton's method performs the following update:\n",
      "\u0012:=\u0012\u0000f(\u0012)\n",
      "f0(\u0012):\n",
      "This method has a natural interpretation in which we can think of it as\n",
      "approximating the function fvia a linear function that is tangent to fat\n",
      "the current guess \u0012, solving for where that linear function equals to zero, and\n",
      "letting the next guess for \u0012be where that linear function is zero.\n",
      "Here's a picture of the Newton's method in action:\n",
      "In the leftmost \f",
      "gure, we see the function fplotted along with the line\n",
      "y= 0. We're trying to \f",
      "nd \u0012so thatf(\u0012) = 0; the value of \u0012that achieves this\n",
      "is about 1.3. Suppose we initialized the algorithm with \u0012= 4:5. Newton's\n",
      "method then \f",
      "ts a straight line tangent to fat\u0012= 4:5, and solves for the\n",
      "where that line evaluates to 0. (Middle \f",
      "gure.) This give us the next guess\n",
      "for\u0012, which is about 2.8. The rightmost \f",
      "gure shows the result of running\n",
      "one more iteration, which the updates \u0012to about 1.8. After a few more\n",
      "iterations, we rapidly approach \u0012= 1:3.\n",
      "Newton's method gives a way of getting to f(\u0012) = 0. What if we want to\n",
      "use it to maximize some function `? The maxima of `correspond to points\n",
      "where its \f",
      "rst derivative `0(\u0012) is zero. So, by letting f(\u0012) =`0(\u0012), we can use\n",
      "the same algorithm to maximize `, and we obtain update rule:\n",
      "\u0012:=\u0012\u0000`0(\u0012)\n",
      "`00(\u0012):\n",
      "(Something to think about: How would this change if we wanted to use\n",
      "Newton's method to minimize rather than maximize a function?)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "28\n",
      "Lastly, in our logistic regression setting, \u0012is vector-valued, so we need to\n",
      "generalize Newton's method to this setting. The generalization of Newton's\n",
      "method to this multidimensional setting (also called the Newton-Raphson\n",
      "method) is given by\n",
      "\u0012:=\u0012\u0000H\u00001r\u0012`(\u0012):\n",
      "Here,r\u0012`(\u0012) is, as usual, the vector of partial derivatives of `(\u0012) with respect\n",
      "to the\u0012i's; andHis and-by-dmatrix (actually, d+1\u0000by\u0000d+1, assuming that\n",
      "we include the intercept term) called the Hessian , whose entries are given\n",
      "by\n",
      "Hij=@2`(\u0012)\n",
      "@\u0012i@\u0012j:\n",
      "Newton's method typically enjoys faster convergence than (batch) gra-\n",
      "dient descent, and requires many fewer iterations to get very close to the\n",
      "minimum. One iteration of Newton's can, however, be more expensive than\n",
      "one iteration of gradient descent, since it requires \f",
      "nding and inverting an\n",
      "d-by-dHessian; but so long as dis not too large, it is usually much faster\n",
      "overall. When Newton's method is applied to maximize the logistic regres-\n",
      "sion log likelihood function `(\u0012), the resulting method is also called Fisher\n",
      "scoring .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 3\n",
      "Generalized linear models\n",
      "So far, we've seen a regression example, and a classi\f",
      "cation example. In the\n",
      "regression example, we had yjx;\u0012\u0018N (\u0016;\u001b2), and in the classi\f",
      "cation one,\n",
      "yjx;\u0012\u0018Bernoulli(\u001e",
      "), for some appropriate de\f",
      "nitions of \u0016and\u001e",
      "as functions\n",
      "ofxand\u0012. In this section, we will show that both of these methods are\n",
      "special cases of a broader family of models, called Generalized Linear Models\n",
      "(GLMs).1We will also show how other models in the GLM family can be\n",
      "derived and applied to other classi\f",
      "cation and regression problems.\n",
      "3.1 The exponential family\n",
      "To work our way up to GLMs, we will begin by de\f",
      "ning exponential family\n",
      "distributions. We say that a class of distributions is in the exponential family\n",
      "if it can be written in the form\n",
      "p(y;\u0011) =b(y) exp(\u0011TT(y)\u0000a(\u0011)) (3.1)\n",
      "Here,\u0011is called the natural parameter (also called the canonical param-\n",
      "eter) of the distribution; T(y) is the su\u000ecient statistic (for the distribu-\n",
      "tions we consider, it will often be the case that T(y) =y); anda(\u0011) is the log\n",
      "partition function . The quantity e\u0000a(\u0011)essentially plays the role of a nor-\n",
      "malization constant, that makes sure the distribution p(y;\u0011) sums/integrates\n",
      "overyto 1.\n",
      "A \f",
      "xed choice of T,aandbde\f",
      "nes a family (or set) of distributions that\n",
      "is parameterized by \u0011; as we vary \u0011, we then get di\u000b",
      "erent distributions within\n",
      "this family.\n",
      "1The presentation of the material in this section takes inspiration from Michael I.\n",
      "Jordan, Learning in graphical models (unpublished book draft), and also McCullagh and\n",
      "Nelder, Generalized Linear Models (2nd ed.) .\n",
      "29\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "30\n",
      "We now show that the Bernoulli and the Gaussian distributions are ex-\n",
      "amples of exponential family distributions. The Bernoulli distribution with\n",
      "mean\u001e",
      ", written Bernoulli( \u001e",
      "), speci\f",
      "es a distribution over y2f0;1g, so that\n",
      "p(y= 1;\u001e",
      ") =\u001e",
      ";p(y= 0;\u001e",
      ") = 1\u0000\u001e",
      ". As we vary \u001e",
      ", we obtain Bernoulli\n",
      "distributions with di\u000b",
      "erent means. We now show that this class of Bernoulli\n",
      "distributions, ones obtained by varying \u001e",
      ", is in the exponential family; i.e.,\n",
      "that there is a choice of T,aandbso that Equation (3.1) becomes exactly\n",
      "the class of Bernoulli distributions.\n",
      "We write the Bernoulli distribution as:\n",
      "p(y;\u001e",
      ") =\u001e",
      "y(1\u0000\u001e",
      ")1\u0000y\n",
      "= exp(ylog\u001e",
      "+ (1\u0000y) log(1\u0000\u001e",
      "))\n",
      "= exp\u0012\u0012\n",
      "log\u0012\u001e",
      "\n",
      "1\u0000\u001e",
      "\u0013\u0013\n",
      "y+ log(1\u0000\u001e",
      ")\u0013\n",
      ":\n",
      "Thus, the natural parameter is given by \u0011= log(\u001e",
      "=(1\u0000\u001e",
      ")). Interestingly, if\n",
      "we invert this de\f",
      "nition for \u0011by solving for \u001e",
      "in terms of \u0011, we obtain \u001e",
      "=\n",
      "1=(1 +e\u0000\u0011). This is the familiar sigmoid function! This will come up again\n",
      "when we derive logistic regression as a GLM. To complete the formulation\n",
      "of the Bernoulli distribution as an exponential family distribution, we also\n",
      "have\n",
      "T(y) =y\n",
      "a(\u0011) =\u0000log(1\u0000\u001e",
      ")\n",
      "= log(1 + e\u0011)\n",
      "b(y) = 1\n",
      "This shows that the Bernoulli distribution can be written in the form of\n",
      "Equation (3.1), using an appropriate choice of T,aandb.\n",
      "Let's now move on to consider the Gaussian distribution. Recall that,\n",
      "when deriving linear regression, the value of \u001b2had no e\u000b",
      "ect on our \f",
      "nal\n",
      "choice of\u0012andh\u0012(x). Thus, we can choose an arbitrary value for \u001b2without\n",
      "changing anything. To simplify the derivation below, let's set \u001b2= 1.2We\n",
      "2If we leave \u001b2as a variable, the Gaussian distribution can also be shown to be in the\n",
      "exponential family, where \u00112R2is now a 2-dimension vector that depends on both \u0016and\n",
      "\u001b. For the purposes of GLMs, however, the \u001b2parameter can also be treated by considering\n",
      "a more general de\f",
      "nition of the exponential family: p(y;\u0011;\u001c",
      ") =b(a;\u001c",
      ") exp((\u0011TT(y)\u0000\n",
      "a(\u0011))=c(\u001c",
      ")). Here,\u001c",
      "is called the dispersion parameter , and for the Gaussian, c(\u001c",
      ") =\u001b2;\n",
      "but given our simpli\f",
      "cation above, we won't need the more general de\f",
      "nition for the\n",
      "examples we will consider here.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "31\n",
      "then have:\n",
      "p(y;\u0016) =1p\n",
      "2\u0019exp\u0012\n",
      "\u00001\n",
      "2(y\u0000\u0016)2\u0013\n",
      "=1p\n",
      "2\u0019exp\u0012\n",
      "\u00001\n",
      "2y2\u0013\n",
      "\u0001exp\u0012\n",
      "\u0016y\u00001\n",
      "2\u00162\u0013\n",
      "Thus, we see that the Gaussian is in the exponential family, with\n",
      "\u0011=\u0016\n",
      "T(y) =y\n",
      "a(\u0011) =\u00162=2\n",
      "=\u00112=2\n",
      "b(y) = (1=p\n",
      "2\u0019) exp(\u0000y2=2):\n",
      "There're many other distributions that are members of the exponen-\n",
      "tial family: The multinomial (which we'll see later), the Poisson (for mod-\n",
      "elling count-data; also see the problem set); the gamma and the exponen-\n",
      "tial (for modelling continuous, non-negative random variables, such as time-\n",
      "intervals); the beta and the Dirichlet (for distributions over probabilities);\n",
      "and many more. In the next section, we will describe a general \\recipe\"\n",
      "for constructing models in which y(givenxand\u0012) comes from any of these\n",
      "distributions.\n",
      "3.2 Constructing GLMs\n",
      "Suppose you would like to build a model to estimate the number yof cus-\n",
      "tomers arriving in your store (or number of page-views on your website) in\n",
      "any given hour, based on certain features xsuch as store promotions, recent\n",
      "advertising, weather, day-of-week, etc. We know that the Poisson distribu-\n",
      "tion usually gives a good model for numbers of visitors. Knowing this, how\n",
      "can we come up with a model for our problem? Fortunately, the Poisson is an\n",
      "exponential family distribution, so we can apply a Generalized Linear Model\n",
      "(GLM). In this section, we will we will describe a method for constructing\n",
      "GLM models for problems such as these.\n",
      "More generally, consider a classi\f",
      "cation or regression problem where we\n",
      "would like to predict the value of some random variable yas a function of\n",
      "x. To derive a GLM for this problem, we will make the following three\n",
      "assumptions about the conditional distribution of ygivenxand about our\n",
      "model:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "32\n",
      "1.yjx;\u0012\u0018ExponentialFamily( \u0011). I.e., given xand\u0012, the distribution of\n",
      "yfollows some exponential family distribution, with parameter \u0011.\n",
      "2. Givenx, our goal is to predict the expected value of T(y) givenx.\n",
      "In most of our examples, we will have T(y) =y, so this means we\n",
      "would like the prediction h(x) output by our learned hypothesis hto\n",
      "satisfyh(x) = E[yjx]. (Note that this assumption is satis\f",
      "ed in the\n",
      "choices for h\u0012(x) for both logistic regression and linear regression. For\n",
      "instance, in logistic regression, we had h\u0012(x) =p(y= 1jx;\u0012) = 0\u0001p(y=\n",
      "0jx;\u0012) + 1\u0001p(y= 1jx;\u0012) = E[yjx;\u0012].)\n",
      "3. The natural parameter \u0011and the inputs xare related linearly: \u0011=\u0012Tx.\n",
      "(Or, if\u0011is vector-valued, then \u0011i=\u0012T\n",
      "ix.)\n",
      "The third of these assumptions might seem the least well justi\f",
      "ed of\n",
      "the above, and it might be better thought of as a \\design choice\" in our\n",
      "recipe for designing GLMs, rather than as an assumption per se. These\n",
      "three assumptions/design choices will allow us to derive a very elegant class\n",
      "of learning algorithms, namely GLMs, that have many desirable properties\n",
      "such as ease of learning. Furthermore, the resulting models are often very\n",
      "e\u000b",
      "ective for modelling di\u000b",
      "erent types of distributions over y; for example, we\n",
      "will shortly show that both logistic regression and ordinary least squares can\n",
      "both be derived as GLMs.\n",
      "3.2.1 Ordinary least squares\n",
      "To show that ordinary least squares is a special case of the GLM family\n",
      "of models, consider the setting where the target variable y(also called the\n",
      "response variable in GLM terminology) is continuous, and we model the\n",
      "conditional distribution of ygivenxas a GaussianN(\u0016;\u001b2). (Here,\u0016may\n",
      "dependx.) So, we let the ExponentialFamily (\u0011) distribution above be\n",
      "the Gaussian distribution. As we saw previously, in the formulation of the\n",
      "Gaussian as an exponential family distribution, we had \u0016=\u0011. So, we have\n",
      "h\u0012(x) =E[yjx;\u0012]\n",
      "=\u0016\n",
      "=\u0011\n",
      "=\u0012Tx:\n",
      "The \f",
      "rst equality follows from Assumption 2, above; the second equality\n",
      "follows from the fact that yjx;\u0012\u0018N(\u0016;\u001b2), and so its expected value is given\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "33\n",
      "by\u0016; the third equality follows from Assumption 1 (and our earlier derivation\n",
      "showing that \u0016=\u0011in the formulation of the Gaussian as an exponential\n",
      "family distribution); and the last equality follows from Assumption 3.\n",
      "3.2.2 Logistic regression\n",
      "We now consider logistic regression. Here we are interested in binary classi\f",
      "-\n",
      "cation, soy2f0;1g. Given that yis binary-valued, it therefore seems natural\n",
      "to choose the Bernoulli family of distributions to model the conditional dis-\n",
      "tribution of ygivenx. In our formulation of the Bernoulli distribution as\n",
      "an exponential family distribution, we had \u001e",
      "= 1=(1 +e\u0000\u0011). Furthermore,\n",
      "note that if yjx;\u0012\u0018Bernoulli(\u001e",
      "), then E[yjx;\u0012] =\u001e",
      ". So, following a similar\n",
      "derivation as the one for ordinary least squares, we get:\n",
      "h\u0012(x) =E[yjx;\u0012]\n",
      "=\u001e",
      "\n",
      "= 1=(1 +e\u0000\u0011)\n",
      "= 1=(1 +e\u0000\u0012Tx)\n",
      "So, this gives us hypothesis functions of the form h\u0012(x) = 1=(1 +e\u0000\u0012Tx). If\n",
      "you are previously wondering how we came up with the form of the logistic\n",
      "function 1=(1 +e\u0000z), this gives one answer: Once we assume that ycondi-\n",
      "tioned onxis Bernoulli, it arises as a consequence of the de\f",
      "nition of GLMs\n",
      "and exponential family distributions.\n",
      "To introduce a little more terminology, the function ggiving the distri-\n",
      "bution's mean as a function of the natural parameter ( g(\u0011) = E[T(y);\u0011])\n",
      "is called the canonical response function . Its inverse, g\u00001, is called the\n",
      "canonical link function . Thus, the canonical response function for the\n",
      "Gaussian family is just the identify function; and the canonical response\n",
      "function for the Bernoulli is the logistic function.3\n",
      "3Many texts use gto denote the link function, and g\u00001to denote the response function;\n",
      "but the notation we're using here, inherited from the early machine learning literature,\n",
      "will be more consistent with the notation used in the rest of the class.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 4\n",
      "Generative learning algorithms\n",
      "So far, we've mainly been talking about learning algorithms that model\n",
      "p(yjx;\u0012), the conditional distribution of ygivenx. For instance, logistic\n",
      "regression modeled p(yjx;\u0012) ash\u0012(x) =g(\u0012Tx) wheregis the sigmoid func-\n",
      "tion. In these notes, we'll talk about a di\u000b",
      "erent type of learning algorithm.\n",
      "Consider a classi\f",
      "cation problem in which we want to learn to distinguish\n",
      "between elephants ( y= 1) and dogs ( y= 0), based on some features of\n",
      "an animal. Given a training set, an algorithm like logistic regression or\n",
      "the perceptron algorithm (basically) tries to \f",
      "nd a straight line|that is, a\n",
      "decision boundary|that separates the elephants and dogs. Then, to classify\n",
      "a new animal as either an elephant or a dog, it checks on which side of the\n",
      "decision boundary it falls, and makes its prediction accordingly.\n",
      "Here's a di\u000b",
      "erent approach. First, looking at elephants, we can build a\n",
      "model of what elephants look like. Then, looking at dogs, we can build a\n",
      "separate model of what dogs look like. Finally, to classify a new animal, we\n",
      "can match the new animal against the elephant model, and match it against\n",
      "the dog model, to see whether the new animal looks more like the elephants\n",
      "or more like the dogs we had seen in the training set.\n",
      "Algorithms that try to learn p(yjx) directly (such as logistic regression),\n",
      "or algorithms that try to learn mappings directly from the space of inputs X\n",
      "to the labelsf0;1g, (such as the perceptron algorithm) are called discrim-\n",
      "inative learning algorithms. Here, we'll talk about algorithms that instead\n",
      "try to model p(xjy) (andp(y)). These algorithms are called generative\n",
      "learning algorithms. For instance, if yindicates whether an example is a\n",
      "dog (0) or an elephant (1), then p(xjy= 0) models the distribution of dogs'\n",
      "features, and p(xjy= 1) models the distribution of elephants' features.\n",
      "After modeling p(y) (called the class priors ) andp(xjy), our algorithm\n",
      "34\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "35\n",
      "can then use Bayes rule to derive the posterior distribution on ygivenx:\n",
      "p(yjx) =p(xjy)p(y)\n",
      "p(x):\n",
      "Here, the denominator is given by p(x) =p(xjy= 1)p(y= 1) +p(xjy=\n",
      "0)p(y= 0) (you should be able to verify that this is true from the standard\n",
      "properties of probabilities), and thus can also be expressed in terms of the\n",
      "quantitiesp(xjy) andp(y) that we've learned. Actually, if were calculating\n",
      "p(yjx) in order to make a prediction, then we don't actually need to calculate\n",
      "the denominator, since\n",
      "arg max\n",
      "yp(yjx) = arg max\n",
      "yp(xjy)p(y)\n",
      "p(x)\n",
      "= arg max\n",
      "yp(xjy)p(y):\n",
      "4.1 Gaussian discriminant analysis\n",
      "The \f",
      "rst generative learning algorithm that we'll look at is Gaussian discrim-\n",
      "inant analysis (GDA). In this model, we'll assume that p(xjy) is distributed\n",
      "y about theo a multivariate normal distribution. Let's talk brie\n",
      "properties of multivariate normal distributions before moving on to the GDA\n",
      "model itself.\n",
      "4.1.1 The multivariate normal distribution\n",
      "The multivariate normal distribution in d-dimensions, also called the multi-\n",
      "variate Gaussian distribution, is parameterized by a mean vector \u00162Rd\n",
      "and a covariance matrix \u00062Rd\u0002d, where \u0006\u00150 is symmetric and positive\n",
      "semi-de\f",
      "nite. Also written \\ N(\u0016;\u0006)\", its density is given by:\n",
      "p(x;\u0016;\u0006) =1\n",
      "(2\u0019)d=2j\u0006j1=2exp\u0012\n",
      "\u00001\n",
      "2(x\u0000\u0016)T\u0006\u00001(x\u0000\u0016)\u0013\n",
      ":\n",
      "In the equation above, \\ j\u0006j\" denotes the determinant of the matrix \u0006.\n",
      "For a random variable XdistributedN(\u0016;\u0006), the mean is (unsurpris-\n",
      "ingly) given by \u0016:\n",
      "E[X] =Z\n",
      "xxp(x;\u0016;\u0006)dx=\u0016\n",
      "Thecovariance of a vector-valued random variable Zis de\f",
      "ned as Cov( Z) =\n",
      "E[(Z\u0000E[Z])(Z\u0000E[Z])T]. This generalizes the notion of the variance of a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "36\n",
      "real-valued random variable. The covariance can also be de\f",
      "ned as Cov( Z) =\n",
      "E[ZZT]\u0000(E[Z])(E[Z])T. (You should be able to prove to yourself that these\n",
      "two de\f",
      "nitions are equivalent.) If X\u0018N(\u0016;\u0006), then\n",
      "Cov(X) = \u0006:\n",
      "Here are some examples of what the density of a Gaussian distribution\n",
      "looks like:\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "The left-most \f",
      "gure shows a Gaussian with mean zero (that is, the 2x1\n",
      "zero-vector) and covariance matrix \u0006 = I(the 2x2 identity matrix). A Gaus-\n",
      "sian with zero mean and identity covariance is also called the standard nor-\n",
      "mal distribution . The middle \f",
      "gure shows the density of a Gaussian with\n",
      "zero mean and \u0006 = 0 :6I; and in the rightmost \f",
      "gure shows one with , \u0006 = 2 I.\n",
      "We see that as \u0006 becomes larger, the Gaussian becomes more \\spread-out,\"\n",
      "and as it becomes smaller, the distribution becomes more \\compressed.\"\n",
      "Let's look at some more examples.\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "The \f",
      "gures above show Gaussians with mean 0, and with covariance\n",
      "matrices respectively\n",
      "\u0006 =\u00141 0\n",
      "0 1\u0015\n",
      "; \u0006 =\u00141 0.5\n",
      "0.5 1\u0015\n",
      "; \u0006 =\u00141 0.8\n",
      "0.8 1\u0015\n",
      ":\n",
      "The leftmost \f",
      "gure shows the familiar standard normal distribution, and we\n",
      "see that as we increase the o\u000b",
      "-diagonal entry in \u0006, the density becomes more\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "37\n",
      "\\compressed\" towards the 45\u000eline (given by x1=x2). We can see this more\n",
      "clearly when we look at the contours of the same three densities:\n",
      "−3 −2 −1 0 1 2 3−3−2−10123\n",
      "−3 −2 −1 0 1 2 3−3−2−10123\n",
      "−3 −2 −1 0 1 2 3−3−2−10123\n",
      "Here's one last set of examples generated by varying \u0006:\n",
      "−3 −2 −1 0 1 2 3−3−2−10123\n",
      "−3 −2 −1 0 1 2 3−3−2−10123\n",
      "−3 −2 −1 0 1 2 3−3−2−10123\n",
      "The plots above used, respectively,\n",
      "\u0006 =\u00141 -0.5\n",
      "-0.5 1\u0015\n",
      "; \u0006 =\u00141 -0.8\n",
      "-0.8 1\u0015\n",
      "; \u0006 =\u00143 0.8\n",
      "0.8 1\u0015\n",
      ":\n",
      "From the leftmost and middle \f",
      "gures, we see that by decreasing the o\u000b",
      "-\n",
      "diagonal elements of the covariance matrix, the density now becomes \\com-\n",
      "pressed\" again, but in the opposite direction. Lastly, as we vary the pa-\n",
      "rameters, more generally the contours will form ellipses (the rightmost \f",
      "gure\n",
      "showing an example).\n",
      "As our last set of examples, \f",
      "xing \u0006 = I, by varying \u0016, we can also move\n",
      "the mean of the density around.\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "−3−2−10123\n",
      "−3−2−101230.050.10.150.20.25\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "38\n",
      "The \f",
      "gures above were generated using \u0006 = I, and respectively\n",
      "\u0016=\u00141\n",
      "0\u0015\n",
      ";\u0016=\u0014-0.5\n",
      "0\u0015\n",
      ";\u0016=\u0014-1\n",
      "-1.5\u0015\n",
      ":\n",
      "4.1.2 The Gaussian discriminant analysis model\n",
      "When we have a classi\f",
      "cation problem in which the input features xare\n",
      "continuous-valued random variables, we can then use the Gaussian Discrim-\n",
      "inant Analysis (GDA) model, which models p(xjy) using a multivariate nor-\n",
      "mal distribution. The model is:\n",
      "y\u0018Bernoulli(\u001e",
      ")\n",
      "xjy= 0\u0018 N (\u00160;\u0006)\n",
      "xjy= 1\u0018 N (\u00161;\u0006)\n",
      "Writing out the distributions, this is:\n",
      "p(y) =\u001e",
      "y(1\u0000\u001e",
      ")1\u0000y\n",
      "p(xjy= 0) =1\n",
      "(2\u0019)d=2j\u0006j1=2exp\u0012\n",
      "\u00001\n",
      "2(x\u0000\u00160)T\u0006\u00001(x\u0000\u00160)\u0013\n",
      "p(xjy= 1) =1\n",
      "(2\u0019)d=2j\u0006j1=2exp\u0012\n",
      "\u00001\n",
      "2(x\u0000\u00161)T\u0006\u00001(x\u0000\u00161)\u0013\n",
      "Here, the parameters of our model are \u001e",
      ", \u0006,\u00160and\u00161. (Note that while\n",
      "there're two di\u000b",
      "erent mean vectors \u00160and\u00161, this model is usually applied\n",
      "using only one covariance matrix \u0006.) The log-likelihood of the data is given\n",
      "by\n",
      "`(\u001e",
      ";\u0016 0;\u00161;\u0006) = lognY\n",
      "i=1p(x(i);y(i);\u001e",
      ";\u0016 0;\u00161;\u0006)\n",
      "= lognY\n",
      "i=1p(x(i)jy(i);\u00160;\u00161;\u0006)p(y(i);\u001e",
      "):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "39\n",
      "By maximizing `with respect to the parameters, we \f",
      "nd the maximum like-\n",
      "lihood estimate of the parameters (see problem set 1) to be:\n",
      "\u001e",
      "=1\n",
      "nnX\n",
      "i=11fy(i)= 1g\n",
      "\u00160=Pn\n",
      "i=11fy(i)= 0gx(i)\n",
      "Pn\n",
      "i=11fy(i)= 0g\n",
      "\u00161=Pn\n",
      "i=11fy(i)= 1gx(i)\n",
      "Pn\n",
      "i=11fy(i)= 1g\n",
      "\u0006 =1\n",
      "nnX\n",
      "i=1(x(i)\u0000\u0016y(i))(x(i)\u0000\u0016y(i))T:\n",
      "Pictorially, what the algorithm is doing can be seen in as follows:\n",
      "−2 −1 0 1 2 3 4 5 6 7−7−6−5−4−3−2−101\n",
      "Shown in the \f",
      "gure are the training set, as well as the contours of the\n",
      "two Gaussian distributions that have been \f",
      "t to the data in each of the\n",
      "two classes. Note that the two Gaussians have contours that are the same\n",
      "shape and orientation, since they share a covariance matrix \u0006, but they have\n",
      "di\u000b",
      "erent means \u00160and\u00161. Also shown in the \f",
      "gure is the straight line\n",
      "giving the decision boundary at which p(y= 1jx) = 0:5. On one side of\n",
      "the boundary, we'll predict y= 1 to be the most likely outcome, and on the\n",
      "other side, we'll predict y= 0.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "40\n",
      "4.1.3 Discussion: GDA and logistic regression\n",
      "The GDA model has an interesting relationship to logistic regression. If we\n",
      "view the quantity p(y= 1jx;\u001e",
      ";\u0016 0;\u00161;\u0006) as a function of x, we'll \f",
      "nd that it\n",
      "can be expressed in the form\n",
      "p(y= 1jx;\u001e",
      ";\u0006;\u00160;\u00161) =1\n",
      "1 + exp(\u0000\u0012Tx);\n",
      "where\u0012is some appropriate function of \u001e",
      ";\u0006;\u00160;\u00161.1This is exactly the form\n",
      "that logistic regression|a discriminative algorithm|used to model p(y=\n",
      "1jx).\n",
      "When would we prefer one model over another? GDA and logistic regres-\n",
      "sion will, in general, give di\u000b",
      "erent decision boundaries when trained on the\n",
      "same dataset. Which is better?\n",
      "We just argued that if p(xjy) is multivariate gaussian (with shared \u0006),\n",
      "thenp(yjx) necessarily follows a logistic function. The converse, however,\n",
      "is not true; i.e., p(yjx) being a logistic function does not imply p(xjy) is\n",
      "multivariate gaussian. This shows that GDA makes stronger modeling as-\n",
      "sumptions about the data than does logistic regression. It turns out that\n",
      "when these modeling assumptions are correct, then GDA will \f",
      "nd better \f",
      "ts\n",
      "to the data, and is a better model. Speci\f",
      "cally, when p(xjy) is indeed gaus-\n",
      "sian (with shared \u0006), then GDA is asymptotically e\u000ecient . Informally,\n",
      "this means that in the limit of very large training sets (large n), there is no\n",
      "algorithm that is strictly better than GDA (in terms of, say, how accurately\n",
      "they estimate p(yjx)). In particular, it can be shown that in this setting,\n",
      "GDA will be a better algorithm than logistic regression; and more generally,\n",
      "even for small training set sizes, we would generally expect GDA to better.\n",
      "In contrast, by making signi\f",
      "cantly weaker assumptions, logistic regres-\n",
      "sion is also more robust and less sensitive to incorrect modeling assumptions.\n",
      "There are many di\u000b",
      "erent sets of assumptions that would lead to p(yjx) taking\n",
      "the form of a logistic function. For example, if xjy= 0\u0018Poisson(\u00150), and\n",
      "xjy= 1\u0018Poisson(\u00151), thenp(yjx) will be logistic. Logistic regression will\n",
      "also work well on Poisson data like this. But if we were to use GDA on such\n",
      "data|and \f",
      "t Gaussian distributions to such non-Gaussian data|then the\n",
      "results will be less predictable, and GDA may (or may not) do well.\n",
      "To summarize: GDA makes stronger modeling assumptions, and is more\n",
      "data e\u000ecient (i.e., requires less training data to learn \\well\") when the mod-\n",
      "eling assumptions are correct or at least approximately correct. Logistic\n",
      "1This uses the convention of rede\f",
      "ning the x(i)'s on the right-hand-side to be ( d+ 1)-\n",
      "dimensional vectors by adding the extra coordinate x(i)\n",
      "0= 1; see problem set 1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "41\n",
      "regression makes weaker assumptions, and is signi\f",
      "cantly more robust to\n",
      "deviations from modeling assumptions. Speci\f",
      "cally, when the data is in-\n",
      "deed non-Gaussian, then in the limit of large datasets, logistic regression will\n",
      "almost always do better than GDA. For this reason, in practice logistic re-\n",
      "gression is used more often than GDA. (Some related considerations about\n",
      "discriminative vs. generative models also apply for the Naive Bayes algo-\n",
      "rithm that we discuss next, but the Naive Bayes algorithm is still considered\n",
      "a very good, and is certainly also a very popular, classi\f",
      "cation algorithm.)\n",
      "4.2 Naive bayes (Option Reading)\n",
      "In GDA, the feature vectors xwere continuous, real-valued vectors. Let's\n",
      "now talk about a di\u000b",
      "erent learning algorithm in which the xj's are discrete-\n",
      "valued.\n",
      "For our motivating example, consider building an email spam \f",
      "lter using\n",
      "machine learning. Here, we wish to classify messages according to whether\n",
      "they are unsolicited commercial (spam) email, or non-spam email. After\n",
      "learning to do this, we can then have our mail reader automatically \f",
      "lter\n",
      "out the spam messages and perhaps place them in a separate mail folder.\n",
      "Classifying emails is one example of a broader set of problems called text\n",
      "classi\f",
      "cation .\n",
      "Let's say we have a training set (a set of emails labeled as spam or non-\n",
      "spam). We'll begin our construction of our spam \f",
      "lter by specifying the\n",
      "featuresxjused to represent an email.\n",
      "We will represent an email via a feature vector whose length is equal to\n",
      "the number of words in the dictionary. Speci\f",
      "cally, if an email contains the\n",
      "j-th word of the dictionary, then we will set xj= 1; otherwise, we let xj= 0.\n",
      "For instance, the vector\n",
      "x=2\n",
      "66666666641\n",
      "0\n",
      "0\n",
      "...\n",
      "1\n",
      "...\n",
      "03\n",
      "7777777775a\n",
      "aardvark\n",
      "aardwolf\n",
      "...\n",
      "buy\n",
      "...\n",
      "zygmurgy\n",
      "is used to represent an email that contains the words \\a\" and \\buy,\" but not\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "42\n",
      "\\aardvark,\" \\aardwolf\" or \\zygmurgy.\"2The set of words encoded into the\n",
      "feature vector is called the vocabulary , so the dimension of xis equal to\n",
      "the size of the vocabulary.\n",
      "Having chosen our feature vector, we now want to build a generative\n",
      "model. So, we have to model p(xjy). But if we have, say, a vocabulary of\n",
      "50000 words, then x2f0;1g50000(xis a 50000-dimensional vector of 0's and\n",
      "1's), and if we were to model xexplicitly with a multinomial distribution over\n",
      "the 250000possible outcomes, then we'd end up with a (250000\u00001)-dimensional\n",
      "parameter vector. This is clearly too many parameters.\n",
      "To modelp(xjy), we will therefore make a very strong assumption. We will\n",
      "assume that the xi's are conditionally independent given y. This assumption\n",
      "is called the Naive Bayes (NB) assumption , and the resulting algorithm is\n",
      "called the Naive Bayes classi\f",
      "er . For instance, if y= 1 means spam email;\n",
      "\\buy\" is word 2087 and \\price\" is word 39831; then we are assuming that if\n",
      "I tell youy= 1 (that a particular piece of email is spam), then knowledge\n",
      "ofx2087(knowledge of whether \\buy\" appears in the message) will have no\n",
      "e\u000b",
      "ect on your beliefs about the value of x39831 (whether \\price\" appears).\n",
      "More formally, this can be written p(x2087jy) =p(x2087jy;x 39831). (Note that\n",
      "this is notthe same as saying that x2087andx39831 are independent, which\n",
      "would have been written \\ p(x2087) =p(x2087jx39831)\"; rather, we are only\n",
      "assuming that x2087andx39831 are conditionally independent giveny.)\n",
      "We now have:\n",
      "p(x1;:::;x 50000jy)\n",
      "=p(x1jy)p(x2jy;x 1)p(x3jy;x 1;x2)\u0001\u0001\u0001p(x50000jy;x 1;:::;x 49999)\n",
      "=p(x1jy)p(x2jy)p(x3jy)\u0001\u0001\u0001p(x50000jy)\n",
      "=dY\n",
      "j=1p(xjjy)\n",
      "The \f",
      "rst equality simply follows from the usual properties of probabilities,\n",
      "and the second equality used the NB assumption. We note that even though\n",
      "2Actually, rather than looking through an English dictionary for the list of all English\n",
      "words, in practice it is more common to look through our training set and encode in our\n",
      "feature vector only the words that occur at least once there. Apart from reducing the\n",
      "number of words modeled and hence reducing our computational and space requirements,\n",
      "this also has the advantage of allowing us to model/include as a feature many words\n",
      "that may appear in your email (such as \\cs229\") but that you won't \f",
      "nd in a dictionary.\n",
      "Sometimes (as in the homework), we also exclude the very high frequency words (which\n",
      "will be words like \\the,\" \\of,\" \\and\"; these high frequency, \\content free\" words are called\n",
      "stop words ) since they occur in so many documents and do little to indicate whether an\n",
      "email is spam or non-spam.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "43\n",
      "the Naive Bayes assumption is an extremely strong assumptions, the resulting\n",
      "algorithm works well on many problems.\n",
      "Our model is parameterized by \u001e",
      "jjy=1=p(xj= 1jy= 1),\u001e",
      "jjy=0=p(xj=\n",
      "1jy= 0), and\u001e",
      "y=p(y= 1). As usual, given a training set f(x(i);y(i));i=\n",
      "1;:::;ng, we can write down the joint likelihood of the data:\n",
      "L(\u001e",
      "y;\u001e",
      "jjy=0;\u001e",
      "jjy=1) =nY\n",
      "i=1p(x(i);y(i)):\n",
      "Maximizing this with respect to \u001e",
      "y;\u001e",
      "jjy=0and\u001e",
      "jjy=1gives the maximum\n",
      "likelihood estimates:\n",
      "\u001e",
      "jjy=1=Pn\n",
      "i=11fx(i)\n",
      "j= 1^y(i)= 1gPn\n",
      "i=11fy(i)= 1g\n",
      "\u001e",
      "jjy=0=Pn\n",
      "i=11fx(i)\n",
      "j= 1^y(i)= 0gPn\n",
      "i=11fy(i)= 0g\n",
      "\u001e",
      "y=Pn\n",
      "i=11fy(i)= 1g\n",
      "n\n",
      "In the equations above, the \\ ^\" symbol means \\and.\" The parameters have\n",
      "a very natural interpretation. For instance, \u001e",
      "jjy=1is just the fraction of the\n",
      "spam (y= 1) emails in which word jdoes appear.\n",
      "Having \f",
      "t all these parameters, to make a prediction on a new example\n",
      "with features x, we then simply calculate\n",
      "p(y= 1jx) =p(xjy= 1)p(y= 1)\n",
      "p(x)\n",
      "=\u0010Qd\n",
      "j=1p(xjjy= 1)\u0011\n",
      "p(y= 1)\n",
      "\u0010Qd\n",
      "j=1p(xjjy= 1)\u0011\n",
      "p(y= 1) +\u0010Qd\n",
      "j=1p(xjjy= 0)\u0011\n",
      "p(y= 0);\n",
      "and pick whichever class has the higher posterior probability.\n",
      "Lastly, we note that while we have developed the Naive Bayes algorithm\n",
      "mainly for the case of problems where the features xjare binary-valued, the\n",
      "generalization to where xjcan take values in f1;2;:::;kjgis straightforward.\n",
      "Here, we would simply model p(xjjy) as multinomial rather than as Bernoulli.\n",
      "Indeed, even if some original input attribute (say, the living area of a house,\n",
      "as in our earlier example) were continuous valued, it is quite common to\n",
      "discretize it|that is, turn it into a small set of discrete values|and apply\n",
      "Naive Bayes. For instance, if we use some feature xjto represent living area,\n",
      "we might discretize the continuous values as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "44\n",
      "Living area (sq. feet) <400 400-800 800-1200 1200-1600 >1600\n",
      "xi 1 2 3 4 5\n",
      "Thus, for a house with living area 890 square feet, we would set the value\n",
      "of the corresponding feature xjto 3. We can then apply the Naive Bayes\n",
      "algorithm, and model p(xjjy) with a multinomial distribution, as described\n",
      "previously. When the original, continuous-valued attributes are not well-\n",
      "modeled by a multivariate normal distribution, discretizing the features and\n",
      "using Naive Bayes (instead of GDA) will often result in a better classi\f",
      "er.\n",
      "4.2.1 Laplace smoothing\n",
      "The Naive Bayes algorithm as we have described it will work fairly well\n",
      "for many problems, but there is a simple change that makes it work much\n",
      "y discuss a problem withext classi\f",
      "cation. Let's brie\n",
      "the algorithm in its current form, and then talk about how we can \f",
      "x it.\n",
      "Consider spam/email classi\f",
      "cation, and let's suppose that, we are in the\n",
      "year of 20xx, after completing CS229 and having done excellent work on the\n",
      "project, you decide around May 20xx to submit work you did to the NeurIPS\n",
      "conference for publication.3Because you end up discussing the conference\n",
      "in your emails, you also start getting messages with the word \\neurips\"\n",
      "in it. But this is your \f",
      "rst NeurIPS paper, and until this time, you had\n",
      "not previously seen any emails containing the word \\neurips\"; in particular\n",
      "\\neurips\" did not ever appear in your training set of spam/non-spam emails.\n",
      "Assuming that \\neurips\" was the 35000th word in the dictionary, your Naive\n",
      "Bayes spam \f",
      "lter therefore had picked its maximum likelihood estimates of\n",
      "the parameters \u001e",
      "35000jyto be\n",
      "\u001e",
      "35000jy=1=Pn\n",
      "i=11fx(i)\n",
      "35000 = 1^y(i)= 1gPn\n",
      "i=11fy(i)= 1g= 0\n",
      "\u001e",
      "35000jy=0=Pn\n",
      "i=11fx(i)\n",
      "35000 = 1^y(i)= 0gPn\n",
      "i=11fy(i)= 0g= 0\n",
      "I.e., because it has never seen \\neurips\" before in either spam or non-spam\n",
      "training examples, it thinks the probability of seeing it in either type of email\n",
      "is zero. Hence, when trying to decide if one of these messages containing\n",
      "3NeurIPS is one of the top machine learning conferences. The deadline for submitting\n",
      "a paper is typically in May-June.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "45\n",
      "\\neurips\" is spam, it calculates the class posterior probabilities, and obtains\n",
      "p(y= 1jx) =Qd\n",
      "j=1p(xjjy= 1)p(y= 1)\n",
      "Qd\n",
      "j=1p(xjjy= 1)p(y= 1) +Qd\n",
      "j=1p(xjjy= 0)p(y= 0)\n",
      "=0\n",
      "0:\n",
      "This is because each of the terms \\Qd\n",
      "j=1p(xjjy)\" includes a term p(x35000jy) =\n",
      "0 that is multiplied into it. Hence, our algorithm obtains 0 =0, and doesn't\n",
      "know how to make a prediction.\n",
      "Stating the problem more broadly, it is statistically a bad idea to esti-\n",
      "mate the probability of some event to be zero just because you haven't seen\n",
      "it before in your \f",
      "nite training set. Take the problem of estimating the mean\n",
      "of a multinomial random variable ztaking values inf1;:::;kg. We can pa-\n",
      "rameterize our multinomial with \u001e",
      "j=p(z=j). Given a set of nindependent\n",
      "observationsfz(1);:::;z(n)g, the maximum likelihood estimates are given by\n",
      "\u001e",
      "j=Pn\n",
      "i=11fz(i)=jg\n",
      "n:\n",
      "As we saw previously, if we were to use these maximum likelihood estimates,\n",
      "then some of the \u001e",
      "j's might end up as zero, which was a problem. To avoid\n",
      "this, we can use Laplace smoothing , which replaces the above estimate\n",
      "with\n",
      "\u001e",
      "j=1 +Pn\n",
      "i=11fz(i)=jg\n",
      "k+n:\n",
      "Here, we've added 1 to the numerator, and kto the denominator. Note thatPk\n",
      "j=1\u001e",
      "j= 1 still holds (check this yourself!), which is a desirable property\n",
      "since the\u001e",
      "j's are estimates for probabilities that we know must sum to 1.\n",
      "Also,\u001e",
      "j6= 0 for all values of j, solving our problem of probabilities being\n",
      "estimated as zero. Under certain (arguably quite strong) conditions, it can\n",
      "be shown that the Laplace smoothing actually gives the optimal estimator\n",
      "of the\u001e",
      "j's.\n",
      "Returning to our Naive Bayes classi\f",
      "er, with Laplace smoothing, we\n",
      "therefore obtain the following estimates of the parameters:\n",
      "\u001e",
      "jjy=1=1 +Pn\n",
      "i=11fx(i)\n",
      "j= 1^y(i)= 1g\n",
      "2 +Pn\n",
      "i=11fy(i)= 1g\n",
      "\u001e",
      "jjy=0=1 +Pn\n",
      "i=11fx(i)\n",
      "j= 1^y(i)= 0g\n",
      "2 +Pn\n",
      "i=11fy(i)= 0g\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "46\n",
      "(In practice, it usually doesn't matter much whether we apply Laplace smooth-\n",
      "ing to\u001e",
      "yor not, since we will typically have a fair fraction each of spam and\n",
      "non-spam messages, so \u001e",
      "ywill be a reasonable estimate of p(y= 1) and will\n",
      "be quite far from 0 anyway.)\n",
      "4.2.2 Event models for text classi\f",
      "cation\n",
      "To close o\u000b",
      " our discussion of generative learning algorithms, let's talk about\n",
      "one more model that is speci\f",
      "cally for text classi\f",
      "cation. While Naive Bayes\n",
      "as we've presented it will work well for many classi\f",
      "cation problems, for text\n",
      "classi\f",
      "cation, there is a related model that does even better.\n",
      "In the speci\f",
      "c context of text classi\f",
      "cation, Naive Bayes as presented uses\n",
      "the what's called the Bernoulli event model (or sometimes multi-variate\n",
      "Bernoulli event model ). In this model, we assumed that the way an email\n",
      "is generated is that \f",
      "rst it is randomly determined (according to the class\n",
      "priorsp(y)) whether a spammer or non-spammer will send you your next\n",
      "message. Then, the person sending the email runs through the dictionary,\n",
      "deciding whether to include each word jin that email independently and\n",
      "according to the probabilities p(xj= 1jy) =\u001e",
      "jjy. Thus, the probability of a\n",
      "message was given by p(y)Qd\n",
      "j=1p(xjjy).\n",
      "Here's a di\u000b",
      "erent model, called the Multinomial event model . To\n",
      "describe this model, we will use a di\u000b",
      "erent notation and set of features for\n",
      "representing emails. We let xjdenote the identity of the j-th word in the\n",
      "email. Thus, xjis now an integer taking values in f1;:::;jVjg, wherejVj\n",
      "is the size of our vocabulary (dictionary). An email of dwords is now rep-\n",
      "resented by a vector ( x1;x2;:::;xd) of length d; note that dcan vary for\n",
      "di\u000b",
      "erent documents. For instance, if an email starts with \\A NeurIPS . . . ,\"\n",
      "thenx1= 1 (\\a\" is the \f",
      "rst word in the dictionary), and x2= 35000 (if\n",
      "\\neurips\" is the 35000th word in the dictionary).\n",
      "In the multinomial event model, we assume that the way an email is\n",
      "generated is via a random process in which spam/non-spam is \f",
      "rst deter-\n",
      "mined (according to p(y)) as before. Then, the sender of the email writes the\n",
      "email by \f",
      "rst generating x1from some multinomial distribution over words\n",
      "(p(x1jy)). Next, the second word x2is chosen independently of x1but from\n",
      "the same multinomial distribution, and similarly for x3,x4, and so on, until\n",
      "alldwords of the email have been generated. Thus, the overall probability of\n",
      "a message is given by p(y)Qd\n",
      "j=1p(xjjy). Note that this formula looks like the\n",
      "one we had earlier for the probability of a message under the Bernoulli event\n",
      "model, but that the terms in the formula now mean very di\u000b",
      "erent things. In\n",
      "particularxjjyis now a multinomial, rather than a Bernoulli distribution.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "47\n",
      "The parameters for our new model are \u001e",
      "y=p(y) as before, \u001e",
      "kjy=1=\n",
      "p(xj=kjy= 1) (for any j) and\u001e",
      "kjy=0=p(xj=kjy= 0). Note that we have\n",
      "assumed that p(xjjy) is the same for all values of j(i.e., that the distribution\n",
      "according to which a word is generated does not depend on its position j\n",
      "within the email).\n",
      "If we are given a training set f(x(i);y(i));i= 1;:::;ngwherex(i)=\n",
      "(x(i)\n",
      "1;x(i)\n",
      "2;:::;x(i)\n",
      "di) (here,diis the number of words in the i-training example),\n",
      "the likelihood of the data is given by\n",
      "L(\u001e",
      "y;\u001e",
      "kjy=0;\u001e",
      "kjy=1) =nY\n",
      "i=1p(x(i);y(i))\n",
      "=nY\n",
      "i=1 diY\n",
      "j=1p(x(i)\n",
      "jjy;\u001e",
      "kjy=0;\u001e",
      "kjy=1)!\n",
      "p(y(i);\u001e",
      "y):\n",
      "Maximizing this yields the maximum likelihood estimates of the parameters:\n",
      "\u001e",
      "kjy=1=Pn\n",
      "i=1Pdi\n",
      "j=11fx(i)\n",
      "j=k^y(i)= 1gPn\n",
      "i=11fy(i)= 1gdi\n",
      "\u001e",
      "kjy=0=Pn\n",
      "i=1Pdi\n",
      "j=11fx(i)\n",
      "j=k^y(i)= 0gPn\n",
      "i=11fy(i)= 0gdi\n",
      "\u001e",
      "y=Pn\n",
      "i=11fy(i)= 1g\n",
      "n:\n",
      "If we were to apply Laplace smoothing (which is needed in practice for good\n",
      "performance) when estimating \u001e",
      "kjy=0and\u001e",
      "kjy=1, we add 1 to the numerators\n",
      "andjVjto the denominators, and obtain:\n",
      "\u001e",
      "kjy=1=1 +Pn\n",
      "i=1Pdi\n",
      "j=11fx(i)\n",
      "j=k^y(i)= 1g\n",
      "jVj+Pn\n",
      "i=11fy(i)= 1gdi\n",
      "\u001e",
      "kjy=0=1 +Pn\n",
      "i=1Pdi\n",
      "j=11fx(i)\n",
      "j=k^y(i)= 0g\n",
      "jVj+Pn\n",
      "i=11fy(i)= 0gdi:\n",
      "While not necessarily the very best classi\f",
      "cation algorithm, the Naive Bayes\n",
      "classi\f",
      "er often works surprisingly well. It is often also a very good \\\f",
      "rst thing\n",
      "to try,\" given its simplicity and ease of implementation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 5\n",
      "Kernel methods\n",
      "5.1 Feature maps\n",
      "Recall that in our discussion about linear regression, we considered the prob-\n",
      "lem of predicting the price of a house (denoted by y) from the living area of\n",
      "the house (denoted by x), and we \f",
      "t a linear function of xto the training\n",
      "data. What if the price ycan be more accurately represented as a non-linear\n",
      "function of x? In this case, we need a more expressive family of models than\n",
      "linear models.\n",
      "We start by considering \f",
      "tting cubic functions y=\u00123x3+\u00122x2+\u00121x+\u00120.\n",
      "It turns out that we can view the cubic function as a linear function over\n",
      "the a di\u000b",
      "erent set of feature variables (de\f",
      "ned below). Concretely, let the\n",
      "function\u001e",
      ":R!R4be de\f",
      "ned as\n",
      "\u001e",
      "(x) =2\n",
      "6641\n",
      "x\n",
      "x2\n",
      "x33\n",
      "7752R4: (5.1)\n",
      "Let\u00122R4be the vector containing \u00120;\u00121;\u00122;\u00123as entries. Then we can\n",
      "rewrite the cubic function in xas:\n",
      "\u00123x3+\u00122x2+\u00121x+\u00120=\u0012T\u001e",
      "(x)\n",
      "Thus, a cubic function of the variable xcan be viewed as a linear function\n",
      "over the variables \u001e",
      "(x). To distinguish between these two sets of variables,\n",
      "in the context of kernel methods, we will call the \\original\" input value the\n",
      "input attributes of a problem (in this case, x, the living area). When the\n",
      "48\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "49\n",
      "original input is mapped to some new set of quantities \u001e",
      "(x), we will call those\n",
      "new quantities the features variables. (Unfortunately, di\u000b",
      "erent authors use\n",
      "di\u000b",
      "erent terms to describe these two things in di\u000b",
      "erent contexts.) We will\n",
      "call\u001e",
      "afeature map , which maps the attributes to the features.\n",
      "5.2 LMS (least mean squares) with features\n",
      "We will derive the gradient descent algorithm for \f",
      "tting the model \u0012T\u001e",
      "(x).\n",
      "First recall that for ordinary least square problem where we were to \f",
      "t \u0012Tx,\n",
      "the batch gradient descent update is (see the \f",
      "rst lecture note for its deriva-\n",
      "tion):\n",
      "\u0012:=\u0012+\u000b",
      "nX\n",
      "i=1\u0000\n",
      "y(i)\u0000h\u0012(x(i))\u0001\n",
      "x(i)\n",
      ":=\u0012+\u000b",
      "nX\n",
      "i=1\u0000\n",
      "y(i)\u0000\u0012Tx(i)\u0001\n",
      "x(i): (5.2)\n",
      "Let\u001e",
      ":Rd!Rpbe a feature map that maps attribute x(inRd) to the\n",
      "features\u001e",
      "(x) inRp. (In the motivating example in the previous subsection,\n",
      "we haved= 1 andp= 4.) Now our goal is to \f",
      "t the function \u0012T\u001e",
      "(x), with\n",
      "\u0012being a vector in Rpinstead of Rd. We can replace all the occurrences of\n",
      "x(i)in the algorithm above by \u001e",
      "(x(i)) to obtain the new update:\n",
      "\u0012:=\u0012+\u000b",
      "nX\n",
      "i=1\u0000\n",
      "y(i)\u0000\u0012T\u001e",
      "(x(i))\u0001\n",
      "\u001e",
      "(x(i)) (5.3)\n",
      "Similarly, the corresponding stochastic gradient descent update rule is\n",
      "\u0012:=\u0012+\u000b",
      "\u0000\n",
      "y(i)\u0000\u0012T\u001e",
      "(x(i))\u0001\n",
      "\u001e",
      "(x(i)) (5.4)\n",
      "5.3 LMS with the kernel trick\n",
      "The gradient descent update, or stochastic gradient update above becomes\n",
      "computationally expensive when the features \u001e",
      "(x) is high-dimensional. For\n",
      "example, consider the direct extension of the feature map in equation (5.1)\n",
      "to high-dimensional input x: supposex2Rd, and let\u001e",
      "(x) be the vector that\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "50\n",
      "contains all the monomials of xwith degree\u00143\n",
      "\u001e",
      "(x) =2\n",
      "6666666666666666666666641\n",
      "x1\n",
      "x2\n",
      "...\n",
      "x2\n",
      "1\n",
      "x1x2\n",
      "x1x3\n",
      "...\n",
      "x2x1\n",
      "...\n",
      "x3\n",
      "1\n",
      "x2\n",
      "1x2\n",
      "...3\n",
      "777777777777777777777775: (5.5)\n",
      "The dimension of the features \u001e",
      "(x) is on the order of d3.1This is a pro-\n",
      "hibitively long vector for computational purpose | when d= 1000, each\n",
      "update requires at least computing and storing a 10003= 109dimensional\n",
      "vector, which is 106times slower than the update rule for for ordinary least\n",
      "squares updates (5.2).\n",
      "It may appear at \f",
      "rst that such d3runtime per update and memory usage\n",
      "are inevitable, because the vector \u0012itself is of dimension p\u0019d3, and we may\n",
      "need to update every entry of \u0012and store it. However, we will introduce the\n",
      "kernel trick with which we will not need to store \u0012explicitly, and the runtime\n",
      "can be signi\f",
      "cantly improved.\n",
      "For simplicity, we assume the initialize the value \u0012= 0, and we focus\n",
      "on the iterative update (5.3). The main observation is that at any time, \u0012\n",
      "can be represented as a linear combination of the vectors \u001e",
      "(x(1));:::;\u001e",
      " (x(n)).\n",
      "Indeed, we can show this inductively as follows. At initialization, \u0012= 0 =Pn\n",
      "i=10\u0001\u001e",
      "(x(i)). Assume at some point, \u0012can be represented as\n",
      "\u0012=nX\n",
      "i=1\f",
      "i\u001e",
      "(x(i)) (5.6)\n",
      "1Here, for simplicity, we include all the monomials with repetitions (so that, e.g., x1x2x3\n",
      "andx2x3x1both appear in \u001e",
      "(x)). Therefore, there are totally 1 + d+d2+d3entries in\n",
      "\u001e",
      "(x).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "51\n",
      "for some\f",
      "1;:::;\f",
      "n2R. Then we claim that in the next round, \u0012is still a\n",
      "linear combination of \u001e",
      "(x(1));:::;\u001e",
      " (x(n)) because\n",
      "\u0012:=\u0012+\u000b",
      "nX\n",
      "i=1\u0000\n",
      "y(i)\u0000\u0012T\u001e",
      "(x(i))\u0001\n",
      "\u001e",
      "(x(i))\n",
      "=nX\n",
      "i=1\f",
      "i\u001e",
      "(x(i)) +\u000b",
      "nX\n",
      "i=1\u0000\n",
      "y(i)\u0000\u0012T\u001e",
      "(x(i))\u0001\n",
      "\u001e",
      "(x(i))\n",
      "=nX\n",
      "i=1(\f",
      "i+\u000b",
      "\u0000\n",
      "y(i)\u0000\u0012T\u001e",
      "(x(i))\u0001\n",
      ")|{z}\n",
      "new\f",
      "i\u001e",
      "(x(i)) (5.7)\n",
      "You may realize that our general strategy is to implicitly represent the p-\n",
      "dimensional vector \u0012by a set of coe\u000ecients \f",
      "1;:::;\f",
      "n. Towards doing this,\n",
      "we derive the update rule of the coe\u000ecients \f",
      "1;:::;\f",
      "n. Using the equation\n",
      "above, we see that the new \f",
      "idepends on the old one via\n",
      "\f",
      "i:=\f",
      "i+\u000b",
      "\u0000\n",
      "y(i)\u0000\u0012T\u001e",
      "(x(i))\u0001\n",
      "(5.8)\n",
      "Here we still have the old \u0012on the RHS of the equation. Replacing \u0012by\n",
      "\u0012=Pn\n",
      "j=1\f",
      "j\u001e",
      "(x(j)) gives\n",
      "8i2f1;:::;ng;\f",
      "i:=\f",
      "i+\u000b",
      " \n",
      "y(i)\u0000nX\n",
      "j=1\f",
      "j\u001e",
      "(x(j))T\u001e",
      "(x(i))!\n",
      "We often rewrite \u001e",
      "(x(j))T\u001e",
      "(x(i)) ash\u001e",
      "(x(j));\u001e",
      "(x(i))ito emphasize that it's the\n",
      "inner product of the two feature vectors. Viewing \f",
      "i's as the new representa-\n",
      "tion of\u0012, we have successfully translated the batch gradient descent algorithm\n",
      "into an algorithm that updates the value of \f",
      "iteratively. It may appear that\n",
      "at every iteration, we still need to compute the values of h\u001e",
      "(x(j));\u001e",
      "(x(i))ifor\n",
      "all pairs of i;j, each of which may take roughly O(p) operation. However,\n",
      "two important properties come to rescue:\n",
      "1. We can pre-compute the pairwise inner products h\u001e",
      "(x(j));\u001e",
      "(x(i))ifor all\n",
      "pairs ofi;jbefore the loop starts.\n",
      "2. For the feature map \u001e",
      "de\f",
      "ned in (5.5) (or many other interesting fea-\n",
      "ture maps), computing h\u001e",
      "(x(j));\u001e",
      "(x(i))ican be e\u000ecient and does not\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "52\n",
      "necessarily require computing \u001e",
      "(x(i)) explicitly. This is because:\n",
      "h\u001e",
      "(x);\u001e",
      "(z)i= 1 +dX\n",
      "i=1xizi+X\n",
      "i;j2f1;:::;dgxixjzizj+X\n",
      "i;j;k2f1;:::;dgxixjxkzizjzk\n",
      "= 1 +dX\n",
      "i=1xizi+ dX\n",
      "i=1xizi!2\n",
      "+ dX\n",
      "i=1xizi!3\n",
      "= 1 +hx;zi+hx;zi2+hx;zi3(5.9)\n",
      "Therefore, to compute h\u001e",
      "(x);\u001e",
      "(z)i, we can \f",
      "rst compute hx;ziwith\n",
      "O(d) time and then take another constant number of operations to com-\n",
      "pute 1 +hx;zi+hx;zi2+hx;zi3.\n",
      "As you will see, the inner products between the features h\u001e",
      "(x);\u001e",
      "(z)iare\n",
      "essential here. We de\f",
      "ne the Kernel corresponding to the feature map \u001e",
      "as\n",
      "a function that maps X\u0002X! Rsatisfying:2\n",
      "K(x;z),h\u001e",
      "(x);\u001e",
      "(z)i (5.10)\n",
      "To wrap up the discussion, we write the down the \f",
      "nal algorithm as\n",
      "follows:\n",
      "1. Compute all the values K(x(i);x(j)),h\u001e",
      "(x(i));\u001e",
      "(x(j))iusing equa-\n",
      "tion (5.9) for all i;j2f1;:::;ng. Set\f",
      ":= 0.\n",
      "2.Loop:\n",
      "8i2f1;:::;ng;\f",
      "i:=\f",
      "i+\u000b",
      " \n",
      "y(i)\u0000nX\n",
      "j=1\f",
      "jK(x(i);x(j))!\n",
      "(5.11)\n",
      "Or in vector notation, letting Kbe then\u0002nmatrix with Kij=\n",
      "K(x(i);x(j)), we have\n",
      "\f",
      ":=\f",
      "+\u000b",
      "(~ y\u0000K\f",
      ")\n",
      "With the algorithm above, we can update the representation \f",
      "of the\n",
      "vector\u0012e\u000eciently with O(n) time per update. Finally, we need to show that\n",
      "2Recall thatXis the space of the input x. In our running example, X=Rd\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "53\n",
      "the knowledge of the representation \f",
      "su\u000eces to compute the prediction\n",
      "\u0012T\u001e",
      "(x). Indeed, we have\n",
      "\u0012T\u001e",
      "(x) =nX\n",
      "i=1\f",
      "i\u001e",
      "(x(i))T\u001e",
      "(x) =nX\n",
      "i=1\f",
      "iK(x(i);x) (5.12)\n",
      "You may realize that fundamentally all we need to know about the feature\n",
      "map\u001e",
      "(\u0001) is encapsulated in the corresponding kernel function K(\u0001;\u0001). We\n",
      "will expand on this in the next section.\n",
      "5.4 Properties of kernels\n",
      "In the last subsection, we started with an explicitly de\f",
      "ned feature map \u001e",
      ",\n",
      "which induces the kernel function K(x;z),h\u001e",
      "(x);\u001e",
      "(z)i. Then we saw that\n",
      "the kernel function is so intrinsic so that as long as the kernel function is\n",
      "de\f",
      "ned, the whole training algorithm can be written entirely in the language\n",
      "of the kernel without referring to the feature map \u001e",
      ", so can the prediction of\n",
      "a test example x(equation (5.12).)\n",
      "Therefore, it would be tempted to de\f",
      "ne other kernel function K(\u0001;\u0001) and\n",
      "run the algorithm (5.11). Note that the algorithm (5.11) does not need to\n",
      "explicitly access the feature map \u001e",
      ", and therefore we only need to ensure the\n",
      "existence of the feature map \u001e",
      ", but do not necessarily need to be able to\n",
      "explicitly write \u001e",
      "down.\n",
      "What kinds of functions K(\u0001;\u0001) can correspond to some feature map \u001e",
      "? In\n",
      "other words, can we tell if there is some feature mapping \u001e",
      "so thatK(x;z) =\n",
      "\u001e",
      "(x)T\u001e",
      "(z) for allx,z?\n",
      "If we can answer this question by giving a precise characterization of valid\n",
      "kernel functions, then we can completely change the interface of selecting\n",
      "feature maps \u001e",
      "to the interface of selecting kernel function K. Concretely,\n",
      "we can pick a function K, verify that it satis\f",
      "es the characterization (so\n",
      "that there exists a feature map \u001e",
      "thatKcorresponds to), and then we can\n",
      "run update rule (5.11). The bene\f",
      "t here is that we don't have to be able\n",
      "to compute \u001e",
      "or write it down analytically, and we only need to know its\n",
      "existence. We will answer this question at the end of this subsection after\n",
      "we go through several concrete examples of kernels.\n",
      "Supposex;z2Rd, and let's \f",
      "rst consider the function K(\u0001;\u0001) de\f",
      "ned as:\n",
      "K(x;z) = (xTz)2:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "54\n",
      "We can also write this as\n",
      "K(x;z) = dX\n",
      "i=1xizi! dX\n",
      "j=1xjzj!\n",
      "=dX\n",
      "i=1dX\n",
      "j=1xixjzizj\n",
      "=dX\n",
      "i;j=1(xixj)(zizj)\n",
      "Thus, we see that K(x;z) =h\u001e",
      "(x);\u001e",
      "(z)iis the kernel function that corre-\n",
      "sponds to the the feature mapping \u001e",
      "given (shown here for the case of d= 3)\n",
      "by\n",
      "\u001e",
      "(x) =2\n",
      "6666666666664x1x1\n",
      "x1x2\n",
      "x1x3\n",
      "x2x1\n",
      "x2x2\n",
      "x2x3\n",
      "x3x1\n",
      "x3x2\n",
      "x3x33\n",
      "7777777777775:\n",
      "Revisiting the computational e\u000eciency perspective of kernel, note that whereas\n",
      "calculating the high-dimensional \u001e",
      "(x) requiresO(d2) time, \f",
      "nding K(x;z)\n",
      "takes onlyO(d) time|linear in the dimension of the input attributes.\n",
      "For another related example, also consider K(\u0001;\u0001) de\f",
      "ned by\n",
      "K(x;z) = (xTz+c)2\n",
      "=dX\n",
      "i;j=1(xixj)(zizj) +dX\n",
      "i=1(p\n",
      "2cxi)(p\n",
      "2czi) +c2:\n",
      "(Check this yourself.) This function Kis a kernel function that corresponds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "55\n",
      "to the feature mapping (again shown for d= 3)\n",
      "\u001e",
      "(x) =2\n",
      "666666666666666666664x1x1\n",
      "x1x2\n",
      "x1x3\n",
      "x2x1\n",
      "x2x2\n",
      "x2x3\n",
      "x3x1\n",
      "x3x2\n",
      "x3x3p\n",
      "2cx1p\n",
      "2cx2p\n",
      "2cx3\n",
      "c3\n",
      "777777777777777777775;\n",
      "and the parameter ccontrols the relative weighting between the xi(\f",
      "rst\n",
      "order) and the xixj(second order) terms.\n",
      "More broadly, the kernel K(x;z) = (xTz+c)kcorresponds to a feature\n",
      "mapping to an\u0000d+k\n",
      "k\u0001\n",
      "feature space, corresponding of all monomials of the\n",
      "formxi1xi2:::xikthat are up to order k. However, despite working in this\n",
      "O(dk)-dimensional space, computing K(x;z) still takes only O(d) time, and\n",
      "hence we never need to explicitly represent feature vectors in this very high\n",
      "dimensional feature space.\n",
      "Kernels as similarity metrics. Now, let's talk about a slightly di\u000b",
      "erent\n",
      "view of kernels. Intuitively, (and there are things wrong with this intuition,\n",
      "but nevermind), if \u001e",
      "(x) and\u001e",
      "(z) are close together, then we might expect\n",
      "K(x;z) =\u001e",
      "(x)T\u001e",
      "(z) to be large. Conversely, if \u001e",
      "(x) and\u001e",
      "(z) are far apart|\n",
      "say nearly orthogonal to each other|then K(x;z) =\u001e",
      "(x)T\u001e",
      "(z) will be small.\n",
      "So, we can think of K(x;z) as some measurement of how similar are \u001e",
      "(x)\n",
      "and\u001e",
      "(z), or of how similar are xandz.\n",
      "Given this intuition, suppose that for some learning problem that you're\n",
      "working on, you've come up with some function K(x;z) that you think might\n",
      "be a reasonable measure of how similar xandzare. For instance, perhaps\n",
      "you chose\n",
      "K(x;z) = exp\u0012\n",
      "\u0000jjx\u0000zjj2\n",
      "2\u001b2\u0013\n",
      ":\n",
      "This is a reasonable measure of xandz's similarity, and is close to 1 when\n",
      "xandzare close, and near 0 when xandzare far apart. Does there exist\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "56\n",
      "a feature map \u001e",
      "such that the kernel Kde\f",
      "ned above satis\f",
      "es K(x;z) =\n",
      "\u001e",
      "(x)T\u001e",
      "(z)? In this particular example, the answer is yes. This kernel is called\n",
      "theGaussian kernel , and corresponds to an in\f",
      "nite dimensional feature\n",
      "mapping\u001e",
      ". We will give a precise characterization about what properties\n",
      "a functionKneeds to satisfy so that it can be a valid kernel function that\n",
      "corresponds to some feature map \u001e",
      ".\n",
      "Necessary conditions for valid kernels. Suppose for now that Kis\n",
      "indeed a valid kernel corresponding to some feature mapping \u001e",
      ", and we will\n",
      "\f",
      "rst see what properties it satis\f",
      "es. Now, consider some \f",
      "nite set of npoints\n",
      "(not necessarily the training set) fx(1);:::;x(n)g, and let a square, n-by-n\n",
      "matrixKbe de\f",
      "ned so that its ( i;j)-entry is given by Kij=K(x(i);x(j)).\n",
      "This matrix is called the kernel matrix . Note that we've overloaded the\n",
      "notation and used Kto denote both the kernel function K(x;z) and the\n",
      "kernel matrix K, due to their obvious close relationship.\n",
      "Now, ifKis a valid kernel, then Kij=K(x(i);x(j)) =\u001e",
      "(x(i))T\u001e",
      "(x(j)) =\n",
      "\u001e",
      "(x(j))T\u001e",
      "(x(i)) =K(x(j);x(i)) =Kji, and hence Kmust be symmetric. More-\n",
      "over, letting \u001e",
      "k(x) denote the k-th coordinate of the vector \u001e",
      "(x), we \f",
      "nd that\n",
      "for any vector z, we have\n",
      "zTKz =X\n",
      "iX\n",
      "jziKijzj\n",
      "=X\n",
      "iX\n",
      "jzi\u001e",
      "(x(i))T\u001e",
      "(x(j))zj\n",
      "=X\n",
      "iX\n",
      "jziX\n",
      "k\u001e",
      "k(x(i))\u001e",
      "k(x(j))zj\n",
      "=X\n",
      "kX\n",
      "iX\n",
      "jzi\u001e",
      "k(x(i))\u001e",
      "k(x(j))zj\n",
      "=X\n",
      "k X\n",
      "izi\u001e",
      "k(x(i))!2\n",
      "\u00150:\n",
      "The second-to-last step uses the fact thatP\n",
      "i;jaiaj= (P\n",
      "iai)2forai=\n",
      "zi\u001e",
      "k(x(i)). Sincezwas arbitrary, this shows that Kis positive semi-de\f",
      "nite\n",
      "(K\u00150).\n",
      "Hence, we've shown that if Kis a valid kernel (i.e., if it corresponds to\n",
      "some feature mapping \u001e",
      "), then the corresponding kernel matrix K2Rn\u0002n\n",
      "is symmetric positive semide\f",
      "nite.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "57\n",
      "Su\u000ecient conditions for valid kernels. More generally, the condition\n",
      "above turns out to be not only a necessary, but also a su\u000ecient, condition\n",
      "forKto be a valid kernel (also called a Mercer kernel). The following result\n",
      "is due to Mercer.3\n",
      "Theorem (Mercer). LetK:Rd\u0002Rd7!Rbe given. Then for K\n",
      "to be a valid (Mercer) kernel, it is necessary and su\u000ecient that for any\n",
      "fx(1);:::;x(n)g, (n<1), the corresponding kernel matrix is symmetric pos-\n",
      "itive semi-de\f",
      "nite.\n",
      "Given a function K, apart from trying to \f",
      "nd a feature mapping \u001e",
      "that\n",
      "corresponds to it, this theorem therefore gives another way of testing if it is\n",
      "a valid kernel. You'll also have a chance to play with these ideas more in\n",
      "problem set 2.\n",
      "y talked about a couple of other examples of ker-\n",
      "nels. For instance, consider the digit recognition problem, in which given\n",
      "an image (16x16 pixels) of a handwritten digit (0-9), we have to \f",
      "gure out\n",
      "which digit it was. Using either a simple polynomial kernel K(x;z) = (xTz)k\n",
      "or the Gaussian kernel, SVMs were able to obtain extremely good perfor-\n",
      "mance on this problem. This was particularly surprising since the input\n",
      "attributesxwere just 256-dimensional vectors of the image pixel intensity\n",
      "values, and the system had no prior knowledge about vision, or even about\n",
      "which pixels are adjacent to which other ones. Another example that we\n",
      "y talked about in lecture was that if the objects xthat we are trying\n",
      "to classify are strings (say, xis a list of amino acids, which strung together\n",
      "form a protein), then it seems hard to construct a reasonable, \\small\" set of\n",
      "features for most learning algorithms, especially if di\u000b",
      "erent strings have dif-\n",
      "ferent lengths. However, consider letting \u001e",
      "(x) be a feature vector that counts\n",
      "the number of occurrences of each length- ksubstring in x. If we're consid-\n",
      "ering strings of English letters, then there are 26ksuch strings. Hence, \u001e",
      "(x)\n",
      "is a 26kdimensional vector; even for moderate values of k, this is probably\n",
      "too big for us to e\u000eciently work with. (e.g., 264\u0019460000.) However, using\n",
      "(dynamic programming-ish) string matching algorithms, it is possible to ef-\n",
      "\f",
      "ciently compute K(x;z) =\u001e",
      "(x)T\u001e",
      "(z), so that we can now implicitly work\n",
      "in this 26k-dimensional feature space, but without ever explicitly computing\n",
      "feature vectors in this space.\n",
      "3Many texts present Mercer's theorem in a slightly more complicated form involving\n",
      "L2functions, but when the input attributes take values in Rd, the version given here is\n",
      "equivalent.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "58\n",
      "Application of kernel methods: We've seen the application of kernels\n",
      "to linear regression. In the next part, we will introduce the support vector\n",
      "machines to which kernels can be directly applied. dwell too much longer on\n",
      "it here. In fact, the idea of kernels has signi\f",
      "cantly broader applicability than\n",
      "linear regression and SVMs. Speci\f",
      "cally, if you have any learning algorithm\n",
      "that you can write in terms of only inner products hx;zibetween input\n",
      "attribute vectors, then by replacing this with K(x;z) whereKis a kernel,\n",
      "you can \\magically\" allow your algorithm to work e\u000eciently in the high\n",
      "dimensional feature space corresponding to K. For instance, this kernel trick\n",
      "can be applied with the perceptron to derive a kernel perceptron algorithm.\n",
      "Many of the algorithms that we'll see later in this class will also be amenable\n",
      "to this method, which has come to be known as the \\kernel trick.\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 6\n",
      "Support vector machines\n",
      "This set of notes presents the Support Vector Machine (SVM) learning al-\n",
      "gorithm. SVMs are among the best (and many believe are indeed the best)\n",
      "\\o\u000b",
      "-the-shelf\" supervised learning algorithms. To tell the SVM story, we'll\n",
      "need to \f",
      "rst talk about margins and the idea of separating data with a large\n",
      "\\gap.\" Next, we'll talk about the optimal margin classi\f",
      "er, which will lead\n",
      "us into a digression on Lagrange duality. We'll also see kernels, which give\n",
      "a way to apply SVMs e\u000eciently in very high dimensional (such as in\f",
      "nite-\n",
      "dimensional) feature spaces, and \f",
      "nally, we'll close o\u000b",
      " the story with the\n",
      "SMO algorithm, which gives an e\u000ecient implementation of SVMs.\n",
      "6.1 Margins: intuition\n",
      "We'll start our story on SVMs by talking about margins. This section will\n",
      "give the intuitions about margins and about the \\con\f",
      "dence\" of our predic-\n",
      "tions; these ideas will be made formal in Section 6.3.\n",
      "Consider logistic regression, where the probability p(y= 1jx;\u0012) is mod-\n",
      "eled byh\u0012(x) =g(\u0012Tx). We then predict \\1\" on an input xif and only if\n",
      "h\u0012(x)\u00150:5, or equivalently, if and only if \u0012Tx\u00150. Consider a positive\n",
      "training example ( y= 1). The larger \u0012Txis, the larger also is h\u0012(x) =p(y=\n",
      "1jx;\u0012), and thus also the higher our degree of \\con\f",
      "dence\" that the label is 1.\n",
      "Thus, informally we can think of our prediction as being very con\f",
      "dent that\n",
      "y= 1 if\u0012Tx\u001d",
      "0. Similarly, we think of logistic regression as con\f",
      "dently\n",
      "predictingy= 0, if\u0012Tx\u001c",
      "0. Given a training set, again informally it seems\n",
      "that we'd have found a good \f",
      "t to the training data if we can \f",
      "nd \u0012so that\n",
      "\u0012Tx(i)\u001d",
      "0 whenever y(i)= 1, and\u0012Tx(i)\u001c",
      "0 whenever y(i)= 0, since this\n",
      "ect a very con\f",
      "dent (and correct) set of classi\f",
      "cations for all the\n",
      "59\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "60\n",
      "training examples. This seems to be a nice goal to aim for, and we'll soon\n",
      "formalize this idea using the notion of functional margins.\n",
      "For a di\u000b",
      "erent type of intuition, consider the following \f",
      "gure, in which x's\n",
      "represent positive training examples, o's denote negative training examples,\n",
      "a decision boundary (this is the line given by the equation \u0012Tx= 0, and\n",
      "is also called the separating hyperplane ) is also shown, and three points\n",
      "have also been labeled A, B and C.\n",
      "/0 /1\n",
      "/0 /1\n",
      "/0 /1BA\n",
      "C\n",
      "Notice that the point A is very far from the decision boundary. If we are\n",
      "asked to make a prediction for the value of yat A, it seems we should be\n",
      "quite con\f",
      "dent that y= 1 there. Conversely, the point C is very close to\n",
      "the decision boundary, and while it's on the side of the decision boundary\n",
      "on which we would predict y= 1, it seems likely that just a small change to\n",
      "the decision boundary could easily have caused out prediction to be y= 0.\n",
      "Hence, we're much more con\f",
      "dent about our prediction at A than at C. The\n",
      "point B lies in-between these two cases, and more broadly, we see that if\n",
      "a point is far from the separating hyperplane, then we may be signi\f",
      "cantly\n",
      "more con\f",
      "dent in our predictions. Again, informally we think it would be\n",
      "nice if, given a training set, we manage to \f",
      "nd a decision boundary that\n",
      "allows us to make all correct and con\f",
      "dent (meaning far from the decision\n",
      "boundary) predictions on the training examples. We'll formalize this later\n",
      "using the notion of geometric margins.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "61\n",
      "6.2 Notation (option reading)\n",
      "To make our discussion of SVMs easier, we'll \f",
      "rst need to introduce a new\n",
      "notation for talking about classi\f",
      "cation. We will be considering a linear\n",
      "classi\f",
      "er for a binary classi\f",
      "cation problem with labels yand features x.\n",
      "From now, we'll use y2f\u0000 1;1g(instead off0;1g) to denote the class labels.\n",
      "Also, rather than parameterizing our linear classi\f",
      "er with the vector \u0012, we\n",
      "will use parameters w;b, and write our classi\f",
      "er as\n",
      "hw;b(x) =g(wTx+b):\n",
      "Here,g(z) = 1 ifz\u00150, andg(z) =\u00001 otherwise. This \\ w;b\" notation\n",
      "allows us to explicitly treat the intercept term bseparately from the other\n",
      "parameters. (We also drop the convention we had previously of letting x0= 1\n",
      "be an extra coordinate in the input feature vector.) Thus, btakes the role of\n",
      "what was previously \u00120, andwtakes the role of [ \u00121:::\u0012d]T.\n",
      "Note also that, from our de\f",
      "nition of gabove, our classi\f",
      "er will directly\n",
      "predict either 1 or \u00001 (cf. the perceptron algorithm), without \f",
      "rst going\n",
      "through the intermediate step of estimating p(y= 1) (which is what logistic\n",
      "regression does).\n",
      "6.3 Functional and geometric margins (op-\n",
      "tion reading)\n",
      "Let's formalize the notions of the functional and geometric margins. Given a\n",
      "training example ( x(i);y(i)), we de\f",
      "ne the functional margin of (w;b) with\n",
      "respect to the training example as\n",
      "(i)=y(i)(wTx(i)+b):\n",
      "Note that if y(i)= 1, then for the functional margin to be large (i.e., for\n",
      "our prediction to be con\f",
      "dent and correct), we need wTx(i)+bto be a large\n",
      "positive number. Conversely, if y(i)=\u00001, then for the functional margin\n",
      "to be large, we need wTx(i)+bto be a large negative number. Moreover, if\n",
      "y(i)(wTx(i)+b)>0, then our prediction on this example is correct. (Check\n",
      "this yourself.) Hence, a large functional margin represents a con\f",
      "dent and a\n",
      "correct prediction.\n",
      "For a linear classi\f",
      "er with the choice of ggiven above (taking values in\n",
      "f\u00001;1g), there's one property of the functional margin that makes it not a\n",
      "very good measure of con\f",
      "dence, however. Given our choice of g, we note that\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "62\n",
      "if we replace wwith 2wandbwith 2b, then since g(wTx+b) =g(2wTx+2b),\n",
      "this would not change hw;b(x) at all. I.e., g, and hence also hw;b(x), depends\n",
      "only on the sign, but not on the magnitude, of wTx+b. However, replacing\n",
      "(w;b) with (2w;2b) also results in multiplying our functional margin by a\n",
      "factor of 2. Thus, it seems that by exploiting our freedom to scale wandb,\n",
      "we can make the functional margin arbitrarily large without really changing\n",
      "anything meaningful. Intuitively, it might therefore make sense to impose\n",
      "some sort of normalization condition such as that jjwjj2= 1; i.e., we might\n",
      "replace (w;b) with (w=jjwjj2;b=jjwjj2), and instead consider the functional\n",
      "margin of ( w=jjwjj2;b=jjwjj2). We'll come back to this later.\n",
      "Given a training set S=f(x(i);y(i));i= 1;:::;ng, we also de\f",
      "ne the\n",
      "function margin of ( w;b) with respect to Sas the smallest of the functional\n",
      ", this can thereforeidual training examples. Denoted by ^ \n",
      "be written:\n",
      "= min\n",
      "(i)::::;n^\n",
      "Next, let's talk about geometric margins . Consider the picture below:\n",
      "w A\n",
      "γ\n",
      "B(i)\n",
      "The decision boundary corresponding to ( w;b) is shown, along with the\n",
      "vectorw. Note that wis orthogonal (at 90\u000e) to the separating hyperplane.\n",
      "(You should convince yourself that this must be the case.) Consider the\n",
      "point at A, which represents the input x(i)of some training example with\n",
      "(i), is given by the linee to the decision boundary, \n",
      "segment AB.\n",
      "(i)? Well,w=jjwjjis a unit-length vector\n",
      "pointing in the same direction as w. SinceArepresentsx(i), we therefore\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "63\n",
      "(i)\u0001w=jjwjj. But this point lies on)\u0000\n",
      "the decision boundary, and all points xon the decision boundary satisfy the\n",
      "equationwTx+b= 0. Hence,\n",
      "wT\u0012\n",
      "(i)w\u0000\n",
      "jjwjj\u0013\n",
      "+b= 0:\n",
      "(i)yieldsor \n",
      "(i)=wTx(i)+b\n",
      "jjwjj=\u0012w\n",
      "jjwjj\u0013T\n",
      "x(i)+b\n",
      "jjwjj:\n",
      "This was worked out for the case of a positive training example at A in the\n",
      "\f",
      "gure, where being on the \\positive\" side of the decision boundary is good.\n",
      "More generally, we de\f",
      "ne the geometric margin of ( w;b) with respect to a\n",
      "training example ( x(i);y(i)) to be\n",
      "(i)=y(i) \u0012w\n",
      "jjwjj\u0013T\n",
      "x(i)+b\n",
      "jjwjj!\n",
      ":\n",
      "Note that ifjjwjj= 1, then the functional margin equals the geometric\n",
      "margin|this thus gives us a way of relating these two di\u000b",
      "erent notions of\n",
      "margin. Also, the geometric margin is invariant to rescaling of the parame-\n",
      "ters; i.e., if we replace wwith 2wandbwith 2b, then the geometric margin\n",
      "does not change. This will in fact come in handy later. Speci\f",
      "cally, because\n",
      "of this invariance to the scaling of the parameters, when trying to \f",
      "t wandb\n",
      "to training data, we can impose an arbitrary scaling constraint on wwithout\n",
      "changing anything important; for instance, we can demand that jjwjj= 1, or\n",
      "jw1j= 5, orjw1+bj+jw2j= 2, and any of these can be satis\f",
      "ed simply by\n",
      "rescalingwandb.\n",
      "Finally, given a training set S=f(x(i);y(i));i= 1;:::;ng, we also de\f",
      "ne\n",
      "the geometric margin of ( w;b) with respect to Sto be the smallest of the\n",
      "geometric margins on the individual training examples:\n",
      "= min\n",
      "(i)::::;n\n",
      "6.4 The optimal margin classi\f",
      "er (option read-\n",
      "ing)\n",
      "Given a training set, it seems from our previous discussion that a natural\n",
      "desideratum is to try to \f",
      "nd a decision boundary that maximizes the (ge-\n",
      "ect a very con\f",
      "dent set of predictions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "64\n",
      "on the training set and a good \\\f",
      "t\" to the training data. Speci\f",
      "cally, this\n",
      "will result in a classi\f",
      "er that separates the positive and the negative training\n",
      "examples with a \\gap\" (geometric margin).\n",
      "For now, we will assume that we are given a training set that is linearly\n",
      "separable; i.e., that it is possible to separate the positive and negative ex-\n",
      "amples using some separating hyperplane. How will we \f",
      "nd the one that\n",
      "achieves the maximum geometric margin? We can pose the following opti-\n",
      "mization problem:\n",
      ";w;b\n",
      "; i = 1;:::;ni)+b)\u0015\n",
      "jjwjj= 1:\n",
      ", subject to each training example having func-\n",
      ". Thejjwjj= 1 constraint moreover ensures that the\n",
      "functional margin equals to the geometric margin, so we are also guaranteed\n",
      ". Thus, solving this problem wille at least \n",
      "result in (w;b) with the largest possible geometric margin with respect to the\n",
      "training set.\n",
      "If we could solve the optimization problem above, we'd be done. But the\n",
      "\\jjwjj= 1\" constraint is a nasty (non-convex) one, and this problem certainly\n",
      "isn't in any format that we can plug into standard optimization software to\n",
      "solve. So, let's try transforming the problem into a nicer one. Consider:\n",
      ";w;b^\n",
      "jjwjj\n",
      "; i = 1;:::;ni)+b)\u0015^\n",
      "=jjwjj, subject to the functional margins all\n",
      ". Since the geometric and functional margins are related by\n",
      "=jjwj, this will give us the answer we want. Moreover, we've gotten rid\n",
      "of the constraintjjwjj= 1 that we didn't like. The downside is that we now\n",
      "have a nasty (again, non-convex) objective^\n",
      "jjwjjfunction; and, we still don't\n",
      "have any o\u000b",
      "-the-shelf software that can solve this form of an optimization\n",
      "problem.\n",
      "Let's keep going. Recall our earlier discussion that we can add an arbi-\n",
      "trary scaling constraint on wandbwithout changing anything. This is the\n",
      "key idea we'll use now. We will introduce the scaling constraint that the\n",
      "functional margin of w;bwith respect to the training set must be 1:\n",
      "= 1:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "65\n",
      "Since multiplying wandbby some constant results in the functional margin\n",
      "being multiplied by that same constant, this is indeed a scaling constraint,\n",
      "and can be satis\f",
      "ed by rescaling w;b. Plugging this into our problem above,\n",
      "=jjwjj= 1=jjwjjis the same thing as minimizing\n",
      "jjwjj2, we now have the following optimization problem:\n",
      "minw;b1\n",
      "2jjwjj2\n",
      "s.t.y(i)(wTx(i)+b)\u00151; i= 1;:::;n\n",
      "We've now transformed the problem into a form that can be e\u000eciently\n",
      "solved. The above is an optimization problem with a convex quadratic ob-\n",
      "jective and only linear constraints. Its solution gives us the optimal mar-\n",
      "gin classi\f",
      "er . This optimization problem can be solved using commercial\n",
      "quadratic programming (QP) code.1\n",
      "While we could call the problem solved here, what we will instead do is\n",
      "make a digression to talk about Lagrange duality. This will lead us to our\n",
      "optimization problem's dual form, which will play a key role in allowing us to\n",
      "use kernels to get optimal margin classi\f",
      "ers to work e\u000eciently in very high\n",
      "dimensional spaces. The dual form will also allow us to derive an e\u000ecient\n",
      "algorithm for solving the above optimization problem that will typically do\n",
      "much better than generic QP software.\n",
      "6.5 Lagrange duality (optional reading)\n",
      "Let's temporarily put aside SVMs and maximum margin classi\f",
      "ers, and talk\n",
      "about solving constrained optimization problems.\n",
      "Consider a problem of the following form:\n",
      "minwf(w)\n",
      "s.t.hi(w) = 0; i= 1;:::;l:\n",
      "Some of you may recall how the method of Lagrange multipliers can be used\n",
      "to solve it. (Don't worry if you haven't seen it before.) In this method, we\n",
      "de\f",
      "ne the Lagrangian to be\n",
      "L(w;\f",
      ") =f(w) +lX\n",
      "i=1\f",
      "ihi(w)\n",
      "1You may be familiar with linear programming, which solves optimization problems\n",
      "that have linear objectives and linear constraints. QP software is also widely available,\n",
      "which allows convex quadratic objectives and linear constraints.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "66\n",
      "Here, the\f",
      "i's are called the Lagrange multipliers . We would then \f",
      "nd\n",
      "and setL's partial derivatives to zero:\n",
      "@L\n",
      "@wi= 0;@L\n",
      "@\f",
      "i= 0;\n",
      "and solve for wand\f",
      ".\n",
      "In this section, we will generalize this to constrained optimization prob-\n",
      "lems in which we may have inequality as well as equality constraints. Due to\n",
      "time constraints, we won't really be able to do the theory of Lagrange duality\n",
      "justice in this class,2but we will give the main ideas and results, which we\n",
      "will then apply to our optimal margin classi\f",
      "er's optimization problem.\n",
      "Consider the following, which we'll call the primal optimization problem:\n",
      "minwf(w)\n",
      "s.t.gi(w)\u00140; i= 1;:::;k\n",
      "hi(w) = 0; i= 1;:::;l:\n",
      "To solve it, we start by de\f",
      "ning the generalized Lagrangian\n",
      "L(w;\u000b",
      ";\f",
      " ) =f(w) +kX\n",
      "i=1\u000b",
      "igi(w) +lX\n",
      "i=1\f",
      "ihi(w):\n",
      "Here, the\u000b",
      "i's and\f",
      "i's are the Lagrange multipliers. Consider the quantity\n",
      "\u0012P(w) = max\n",
      "\u000b",
      ";\f",
      ":\u000b",
      "i\u00150L(w;\u000b",
      ";\f",
      " ):\n",
      "Here, the \\P\" subscript stands for \\primal.\" Let some wbe given. If w\n",
      "violates any of the primal constraints (i.e., if either gi(w)>0 orhi(w)6= 0\n",
      "for somei), then you should be able to verify that\n",
      "\u0012P(w) = max\n",
      "\u000b",
      ";\f",
      ":\u000b",
      "i\u00150f(w) +kX\n",
      "i=1\u000b",
      "igi(w) +lX\n",
      "i=1\f",
      "ihi(w) (6.1)\n",
      "=1: (6.2)\n",
      "Conversely, if the constraints are indeed satis\f",
      "ed for a particular value of w,\n",
      "then\u0012P(w) =f(w). Hence,\n",
      "\u0012P(w) =\u001af(w) ifwsatis\f",
      "es primal constraints\n",
      "1 otherwise:\n",
      "2Readers interested in learning more about this topic are encouraged to read, e.g., R.\n",
      "T. Rockarfeller (1970), Convex Analysis, Princeton University Press.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "67\n",
      "Thus,\u0012Ptakes the same value as the objective in our problem for all val-\n",
      "ues ofwthat satis\f",
      "es the primal constraints, and is positive in\f",
      "nity if the\n",
      "constraints are violated. Hence, if we consider the minimization problem\n",
      "min\n",
      "w\u0012P(w) = min\n",
      "wmax\n",
      "\u000b",
      ";\f",
      ":\u000b",
      "i\u00150L(w;\u000b",
      ";\f",
      " );\n",
      "we see that it is the same problem (i.e., and has the same solutions as) our\n",
      "original, primal problem. For later use, we also de\f",
      "ne the optimal value of\n",
      "the objective to be p\u0003= minw\u0012P(w); we call this the value of the primal\n",
      "problem.\n",
      "Now, let's look at a slightly di\u000b",
      "erent problem. We de\f",
      "ne\n",
      "\u0012D(\u000b",
      ";\f",
      ") = min\n",
      "wL(w;\u000b",
      ";\f",
      " ):\n",
      "Here, the \\D\" subscript stands for \\dual.\" Note also that whereas in the\n",
      "de\f",
      "nition of \u0012Pwe were optimizing (maximizing) with respect to \u000b",
      ";\f",
      ", here\n",
      "we are minimizing with respect to w.\n",
      "We can now pose the dual optimization problem:\n",
      "max\n",
      "\u000b",
      ";\f",
      ":\u000b",
      "i\u00150\u0012D(\u000b",
      ";\f",
      ") = max\n",
      "\u000b",
      ";\f",
      ":\u000b",
      "i\u00150min\n",
      "wL(w;\u000b",
      ";\f",
      " ):\n",
      "This is exactly the same as our primal problem shown above, except that the\n",
      "order of the \\max\" and the \\min\" are now exchanged. We also de\f",
      "ne the\n",
      "optimal value of the dual problem's objective to be d\u0003= max\u000b",
      ";\f",
      ":\u000b",
      "i\u00150\u0012D(w).\n",
      "How are the primal and the dual problems related? It can easily be shown\n",
      "that\n",
      "d\u0003= max\n",
      "\u000b",
      ";\f",
      ":\u000b",
      "i\u00150min\n",
      "wL(w;\u000b",
      ";\f",
      " )\u0014min\n",
      "wmax\n",
      "\u000b",
      ";\f",
      ":\u000b",
      "i\u00150L(w;\u000b",
      ";\f",
      " ) =p\u0003:\n",
      "(You should convince yourself of this; this follows from the \\max min\" of a\n",
      "function always being less than or equal to the \\min max.\") However, under\n",
      "certain conditions, we will have\n",
      "d\u0003=p\u0003;\n",
      "so that we can solve the dual problem in lieu of the primal problem. Let's\n",
      "see what these conditions are.\n",
      "Supposefand thegi's are convex,3and thehi's are a\u000ene.4Suppose\n",
      "further that the constraints giare (strictly) feasible; this means that there\n",
      "exists some wso thatgi(w)<0 for alli.\n",
      "3Whenfhas a Hessian, then it is convex if and only if the Hessian is positive semi-\n",
      "de\f",
      "nite. For instance, f(w) =wTwis convex; similarly, all linear (and a\u000ene) functions\n",
      "are also convex. (A function fcan also be convex without being di\u000b",
      "erentiable, but we\n",
      "won't need those more general de\f",
      "nitions of convexity here.)\n",
      "4I.e., there exists ai,bi, so thathi(w) =aT\n",
      "iw+bi. \\A\u000ene\" means the same thing as\n",
      "linear, except that we also allow the extra intercept term bi.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "68\n",
      "Under our above assumptions, there must exist w\u0003;\u000b",
      "\u0003;\f",
      "\u0003so thatw\u0003is the\n",
      "solution to the primal problem, \u000b",
      "\u0003;\f",
      "\u0003are the solution to the dual problem,\n",
      "and moreover p\u0003=d\u0003=L(w\u0003;\u000b",
      "\u0003;\f",
      "\u0003). Moreover, w\u0003;\u000b",
      "\u0003and\f",
      "\u0003satisfy the\n",
      "Karush-Kuhn-Tucker (KKT) conditions , which are as follows:\n",
      "@\n",
      "@wiL(w\u0003;\u000b",
      "\u0003;\f",
      "\u0003) = 0; i= 1;:::;d (6.3)\n",
      "@\n",
      "@\f",
      "iL(w\u0003;\u000b",
      "\u0003;\f",
      "\u0003) = 0; i= 1;:::;l (6.4)\n",
      "\u000b",
      "\u0003\n",
      "igi(w\u0003) = 0; i= 1;:::;k (6.5)\n",
      "gi(w\u0003)\u00140; i= 1;:::;k (6.6)\n",
      "\u000b",
      "\u0003\u00150; i= 1;:::;k (6.7)\n",
      "Moreover, if some w\u0003;\u000b",
      "\u0003;\f",
      "\u0003satisfy the KKT conditions, then it is also a solution to t he primal and dual\n",
      "problems.\n",
      "We draw attention to Equation (6.5), which is called the KKT dual\n",
      "complementarity condition. Speci\f",
      "cally, it implies that if \u000b",
      "\u0003\n",
      "i>0, then\n",
      "gi(w\u0003) = 0. (I.e., the \\ gi(w)\u00140\" constraint is active , meaning it holds with\n",
      "equality rather than with inequality.) Later on, this will be key for showing\n",
      "that the SVM has only a small number of \\support vectors\"; the KKT dual\n",
      "complementarity condition will also give us our convergence test when we\n",
      "talk about the SMO algorithm.\n",
      "6.6 Optimal margin classi\f",
      "ers: the dual form\n",
      "(option reading)\n",
      "Note: The equivalence of optimization problem (6.8) and the optimization\n",
      "problem (6.12) , and the relationship between the primary and dual variables\n",
      "in equation (6.10) are the most important take home messages of this section.\n",
      "Previously, we posed the following (primal) optimization problem for \f",
      "nd-\n",
      "ing the optimal margin classi\f",
      "er:\n",
      "minw;b1\n",
      "2jjwjj2(6.8)\n",
      "s.t.y(i)(wTx(i)+b)\u00151; i= 1;:::;n\n",
      "We can write the constraints as\n",
      "gi(w) =\u0000y(i)(wTx(i)+b) + 1\u00140:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "69\n",
      "We have one such constraint for each training example. Note that from the\n",
      "KKT dual complementarity condition, we will have \u000b",
      "i>0 only for the train-\n",
      "ing examples that have functional margin exactly equal to one (i.e., the ones\n",
      "corresponding to constraints that hold with equality, gi(w) = 0). Consider\n",
      "the \f",
      "gure below, in which a maximum margin separating hyperplane is shown\n",
      "by the solid line.\n",
      "The points with the smallest margins are exactly the ones closest to the\n",
      "decision boundary; here, these are the three points (one negative and two pos-\n",
      "itive examples) that lie on the dashed lines parallel to the decision boundary.\n",
      "Thus, only three of the \u000b",
      "i's|namely, the ones corresponding to these three\n",
      "training examples|will be non-zero at the optimal solution to our optimiza-\n",
      "tion problem. These three points are called the support vectors in this\n",
      "problem. The fact that the number of support vectors can be much smaller\n",
      "than the size the training set will be useful later.\n",
      "Let's move on. Looking ahead, as we develop the dual form of the prob-\n",
      "lem, one key idea to watch out for is that we'll try to write our algorithm\n",
      "in terms of only the inner product hx(i);x(j)i(think of this as ( x(i))Tx(j))\n",
      "between points in the input feature space. The fact that we can express our\n",
      "algorithm in terms of these inner products will be key when we apply the\n",
      "kernel trick.\n",
      "When we construct the Lagrangian for our optimization problem we have:\n",
      "L(w;b;\u000b",
      " ) =1\n",
      "2jjwjj2\u0000nX\n",
      "i=1\u000b",
      "i\u0002\n",
      "y(i)(wTx(i)+b)\u00001\u0003\n",
      ": (6.9)\n",
      "Note that there're only \\ \u000b",
      "i\" but no \\ \f",
      "i\" Lagrange multipliers, since the\n",
      "problem has only inequality constraints.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "70\n",
      "Let's \f",
      "nd the dual form of the problem. To do so, we need to \f",
      "rst\n",
      "minimizeL(w;b;\u000b",
      " ) with respect to wandb(for \f",
      "xed\u000b",
      "), to get\u0012D, which\n",
      "we'll do by setting the derivatives of Lwith respect to wandbto zero. We\n",
      "have:\n",
      "rwL(w;b;\u000b",
      " ) =w\u0000nX\n",
      "i=1\u000b",
      "iy(i)x(i)= 0\n",
      "This implies that\n",
      "w=nX\n",
      "i=1\u000b",
      "iy(i)x(i): (6.10)\n",
      "As for the derivative with respect to b, we obtain\n",
      "@\n",
      "@bL(w;b;\u000b",
      " ) =nX\n",
      "i=1\u000b",
      "iy(i)= 0: (6.11)\n",
      "If we take the de\f",
      "nition of win Equation (6.10) and plug that back into\n",
      "the Lagrangian (Equation 6.9), and simplify, we get\n",
      "L(w;b;\u000b",
      " ) =nX\n",
      "i=1\u000b",
      "i\u00001\n",
      "2nX\n",
      "i;j=1y(i)y(j)\u000b",
      "i\u000b",
      "j(x(i))Tx(j)\u0000bnX\n",
      "i=1\u000b",
      "iy(i):\n",
      "But from Equation (6.11), the last term must be zero, so we obtain\n",
      "L(w;b;\u000b",
      " ) =nX\n",
      "i=1\u000b",
      "i\u00001\n",
      "2nX\n",
      "i;j=1y(i)y(j)\u000b",
      "i\u000b",
      "j(x(i))Tx(j):\n",
      "Recall that we got to the equation above by minimizing Lwith respect to\n",
      "wandb. Putting this together with the constraints \u000b",
      "i\u00150 (that we always\n",
      "had) and the constraint (6.11), we obtain the following dual optimization\n",
      "problem:\n",
      "max\u000b",
      "W(\u000b",
      ") =nX\n",
      "i=1\u000b",
      "i\u00001\n",
      "2nX\n",
      "i;j=1y(i)y(j)\u000b",
      "i\u000b",
      "jhx(i);x(j)i: (6.12)\n",
      "s.t.\u000b",
      "i\u00150; i= 1;:::;n\n",
      "nX\n",
      "i=1\u000b",
      "iy(i)= 0;\n",
      "You should also be able to verify that the conditions required for p\u0003=d\u0003\n",
      "and the KKT conditions (Equations 6.3{6.7) to hold are indeed satis\f",
      "ed in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "71\n",
      "our optimization problem. Hence, we can solve the dual in lieu of solving\n",
      "the primal problem. Speci\f",
      "cally, in the dual problem above, we have a\n",
      "maximization problem in which the parameters are the \u000b",
      "i's. We'll talk later\n",
      "about the speci\f",
      "c algorithm that we're going to use to solve the dual problem,\n",
      "but if we are indeed able to solve it (i.e., \f",
      "nd the \u000b",
      "'s that maximize W(\u000b",
      ")\n",
      "subject to the constraints), then we can use Equation (6.10) to go back and\n",
      "\f",
      "nd the optimal w's as a function of the \u000b",
      "'s. Having found w\u0003, by considering\n",
      "the primal problem, it is also straightforward to \f",
      "nd the optimal value for\n",
      "the intercept term bas\n",
      "b\u0003=\u0000maxi:y(i)=\u00001w\u0003Tx(i)+ mini:y(i)=1w\u0003Tx(i)\n",
      "2: (6.13)\n",
      "(Check for yourself that this is correct.)\n",
      "Before moving on, let's also take a more careful look at Equation (6.10),\n",
      "which gives the optimal value of win terms of (the optimal value of) \u000b",
      ".\n",
      "Suppose we've \f",
      "t our model's parameters to a training set, and now wish to\n",
      "make a prediction at a new point input x. We would then calculate wTx+b,\n",
      "and predict y= 1 if and only if this quantity is bigger than zero. But\n",
      "using (6.10), this quantity can also be written:\n",
      "wTx+b= nX\n",
      "i=1\u000b",
      "iy(i)x(i)!T\n",
      "x+b (6.14)\n",
      "=nX\n",
      "i=1\u000b",
      "iy(i)hx(i);xi+b: (6.15)\n",
      "Hence, if we've found the \u000b",
      "i's, in order to make a prediction, we have to\n",
      "calculate a quantity that depends only on the inner product between xand\n",
      "the points in the training set. Moreover, we saw earlier that the \u000b",
      "i's will all\n",
      "be zero except for the support vectors. Thus, many of the terms in the sum\n",
      "above will be zero, and we really need to \f",
      "nd only the inner products between\n",
      "xand the support vectors (of which there is often only a small number) in\n",
      "order calculate (6.15) and make our prediction.\n",
      "By examining the dual form of the optimization problem, we gained sig-\n",
      "ni\f",
      "cant insight into the structure of the problem, and were also able to write\n",
      "the entire algorithm in terms of only inner products between input feature\n",
      "vectors. In the next section, we will exploit this property to apply the ker-\n",
      "nels to our classi\f",
      "cation problem. The resulting algorithm, support vector\n",
      "machines , will be able to e\u000eciently learn in very high dimensional spaces.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "72\n",
      "6.7 Regularization and the non-separable case\n",
      "(optional reading)\n",
      "The derivation of the SVM as presented so far assumed that the data is\n",
      "linearly separable. While mapping data to a high dimensional feature space\n",
      "via\u001e",
      "does generally increase the likelihood that the data is separable, we\n",
      "can't guarantee that it always will be so. Also, in some cases it is not clear\n",
      "that \f",
      "nding a separating hyperplane is exactly what we'd want to do, since\n",
      "that might be susceptible to outliers. For instance, the left \f",
      "gure below\n",
      "shows an optimal margin classi\f",
      "er, and when a single outlier is added in the\n",
      "upper-left region (right \f",
      "gure), it causes the decision boundary to make a\n",
      "dramatic swing, and the resulting classi\f",
      "er has a much smaller margin.\n",
      "To make the algorithm work for non-linearly separable datasets as well\n",
      "as be less sensitive to outliers, we reformulate our optimization (using `1\n",
      "regularization ) as follows:\n",
      ";w;b1\n",
      "2jjwjj2+CnX\n",
      "i=1\u0018i\n",
      "s.t.y(i)(wTx(i)+b)\u00151\u0000\u0018i; i= 1;:::;n\n",
      "\u0018i\u00150; i= 1;:::;n:\n",
      "Thus, examples are now permitted to have (functional) margin less than 1,\n",
      "and if an example has functional margin 1 \u0000\u0018i(with\u0018 >0), we would pay\n",
      "a cost of the objective function being increased by C\u0018i. The parameter C\n",
      "controls the relative weighting between the twin goals of making the jjwjj2\n",
      "small (which we saw earlier makes the margin large) and of ensuring that\n",
      "most examples have functional margin at least 1.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "73\n",
      "As before, we can form the Lagrangian:\n",
      "L(w;b;\u0018;\u000b",
      ";r ) =1\n",
      "2wTw+CnX\n",
      "i=1\u0018i\u0000nX\n",
      "i=1\u000b",
      "i\u0002\n",
      "y(i)(xTw+b)\u00001 +\u0018i\u0003\n",
      "\u0000nX\n",
      "i=1ri\u0018i:\n",
      "Here, the\u000b",
      "i's andri's are our Lagrange multipliers (constrained to be \u00150).\n",
      "We won't go through the derivation of the dual again in detail, but after\n",
      "setting the derivatives with respect to wandbto zero as before, substituting\n",
      "them back in, and simplifying, we obtain the following dual form of the\n",
      "problem:\n",
      "max\u000b",
      "W(\u000b",
      ") =nX\n",
      "i=1\u000b",
      "i\u00001\n",
      "2nX\n",
      "i;j=1y(i)y(j)\u000b",
      "i\u000b",
      "jhx(i);x(j)i\n",
      "s.t. 0\u0014\u000b",
      "i\u0014C; i = 1;:::;n\n",
      "nX\n",
      "i=1\u000b",
      "iy(i)= 0;\n",
      "As before, we also have that wcan be expressed in terms of the \u000b",
      "i's as\n",
      "given in Equation (6.10), so that after solving the dual problem, we can con-\n",
      "tinue to use Equation (6.15) to make our predictions. Note that, somewhat\n",
      "surprisingly, in adding `1regularization, the only change to the dual prob-\n",
      "lem is that what was originally a constraint that 0 \u0014\u000b",
      "ihas now become\n",
      "0\u0014\u000b",
      "i\u0014C. The calculation for b\u0003also has to be modi\f",
      "ed (Equation 6.13 is\n",
      "no longer valid); see the comments in the next section/Platt's paper.\n",
      "Also, the KKT dual-complementarity conditions (which in the next sec-\n",
      "tion will be useful for testing for the convergence of the SMO algorithm)\n",
      "are:\n",
      "\u000b",
      "i= 0)y(i)(wTx(i)+b)\u00151 (6.16)\n",
      "\u000b",
      "i=C)y(i)(wTx(i)+b)\u00141 (6.17)\n",
      "0<\u000b",
      "i<C)y(i)(wTx(i)+b) = 1: (6.18)\n",
      "Now, all that remains is to give an algorithm for actually solving the dual\n",
      "problem, which we will do in the next section.\n",
      "6.8 The SMO algorithm (optional reading)\n",
      "The SMO (sequential minimal optimization) algorithm, due to John Platt,\n",
      "gives an e\u000ecient way of solving the dual problem arising from the derivation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "74\n",
      "of the SVM. Partly to motivate the SMO algorithm, and partly because it's\n",
      "interesting in its own right, let's \f",
      "rst take another digression to talk about\n",
      "the coordinate ascent algorithm.\n",
      "6.8.1 Coordinate ascent\n",
      "Consider trying to solve the unconstrained optimization problem\n",
      "max\n",
      "\u000b",
      "W(\u000b",
      "1;\u000b",
      "2;:::;\u000b",
      "n):\n",
      "Here, we think of Was just some function of the parameters \u000b",
      "i's, and for now\n",
      "ignore any relationship between this problem and SVMs. We've already seen\n",
      "two optimization algorithms, gradient ascent and Newton's method. The\n",
      "new algorithm we're going to consider here is called coordinate ascent :\n",
      "Loop until convergence: f\n",
      "Fori= 1;:::;n ,f\n",
      "\u000b",
      "i:= arg max ^\u000b",
      "iW(\u000b",
      "1;:::;\u000b",
      "i\u00001;^\u000b",
      "i;\u000b",
      "i+1;:::;\u000b",
      "n).\n",
      "g\n",
      "g\n",
      "Thus, in the innermost loop of this algorithm, we will hold all the variables\n",
      "except for some \u000b",
      "i\f",
      "xed, and reoptimize Wwith respect to just the parameter\n",
      "\u000b",
      "i. In the version of this method presented here, the inner-loop reoptimizes\n",
      "the variables in order \u000b",
      "1;\u000b",
      "2;:::;\u000b",
      "n;\u000b",
      "1;\u000b",
      "2;:::. (A more sophisticated version\n",
      "might choose other orderings; for instance, we may choose the next variable\n",
      "to update according to which one we expect to allow us to make the largest\n",
      "increase in W(\u000b",
      ").)\n",
      "When the function Whappens to be of such a form that the \\arg max\"\n",
      "in the inner loop can be performed e\u000eciently, then coordinate ascent can be\n",
      "a fairly e\u000ecient algorithm. Here's a picture of coordinate ascent in action:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "75\n",
      "−2 −1.5 −1 −0.5 0 0.5 1 1.5 2 2.5−2−1.5−1−0.500.511.522.5\n",
      "The ellipses in the \f",
      "gure are the contours of a quadratic function that\n",
      "we want to optimize. Coordinate ascent was initialized at (2 ;\u00002), and also\n",
      "plotted in the \f",
      "gure is the path that it took on its way to the global maximum.\n",
      "Notice that on each step, coordinate ascent takes a step that's parallel to one\n",
      "of the axes, since only one variable is being optimized at a time.\n",
      "6.8.2 SMO\n",
      "We close o\u000b",
      " the discussion of SVMs by sketching the derivation of the SMO\n",
      "algorithm.\n",
      "Here's the (dual) optimization problem that we want to solve:\n",
      "max\u000b",
      "W(\u000b",
      ") =nX\n",
      "i=1\u000b",
      "i\u00001\n",
      "2nX\n",
      "i;j=1y(i)y(j)\u000b",
      "i\u000b",
      "jhx(i);x(j)i: (6.19)\n",
      "s.t. 0\u0014\u000b",
      "i\u0014C; i = 1;:::;n (6.20)\n",
      "nX\n",
      "i=1\u000b",
      "iy(i)= 0: (6.21)\n",
      "Let's say we have set of \u000b",
      "i's that satisfy the constraints (6.20-6.21). Now,\n",
      "suppose we want to hold \u000b",
      "2;:::;\u000b",
      "n\f",
      "xed, and take a coordinate ascent step\n",
      "and reoptimize the objective with respect to \u000b",
      "1. Can we make any progress?\n",
      "The answer is no, because the constraint (6.21) ensures that\n",
      "\u000b",
      "1y(1)=\u0000nX\n",
      "i=2\u000b",
      "iy(i):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "76\n",
      "Or, by multiplying both sides by y(1), we equivalently have\n",
      "\u000b",
      "1=\u0000y(1)nX\n",
      "i=2\u000b",
      "iy(i):\n",
      "(This step used the fact that y(1)2f\u0000 1;1g, and hence ( y(1))2= 1.) Hence,\n",
      "\u000b",
      "1is exactly determined by the other \u000b",
      "i's, and if we were to hold \u000b",
      "2;:::;\u000b",
      "n\n",
      "\f",
      "xed, then we can't make any change to \u000b",
      "1without violating the con-\n",
      "straint (6.21) in the optimization problem.\n",
      "Thus, if we want to update some subject of the \u000b",
      "i's, we must update at\n",
      "least two of them simultaneously in order to keep satisfying the constraints.\n",
      "This motivates the SMO algorithm, which simply does the following:\n",
      "Repeat till convergence f\n",
      "1. Select some pair \u000b",
      "iand\u000b",
      "jto update next (using a heuristic that\n",
      "tries to pick the two that will allow us to make the biggest progress\n",
      "towards the global maximum).\n",
      "2. Reoptimize W(\u000b",
      ") with respect to \u000b",
      "iand\u000b",
      "j, while holding all the\n",
      "other\u000b",
      "k's (k6=i;j) \f",
      "xed.\n",
      "g\n",
      "To test for convergence of this algorithm, we can check whether the KKT\n",
      "conditions (Equations 6.16-6.18) are satis\f",
      "ed to within some tol. Here, tolis\n",
      "the convergence tolerance parameter, and is typically set to around 0.01 to\n",
      "0.001. (See the paper and pseudocode for details.)\n",
      "The key reason that SMO is an e\u000ecient algorithm is that the update to\n",
      "y sketch the mainted very e\u000eciently. Let's now brie\n",
      "ideas for deriving the e\u000ecient update.\n",
      "Let's say we currently have some setting of the \u000b",
      "i's that satisfy the con-\n",
      "straints (6.20-6.21), and suppose we've decided to hold \u000b",
      "3;:::;\u000b",
      "n\f",
      "xed, and\n",
      "want to reoptimize W(\u000b",
      "1;\u000b",
      "2;:::;\u000b",
      "n) with respect to \u000b",
      "1and\u000b",
      "2(subject to\n",
      "the constraints). From (6.21), we require that\n",
      "\u000b",
      "1y(1)+\u000b",
      "2y(2)=\u0000nX\n",
      "i=3\u000b",
      "iy(i):\n",
      "Since the right hand side is \f",
      "xed (as we've \f",
      "xed \u000b",
      "3;:::\u000b",
      "n), we can just let\n",
      "it be denoted by some constant \u0010:\n",
      "\u000b",
      "1y(1)+\u000b",
      "2y(2)=\u0010: (6.22)\n",
      "We can thus picture the constraints on \u000b",
      "1and\u000b",
      "2as follows:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "77\n",
      "α2\n",
      "α1α1 α2\n",
      "CC\n",
      "(1)+(2)y y=ζH\n",
      "L\n",
      "From the constraints (6.20), we know that \u000b",
      "1and\u000b",
      "2must lie within the box\n",
      "[0;C]\u0002[0;C] shown. Also plotted is the line \u000b",
      "1y(1)+\u000b",
      "2y(2)=\u0010, on which we\n",
      "know\u000b",
      "1and\u000b",
      "2must lie. Note also that, from these constraints, we know\n",
      "L\u0014\u000b",
      "2\u0014H; otherwise, ( \u000b",
      "1;\u000b",
      "2) can't simultaneously satisfy both the box\n",
      "and the straight line constraint. In this example, L= 0. But depending on\n",
      "what the line \u000b",
      "1y(1)+\u000b",
      "2y(2)=\u0010looks like, this won't always necessarily be\n",
      "the case; but more generally, there will be some lower-bound Land some\n",
      "upper-bound Hon the permissible values for \u000b",
      "2that will ensure that \u000b",
      "1,\u000b",
      "2\n",
      "lie within the box [0 ;C]\u0002[0;C].\n",
      "Using Equation (6.22), we can also write \u000b",
      "1as a function of \u000b",
      "2:\n",
      "\u000b",
      "1= (\u0010\u0000\u000b",
      "2y(2))y(1):\n",
      "(Check this derivation yourself; we again used the fact that y(1)2f\u0000 1;1gso\n",
      "that (y(1))2= 1.) Hence, the objective W(\u000b",
      ") can be written\n",
      "W(\u000b",
      "1;\u000b",
      "2;:::;\u000b",
      "n) =W((\u0010\u0000\u000b",
      "2y(2))y(1);\u000b",
      "2;:::;\u000b",
      "n):\n",
      "Treating\u000b",
      "3;:::;\u000b",
      "nas constants, you should be able to verify that this is\n",
      "just some quadratic function in \u000b",
      "2. I.e., this can also be expressed in the\n",
      "forma\u000b",
      "2\n",
      "2+b\u000b",
      "2+cfor some appropriate a,b, andc. If we ignore the \\box\"\n",
      "constraints (6.20) (or, equivalently, that L\u0014\u000b",
      "2\u0014H), then we can easily\n",
      "maximize this quadratic function by setting its derivative to zero and solving.\n",
      "We'll let\u000b",
      "new;unclipped\n",
      "2 denote the resulting value of \u000b",
      "2. You should also be\n",
      "able to convince yourself that if we had instead wanted to maximize Wwith\n",
      "respect to\u000b",
      "2but subject to the box constraint, then we can \f",
      "nd the resulting\n",
      "value optimal simply by taking \u000b",
      "new;unclipped\n",
      "2 and \\clipping\" it to lie in the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "78\n",
      "[L;H] interval, to get\n",
      "\u000b",
      "new\n",
      "2 =8\n",
      "<\n",
      ":H if\u000b",
      "new;unclipped\n",
      "2 >H\n",
      "\u000b",
      "new;unclipped\n",
      "2 ifL\u0014\u000b",
      "new;unclipped\n",
      "2\u0014H\n",
      "L if\u000b",
      "new;unclipped\n",
      "2 <L\n",
      "Finally, having found the \u000b",
      "new\n",
      "2, we can use Equation (6.22) to go back and\n",
      "\f",
      "nd the optimal value of \u000b",
      "new\n",
      "1.\n",
      "There're a couple more details that are quite easy but that we'll leave you\n",
      "to read about yourself in Platt's paper: One is the choice of the heuristics\n",
      "used to select the next \u000b",
      "i,\u000b",
      "jto update; the other is how to update bas the\n",
      "SMO algorithm is run.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Part II\n",
      "Deep learning\n",
      "79\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 7\n",
      "Deep learning\n",
      "We now begin our study of deep learning. In this set of notes, we give an\n",
      "overview of neural networks, discuss vectorization and discuss training neural\n",
      "networks with backpropagation.\n",
      "7.1 Supervised learning with non-linear mod-\n",
      "els\n",
      "In the supervised learning setting (predicting yfrom the input x), suppose\n",
      "our model/hypothesis is h\u0012(x). In the past lectures, we have considered the\n",
      "cases when h\u0012(x) =\u0012>x(in linear regression) or h\u0012(x) =\u0012>\u001e",
      "(x) (where\u001e",
      "(x)\n",
      "is the feature map). A commonality of these two models is that they are\n",
      "linear in the parameters \u0012. Next we will consider learning general family of\n",
      "models that are non-linear in both the parameters \u0012and the inputs x. The\n",
      "most common non-linear models are neural networks, which we will de\f",
      "ne\n",
      "staring from the next section. For this section, it su\u000eces to think h\u0012(x) as\n",
      "an abstract non-linear model.1\n",
      "Supposef(x(i);y(i))gn\n",
      "i=1are the training examples. We will de\f",
      "ne the\n",
      "nonlinear model and the loss/cost function for learning it.\n",
      "Regression problems. For simplicity, we start with the case where the\n",
      "output is a real number, that is, y(i)2R, and thus the model h\u0012also outputs\n",
      "a real number h\u0012(x)2R. We de\f",
      "ne the least square cost function for the\n",
      "1If a concrete example is helpful, perhaps think about the model h\u0012(x) =\u00122\n",
      "1x2\n",
      "1+\u00122\n",
      "2x2\n",
      "2+\n",
      "\u0001\u0001\u0001+\u00122\n",
      "dx2\n",
      "din this subsection, even though it's not a neural network.\n",
      "80\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "81\n",
      "i-th example ( x(i);y(i)) as\n",
      "J(i)(\u0012) =1\n",
      "2(h\u0012(x(i))\u0000y(i))2; (7.1)\n",
      "and de\f",
      "ne the mean-square cost function for the dataset as\n",
      "J(\u0012) =1\n",
      "nnX\n",
      "i=1J(i)(\u0012); (7.2)\n",
      "which is same as in linear regression except that we introduce a constant\n",
      "1=nin front of the cost function to be consistent with the convention. Note\n",
      "that multiplying the cost function with a scalar will not change the local\n",
      "minima or global minima of the cost function. Also note that the underlying\n",
      "parameterization for h\u0012(x) is di\u000b",
      "erent from the case of linear regression,\n",
      "even though the form of the cost function is the same mean-squared loss.\n",
      "Throughout the notes, we use the words \\loss\" and \\cost\" interchangeably.\n",
      "Binary classi\f",
      "cation. Next we de\f",
      "ne the model and loss function for\n",
      "binary classi\f",
      "cation. Suppose the inputs x2Rd. Let \u0016h\u0012:Rd!Rbe a\n",
      "parameterized model (the analog of \u0012>xin logistic linear regression). We\n",
      "call the output \u0016h\u0012(x)2Rthe logit. Analogous to Section 2.1, we use the\n",
      "logistic function g(\u0001) to turn the logit \u0016h\u0012(x) to a probability h\u0012(x)2[0;1]:\n",
      "h\u0012(x) =g(\u0016h\u0012(x)) = 1=(1 + exp(\u0000\u0016h\u0012(x)): (7.3)\n",
      "We model the conditional distribution of ygivenxand\u0012by\n",
      "P(y= 1jx;\u0012) =h\u0012(x)\n",
      "P(y= 0jx;\u0012) = 1\u0000h\u0012(x)\n",
      "Following the same derivation in Section 2.1 and using the derivation in\n",
      "Remark 2.1.1, the negative likelihood loss function is equal to:\n",
      "J(i)(\u0012) =\u0000logp(y(i)jx(i);\u0012) =`logistic (\u0016h\u0012(x(i));y(i)) (7.4)\n",
      "As done in equation (7.2), the total loss function is also de\f",
      "ned as the average\n",
      "of the loss function over individual training examples, J(\u0012) =1\n",
      "nPn\n",
      "i=1J(i)(\u0012):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "82\n",
      "Multi-class classi\f",
      "cation. Following Section 2.3, we consider a classi\f",
      "ca-\n",
      "tion problem where the response variable ycan take on any one of kvalues,\n",
      "i.e.y2f1;2;:::;kg. Let \u0016h\u0012:Rd!Rkbe a parameterized model. We\n",
      "call the outputs \u0016h\u0012(x)2Rkthe logits. Each logit corresponds to the predic-\n",
      "tion for one of the kclasses. Analogous to Section 2.3, we use the softmax\n",
      "function to turn the logits \u0016h\u0012(x) into a probability vector with non-negative\n",
      "entries that sum up to 1:\n",
      "P(y=jjx;\u0012) =exp(\u0016h\u0012(x)j)\n",
      "Pk\n",
      "s=1exp(\u0016h\u0012(x)s); (7.5)\n",
      "where \u0016h\u0012(x)sdenotes the s-th coordinate of \u0016h\u0012(x).\n",
      "Similarly to Section 2.3, the loss function for a single training example\n",
      "(x(i);y(i)) is its negative log-likelihood:\n",
      "J(i)(\u0012) =\u0000logp(y(i)jx(i);\u0012) =\u0000log \n",
      "exp(\u0016h\u0012(x(i))y(i))\n",
      "Pk\n",
      "s=1exp(\u0016h\u0012(x(i))s)!\n",
      ": (7.6)\n",
      "Using the notations of Section 2.3, we can simply write in an abstract way:\n",
      "J(i)(\u0012) =`ce(\u0016h\u0012(x(i));y(i)): (7.7)\n",
      "The loss function is also de\f",
      "ned as the average of the loss function of indi-\n",
      "vidual training examples, J(\u0012) =1\n",
      "nPn\n",
      "i=1J(i)(\u0012):\n",
      "We also note that the approach above can also be generated to any con-\n",
      "ditional probabilistic model where we have an exponential distribution for\n",
      "y, Exponential-family( y;\u0011), where\u0011=\u0016h\u0012(x) is a parameterized nonlinear\n",
      "function of x. However, the most widely used situations are the three cases\n",
      "discussed above.\n",
      "Optimizers (SGD). Commonly, people use gradient descent (GD), stochas-\n",
      "tic gradient (SGD), or their variants to optimize the loss function J(\u0012). GD's\n",
      "update rule can be written as2\n",
      "\u0012:=\u0012\u0000\u000b",
      "r\u0012J(\u0012) (7.8)\n",
      "where\u000b",
      " > 0 is often referred to as the learning rate or step size. Next, we\n",
      "introduce a version of the SGD (Algorithm 1), which is lightly di\u000b",
      "erent from\n",
      "that in the \f",
      "rst lecture notes.\n",
      "2Recall that, as de\f",
      "ned in the previous lecture notes, we use the notation \\ a:=b\" to\n",
      "denote an operation (in a computer program) in which we setthe value of a variable ato\n",
      "be equal to the value of b. In other words, this operation overwrites awith the value of\n",
      "b. In contrast, we will write \\ a=b\" when we are asserting a statement of fact, that the\n",
      "value ofais equal to the value of b.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "83\n",
      "Algorithm 1 Stochastic Gradient Descent\n",
      "1:Hyperparameter: learning rate \u000b",
      ", number of total iteration niter.\n",
      "2:Initialize\u0012randomly.\n",
      "3:fori= 1 toniterdo\n",
      "4: Samplejuniformly fromf1;:::;ng, and update \u0012by\n",
      "\u0012:=\u0012\u0000\u000b",
      "r\u0012J(j)(\u0012) (7.9)\n",
      "Oftentimes computing the gradient of Bexamples simultaneously for the\n",
      "parameter \u0012can be faster than computing Bgradients separately due to\n",
      "hardware parallelization. Therefore, a mini-batch version of SGD is most\n",
      "commonly used in deep learning, as shown in Algorithm 2. There are also\n",
      "other variants of the SGD or mini-batch SGD with slightly di\u000b",
      "erent sampling\n",
      "schemes.\n",
      "Algorithm 2 Mini-batch Stochastic Gradient Descent\n",
      "1:Hyperparameters: learning rate \u000b",
      ", batch size B, # iterations niter.\n",
      "2:Initialize\u0012randomly\n",
      "3:fori= 1 toniterdo\n",
      "4: SampleBexamplesj1;:::;jB(without replacement) uniformly from\n",
      "f1;:::;ng, and update \u0012by\n",
      "\u0012:=\u0012\u0000\u000b",
      "\n",
      "BBX\n",
      "k=1r\u0012J(jk)(\u0012) (7.10)\n",
      "With these generic algorithms, a typical deep learning model is learned\n",
      "with the following steps. 1. De\f",
      "ne a neural network parametrization h\u0012(x),\n",
      "which we will introduce in Section 7.2, and 2. write the backpropagation\n",
      "algorithm to compute the gradient of the loss function J(j)(\u0012) e\u000eciently,\n",
      "which will be covered in Section 7.4, and 3. run SGD or mini-batch SGD (or\n",
      "other gradient-based optimizers) with the loss function J(\u0012).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "84\n",
      "7.2 Neural networks\n",
      "Neural networks refer to a broad type of non-linear models/parametrizations\n",
      "\u0016h\u0012(x) that involve combinations of matrix multiplications and other entry-\n",
      "wise non-linear operations. To have a uni\f",
      "ed treatment for regression prob-\n",
      "lem and classi\f",
      "cation problem, here we consider \u0016h\u0012(x) as the output of the\n",
      "neural network. For regression problem, the \f",
      "nal prediction h\u0012(x) =\u0016h\u0012(x),\n",
      "and for classi\f",
      "cation problem, \u0016h\u0012(x) is the logits and the predicted probability\n",
      "will beh\u0012(x) = 1=(1+exp(\u0000\u0016h\u0012(x)) (see equation 7.3) for binary classi\f",
      "cation\n",
      "orh\u0012(x) = softmax( \u0016h\u0012(x)) for multi-class classi\f",
      "cation (see equation 7.5).\n",
      "We will start small and slowly build up a neural network, step by step.\n",
      "A Neural Network with a Single Neuron. Recall the housing price\n",
      "prediction problem from before: given the size of the house, we want to\n",
      "predict the price. We will use it as a running example in this subsection.\n",
      "Previously, we \f",
      "t a straight line to the graph of size vs. housing price.\n",
      "Now, instead of \f",
      "tting a straight line, we wish to prevent negative housing\n",
      "prices by setting the absolute minimum price as zero. This produces a \\kink\"\n",
      "in the graph as shown in Figure 7.1. How do we represent such a function\n",
      "with a single kink as \u0016h\u0012(x) with unknown parameter? (After doing so, we\n",
      "can invoke the machinery in Section 7.1.)\n",
      "We de\f",
      "ne a parameterized function \u0016h\u0012(x) with input x, parameterized by\n",
      "\u0012, which outputs the price of the house y. Formally, \u0016h\u0012:x!y. Perhaps\n",
      "one of the simplest parametrization would be\n",
      "\u0016h\u0012(x) = max(wx+b;0);where\u0012= (w;b)2R2(7.11)\n",
      "Here \u0016h\u0012(x) returns a single value: ( wx+b) or zero, whichever is greater. In\n",
      "the context of neural networks, the function max ft;0gis called a ReLU (pro-\n",
      "nounced \\ray-lu\"), or recti\f",
      "ed linear unit, and often denoted by ReLU( t),\n",
      "maxft;0g.\n",
      "Generally, a one-dimensional non-linear function that maps RtoRsuch as\n",
      "ReLU is often referred to as an activation function . The model \u0016h\u0012(x) is said\n",
      "to have a single neuron partly because it has a single non-linear activation\n",
      "function. (We will discuss more about why a non-linear activation is called\n",
      "neuron.)\n",
      "When the input x2Rdhas multiple dimensions, a neural network with\n",
      "a single neuron can be written as\n",
      "\u0016h\u0012(x) = ReLU(w>x+b);wherew2Rd,b2R, and\u0012= (w;b) (7.12)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "85\n",
      "500 1000 1500 2000 2500 3000 3500 4000 4500 500001002003004005006007008009001000housing prices \n",
      "square feet price (in $1000) \n",
      "Figure 7.1: Housing prices with a \\kink\" in the graph.\n",
      "The termbis often referred to as the \\bias\", and the vector wis referred\n",
      "to as the weight vector. Such a neural network has 1 layer. (We will de\f",
      "ne\n",
      "what multiple layers mean in the sequel.)\n",
      "Stacking Neurons. A more complex neural network may take the single\n",
      "neuron described above and \\stack\" them together such that one neuron\n",
      "passes its output as input into the next neuron, resulting in a more complex\n",
      "function.\n",
      "Let us now deepen the housing prediction example. In addition to the size\n",
      "of the house, suppose that you know the number of bedrooms, the zip code\n",
      "and the wealth of the neighborhood. Building neural networks is analogous\n",
      "to Lego bricks: you take individual bricks and stack them together to build\n",
      "complex structures. The same applies to neural networks: we take individual\n",
      "neurons and stack them together to create complex neural networks.\n",
      "Given these features (size, number of bedrooms, zip code, and wealth),\n",
      "we might then decide that the price of the house depends on the maximum\n",
      "family size it can accommodate. Suppose the family size is a function of the\n",
      "size of the house and number of bedrooms (see Figure 7.2). The zip code\n",
      "may provide additional information such as how walkable the neighborhood\n",
      "is (i.e., can you walk to the grocery store or do you need to drive everywhere).\n",
      "Combining the zip code with the wealth of the neighborhood may predict\n",
      "the quality of the local elementary school. Given these three derived features\n",
      "(family size, walkable, school quality), we may conclude that the price of the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "86\n",
      "home ultimately depends on these three features.\n",
      "Family Size \n",
      "School Quality Walkable Size \n",
      "# Bedrooms \n",
      "Zip Code \n",
      "Wealth Price \n",
      "y\n",
      "Figure 7.2: Diagram of a small neural network for predicting housing prices.\n",
      "Formally, the input to a neural network is a set of input features\n",
      "x1;x2;x3;x4. We denote the intermediate variables for \\family size\", \\walk-\n",
      "able\", and \\school quality\" by a1;a2;a3(theseai's are often referred to as\n",
      "\\hidden units\" or \\hidden neurons\"). We represent each of the ai's as a neu-\n",
      "ral network with a single neuron with a subset of x1;:::;x 4as inputs. Then\n",
      "as in Figure 7.1, we will have the parameterization:\n",
      "a1= ReLU(\u00121x1+\u00122x2+\u00123)\n",
      "a2= ReLU(\u00124x3+\u00125)\n",
      "a3= ReLU(\u00126x3+\u00127x4+\u00128)\n",
      "where (\u00121;\u0001\u0001\u0001;\u00128) are parameters. Now we represent the \f",
      "nal output \u0016h\u0012(x)\n",
      "as another linear function with a1;a2;a3as inputs, and we get3\n",
      "\u0016h\u0012(x) =\u00129a1+\u001210a2+\u001211a3+\u001212 (7.13)\n",
      "where\u0012contains all the parameters ( \u00121;\u0001\u0001\u0001;\u001212).\n",
      "Now we represent the output as a quite complex function of xwith pa-\n",
      "rameters\u0012. Then you can use this parametrization \u0016h\u0012with the machinery of\n",
      "Section 7.1 to learn the parameters \u0012.\n",
      "Inspiration from Biological Neural Networks. As the name suggests,\n",
      "arti\f",
      "cial neural networks were inspired by biological neural networks. The\n",
      "hidden units a1;:::;amcorrespond to the neurons in a biological neural net-\n",
      "work, and the parameters \u0012i's correspond to the synapses. However, it's\n",
      "unclear how similar the modern deep arti\f",
      "cial neural networks are to the bi-\n",
      "ological ones. For example, perhaps not many neuroscientists think biological\n",
      "3Typically, for multi-layer neural network, at the end, near the output, we don't apply\n",
      "ReLU, especially when the output is not necessarily a positive number.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "87\n",
      "neural networks could have 1000 layers, while some modern arti\f",
      "cial neural\n",
      "networks do (we will elaborate more on the notion of layers.) Moreover, it's\n",
      "an open question whether human brains update their neural networks in a\n",
      "way similar to the way that computer scientists learn arti\f",
      "cial neural net-\n",
      "works (using backpropagation, which we will introduce in the next section.).\n",
      "Two-layer Fully-Connected Neural Networks. We constructed the\n",
      "neural network in equation (7.13) using a signi\f",
      "cant amount of prior knowl-\n",
      "edge/belief about how the \\family size\", \\walkable\", and \\school quality\" are\n",
      "determined by the inputs. We implicitly assumed that we know the family\n",
      "size is an important quantity to look at and that it can be determined by\n",
      "only the \\size\" and \\# bedrooms\". Such a prior knowledge might not be\n",
      "exible and general to haveations. It would be more \n",
      "a generic parameterization. A simple way would be to write the intermediate\n",
      "variablea1as a function of all x1;:::;x 4:\n",
      "a1= ReLU(w>\n",
      "1x+b1);wherew12R4andb12R (7.14)\n",
      "a2= ReLU(w>\n",
      "2x+b2);wherew22R4andb22R\n",
      "a3= ReLU(w>\n",
      "3x+b3);wherew32R4andb32R\n",
      "We still de\f",
      "ne \u0016h\u0012(x) using equation (7.13) with a1;a2;a3being de\f",
      "ned as\n",
      "above. Thus we have a so-called fully-connected neural network because\n",
      "all the intermediate variables ai's depend on all the inputs xi's.\n",
      "For full generality, a two-layer fully-connected neural network with m\n",
      "hidden units and ddimensional input x2Rdis de\f",
      "ned as\n",
      "8j2[1;:::;m ]; zj=w[1]\n",
      "j>x+b[1]\n",
      "jwherew[1]\n",
      "j2Rd;b[1]\n",
      "j2R (7.15)\n",
      "aj= ReLU(zj);\n",
      "a= [a1;:::;am]>2Rm\n",
      "\u0016h\u0012(x) =w[2]>a+b[2]wherew[2]2Rm;b[2]2R; (7.16)\n",
      "Note that by default the vectors in Rdare viewed as column vectors, and\n",
      "in particular ais a column vector with components a1;a2;:::;am. The indices\n",
      "[1]and[2]are used to distinguish two sets of parameters: the w[1]\n",
      "j's (each of\n",
      "which is a vector in Rd) andw[2](which is a vector in Rm). We will have\n",
      "more of these later.\n",
      "Vectorization. Before we introduce neural networks with more layers and\n",
      "more complex structures, we will simplify the expressions for neural networks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "88\n",
      "with more matrix and vector notations. Another important motivation of\n",
      "vectorization is the speed perspective in the implementation. In order to\n",
      "implement a neural network e\u000eciently, one must be careful when using for\n",
      "loops. The most natural way to implement equation (7.15) in code is perhaps\n",
      "to use a for loop. In practice, the dimensionalities of the inputs and hidden\n",
      "units are high. As a result, code will run very slowly if you use for loops.\n",
      "Leveraging the parallelism in GPUs is/was crucial for the progress of deep\n",
      "learning.\n",
      "This gave rise to vectorization . Instead of using for loops, vectorization\n",
      "takes advantage of matrix algebra and highly optimized numerical linear\n",
      "algebra packages (e.g., BLAS) to make neural network computations run\n",
      "quickly. Before the deep learning era, a for loop may have been su\u000ecient\n",
      "on smaller datasets, but modern deep networks and state-of-the-art datasets\n",
      "will be infeasible to run with for loops.\n",
      "We vectorize the two-layer fully-connected neural network as below. We\n",
      "de\f",
      "ne a weight matrix W[1]inRm\u0002das the concatenation of all the vectors\n",
      "w[1]\n",
      "j's in the following way:\n",
      "W[1]=2\n",
      "66664|w[1]\n",
      "1>|\n",
      "|w[1]\n",
      "2>|\n",
      "...\n",
      "|w[1]\n",
      "m>|3\n",
      "777752Rm\u0002d(7.17)\n",
      "Now by the de\f",
      "nition of matrix vector multiplication, we can write z=\n",
      "[z1;:::;zm]>2Rmas\n",
      "2\n",
      "6664z1\n",
      "...\n",
      "...\n",
      "zm3\n",
      "7775\n",
      "|{z}\n",
      "z2Rm\u00021=2\n",
      "66664|w[1]\n",
      "1>|\n",
      "|w[1]\n",
      "2>|\n",
      "...\n",
      "|w[1]\n",
      "m>|3\n",
      "77775\n",
      "|{z}\n",
      "W[1]2Rm\u0002d2\n",
      "6664x1\n",
      "x2\n",
      "...\n",
      "xd3\n",
      "7775\n",
      "|{z}\n",
      "x2Rd\u00021+2\n",
      "6664b[1]\n",
      "1\n",
      "b[1]\n",
      "2...\n",
      "b[1]\n",
      "m3\n",
      "7775\n",
      "|{z}\n",
      "b[1]2Rm\u00021(7.18)\n",
      "Or succinctly,\n",
      "z=W[1]x+b[1](7.19)\n",
      "We remark again that a vector in Rdin this notes, following the conventions\n",
      "previously established, is automatically viewed as a column vector, and can\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "89\n",
      "also be viewed as a d\u00021 dimensional matrix. (Note that this is di\u000b",
      "erent\n",
      "from numpy where a vector is viewed as a row vector in broadcasting.)\n",
      "Computing the activations a2Rmfromz2Rminvolves an element-\n",
      "wise non-linear application of the ReLU function, which can be computed in\n",
      "parallel e\u000eciently. Overloading ReLU for element-wise application of ReLU\n",
      "(meaning, for a vector t2Rd, ReLU(t) is a vector such that ReLU( t)i=\n",
      "ReLU(ti)), we have\n",
      "a= ReLU(z) (7.20)\n",
      "De\f",
      "neW[2]= [w[2]>]2R1\u0002msimilarly. Then, the model in equa-\n",
      "tion (7.16) can be summarized as\n",
      "a= ReLU(W[1]x+b[1])\n",
      "\u0016h\u0012(x) =W[2]a+b[2](7.21)\n",
      "Here\u0012consists of W[1];W[2](often referred to as the weight matrices) and\n",
      "b[1];b[2](referred to as the biases). The collection of W[1];b[1]is referred to as\n",
      "the \f",
      "rst layer, and W[2];b[2]the second layer. The activation ais referred to as\n",
      "the hidden layer. A two-layer neural network is also called one-hidden-layer\n",
      "neural network.\n",
      "Multi-layer fully-connected neural networks. With this succinct no-\n",
      "tations, we can stack more layers to get a deeper fully-connected neu-\n",
      "ral network. Let rbe the number of layers (weight matrices). Let\n",
      "W[1];:::;W[r];b[1];:::;b[r]be the weight matrices and biases of all the layers.\n",
      "Then a multi-layer neural network can be written as\n",
      "a[1]= ReLU(W[1]x+b[1])\n",
      "a[2]= ReLU(W[2]a[1]+b[2])\n",
      "\u0001\u0001\u0001\n",
      "a[r\u00001]= ReLU(W[r\u00001]a[r\u00002]+b[r\u00001])\n",
      "\u0016h\u0012(x) =W[r]a[r\u00001]+b[r](7.22)\n",
      "We note that the weight matrices and biases need to have compatible\n",
      "dimensions for the equations above to make sense. If a[k]has dimension mk,\n",
      "then the weight matrix W[k]should be of dimension mk\u0002mk\u00001, and the bias\n",
      "b[k]2Rmk. Moreover, W[1]2Rm1\u0002dandW[r]2R1\u0002mr\u00001.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "90\n",
      "The total number of neurons in the network is m1+\u0001\u0001\u0001+mr, and the\n",
      "total number of parameters in this network is ( d+ 1)m1+ (m1+ 1)m2+\u0001\u0001\u0001+\n",
      "(mr\u00001+ 1)mr.\n",
      "Sometimes for notational consistency we also write a[0]=x, anda[r]=\n",
      "h\u0012(x). Then we have simple recursion that\n",
      "a[k]= ReLU(W[k]a[k\u00001]+b[k]);8k= 1;:::;r\u00001 (7.23)\n",
      "Note that this would have be true for k=rif there were an additional\n",
      "ReLU in equation (7.22), but often people like to make the last layer linear\n",
      "(aka without a ReLU) so that negative outputs are possible and it's easier\n",
      "to interpret the last layer as a linear model. (More on the interpretability at\n",
      "the \\connection to kernel method\" paragraph of this section.)\n",
      "Other activation functions. The activation function ReLU can be re-\n",
      "placed by many other non-linear function \u001b(\u0001) that maps RtoRsuch as\n",
      "\u001b(z) =1\n",
      "1 +e\u0000z(sigmoid) (7.24)\n",
      "\u001b(z) =ez\u0000e\u0000z\n",
      "ez+e\u0000z(tanh) (7.25)\n",
      "2(0;1) (leaky ReLU) (7.26)\n",
      "\u001b(z) =z\n",
      "2\u0014\n",
      "1 + erf(zp\n",
      "2)\u0015\n",
      "(GELU) (7.27)\n",
      "\u001b(z) =1\n",
      "\f",
      "log(1 + exp( \f",
      "z));\f",
      " > 0 (Softplus) (7.28)\n",
      "The activation functions are plotted in Figure 7.3. Sigmoid and tanh are\n",
      "less and less used these days partly because their are bounded from both sides\n",
      "and the gradient of them vanishes as zgoes to both positive and negative\n",
      "in\f",
      "nity (whereas all the other activation functions still have gradients as the\n",
      "input goes to positive in\f",
      "nity.) Softplus is not used very often either in\n",
      "practice and can be viewed as a smoothing of the ReLU so that it has a\n",
      "proper second order derivative. GELU and leaky ReLU are both variants of\n",
      "ReLU but they have some non-zero gradient even when the input is negative.\n",
      "GELU (or its slight variant) is used in NLP models such as BERT and GPT\n",
      "(which we will discuss in Chapter 14.)\n",
      "Why do we not use the identity function for \u001b(z)?That is, why\n",
      "not use\u001b(z) =z? Assume for sake of argument that b[1]andb[2]are zeros.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "91\n",
      "Figure 7.3: Activation functions in deep learning.\n",
      "Suppose\u001b(z) =z, then for two-layer neural network, we have that\n",
      "\u0016h\u0012(x) =W[2]a[1](7.29)\n",
      "=W[2]\u001b(z[1]) by de\f",
      "nition (7.30)\n",
      "=W[2]z[1]since\u001b(z) =z (7.31)\n",
      "=W[2]W[1]x from Equation (7.18) (7.32)\n",
      "=~Wx where ~W=W[2]W[1](7.33)\n",
      "Notice how W[2]W[1]collapsed into ~W.\n",
      "This is because applying a linear function to another linear function will\n",
      "result in a linear function over the original input (i.e., you can construct a ~W\n",
      "such that ~Wx=W[2]W[1]x). This loses much of the representational power\n",
      "of the neural network as often times the output we are trying to predict\n",
      "has a non-linear relationship with the inputs. Without non-linear activation\n",
      "functions, the neural network will simply perform linear regression.\n",
      "Connection to the Kernel Method. In the previous lectures, we covered\n",
      "the concept of feature maps. Recall that the main motivation for feature\n",
      "maps is to represent functions that are non-linear in the input xby\u0012>\u001e",
      "(x),\n",
      "where\u0012are the parameters and \u001e",
      "(x), the feature map, is a handcrafted\n",
      "function non-linear in the raw input x. The performance of the learning\n",
      "algorithms can signi\f",
      "cantly depends on the choice of the feature map \u001e",
      "(x).\n",
      "Oftentimes people use domain knowledge to design the feature map \u001e",
      "(x) that\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "92\n",
      "suits the particular applications. The process of choosing the feature maps\n",
      "is often referred to as feature engineering .\n",
      "We can view deep learning as a way to automatically learn the right\n",
      "feature map (sometimes also referred to as \\the representation\") as follows.\n",
      "Suppose we denote by \f",
      "the collection of the parameters in a fully-connected\n",
      "neural networks (equation (7.22)) except those in the last layer. Then we\n",
      "can abstract right a[r\u00001]as a function of the input xand the parameters in\n",
      "\f",
      ":a[r\u00001]=\u001e",
      "\f",
      "(x). Now we can write the model as\n",
      "\u0016h\u0012(x) =W[r]\u001e",
      "\f",
      "(x) +b[r](7.34)\n",
      "When\f",
      "is \f",
      "xed, then \u001e",
      "\f",
      "(\u0001) can viewed as a feature map, and therefore \u0016h\u0012(x)\n",
      "is just a linear model over the features \u001e",
      "\f",
      "(x). However, we will train the\n",
      "neural networks, both the parameters in \f",
      "and the parameters W[r];b[r]are\n",
      "optimized, and therefore we are not learning a linear model in the feature\n",
      "space, but also learning a good feature map \u001e",
      "\f",
      "(\u0001) itself so that it's possi-\n",
      "ble to predict accurately with a linear model on top of the feature map.\n",
      "Therefore, deep learning tends to depend less on the domain knowledge of\n",
      "the particular applications and requires often less feature engineering. The\n",
      "penultimate layer a[r]is often (informally) referred to as the learned features\n",
      "or representations in the context of deep learning.\n",
      "In the example of house price prediction, a fully-connected neural network\n",
      "does not need us to specify the intermediate quantity such \\family size\", and\n",
      "may automatically discover some useful features in the last penultimate layer\n",
      "(the activation a[r\u00001]), and use them to linearly predict the housing price.\n",
      "Often the feature map / representation obtained from one datasets (that is,\n",
      "the function \u001e",
      "\f",
      "(\u0001) can be also useful for other datasets, which indicates they\n",
      "contain essential information about the data. However, oftentimes, the neural\n",
      "network will discover complex features which are very useful for predicting\n",
      "the output but may be di\u000ecult for a human to understand or interpret. This\n",
      "is why some people refer to neural networks as a black box , as it can be\n",
      "di\u000ecult to understand the features it has discovered.\n",
      "7.3 Modules in Modern Neural Networks\n",
      "The multi-layer neural network introduced in equation (7.22) of Section 7.2\n",
      "is often called multi-layer perceptron (MLP) these days. Modern neural net-\n",
      "works used in practice are often much more complex and consist of multiple\n",
      "building blocks or multiple layers of building blocks. In this section, we will\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "93\n",
      "introduce some of the other building blocks and discuss possible ways to\n",
      "combine them.\n",
      "First, each matrix multiplication can be viewed as a building block. Con-\n",
      "sider a matrix multiplication operation with parameters ( W;b) whereWis\n",
      "the weight matrix and bis the bias vector, operating on an input z,\n",
      "MMW;b(z) =Wz+b: (7.35)\n",
      "Note that we implicitly assume all the dimensions are chosen to be compat-\n",
      "ible. We will also drop the subscripts under MM when they are clear in the\n",
      "context or just for convenience when they are not essential to the discussion.\n",
      "Then, the MLP can be written as as a composition of multiple matrix\n",
      "multiplication modules and nonlinear activation modules (which can also be\n",
      "viewed as a building block):\n",
      "MLP(x) = MMW[r];b[r](\u001b(MMW[r\u00001];b[r\u00001](\u001b(\u0001\u0001\u0001MMW[1];b[1](x)))):(7.36)\n",
      "Alternatively, when we drop the subscripts that indicate the parameters for\n",
      "convenience, we can write\n",
      "MLP(x) = MM(\u001b(MM\u001b(\u0001\u0001\u0001MM(x)))): (7.37)\n",
      "Note that in this lecture notes, by default, all the modules have di\u000b",
      "erent\n",
      "sets of parameters, and the dimensions of the parameters are chosen such\n",
      "that the composition is meaningful.\n",
      "Larger modules can be de\f",
      "ned via smaller modules as well, e.g., one\n",
      "activation layer \u001band a matrix multiplication layer MM are often combined\n",
      "and called a \\layer\" in many papers. People often draw the architecture\n",
      "with the basic modules in a \f",
      "gure by indicating the dependency between\n",
      "these modules. E.g., see an illustration of an MLP in Figure 7.4, Left.\n",
      "uential neural network archi-the very in\n",
      "tecture for vision application is ResNet, which uses the residual connections\n",
      "that are essentially used in almost all large-scale deep learning architectures\n",
      "these days. Using our notation above, a very much simpli\f",
      "ed residual block\n",
      "can be de\f",
      "ned as\n",
      "Res(z) =z+\u001b(MM(\u001b(MM(z)))): (7.38)\n",
      "A much simpli\f",
      "ed ResNet is a composition of many residual blocks followed\n",
      "by a matrix multiplication,\n",
      "ResNet-S(x) = MM(Res(Res( \u0001\u0001\u0001Res(x)))): (7.39)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "94\n",
      "𝑥Layer 𝑟−1Layer 𝑖...Layer 1MLP(𝑥)...Layer𝑖MM![\"],#[\"]𝜎MM![$],#[$]\n",
      "𝑥ResRes...ResResNet-S(𝑥)...Res\n",
      "MM𝜎MM𝜎\n",
      "Figure 7.4: Illustrative Figures for Architecture. Left: An MLP with r\n",
      "layers. Right : A residual network.\n",
      "We also draw the dependency of these modules in Figure 7.4, Right.\n",
      "We note that the ResNet-S is still not the same as the ResNet architec-\n",
      "ture introduced in the seminal paper [He et al., 2016] because ResNet uses\n",
      "convolution layers instead of vanilla matrix multiplication, and adds batch\n",
      "normalization between convolutions and activations. We will introduce con-\n",
      "volutional layers and some variants of batch normalization below. ResNet-S\n",
      "and layer normalization are part of the Transformer architecture that are\n",
      "widely used in modern large language models.\n",
      "Layer normalization. Layer normalization, denoted by LN in this text,\n",
      "is a module that maps a vector z2Rmto a more normalized vector LN( z)2\n",
      "Rm. It is oftentimes used after the nonlinear activations.\n",
      "We \f",
      "rst de\f",
      "ne a sub-module of the layer normalization, denoted by LN-S.\n",
      "LN-S(z) =2\n",
      "6664z1\u0000^\u0016\n",
      "^\u001bz2\u0000^\u0016\n",
      "^\u001b...\n",
      "zm\u0000^\u0016\n",
      "^\u001b3\n",
      "7775; (7.40)\n",
      "where ^\u0016=Pm\n",
      "i=1zi\n",
      "mis the empirical mean of the vector zand ^\u001b=qPm\n",
      "i=1(zi\u0000^\u00162)\n",
      "m\n",
      "is the empirical standard deviation of the entries of z.4Intuitively, LN-S( z)\n",
      "is a vector that is normalized to having empirical mean zero and empirical\n",
      "standard deviation 1.\n",
      "4Note that we divide by minstead ofm\u00001 in the empirical standard deviation here\n",
      "because we are interested in making the output of LN-S( z) have sum of squares equal to\n",
      "1 (as opposed to estimating the standard deviation in statistics.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "95\n",
      "Oftentimes zero mean and standard deviation 1 is not the most desired\n",
      "normalization scheme, and thus layernorm introduces to parameters learnable\n",
      "as the desired mean and standard deviation, and use an a\u000ene\n",
      "transformation to turn the output of LN-S( z) into a vector with mean \f",
      "and\n",
      ".tandard deviation \n",
      "\u0001LN-S(z) =2\n",
      "\u0000z1\u0000^\u0016\n",
      "^\u001b\u0001\n",
      "\u0000z2\u0000^\u0016\n",
      "^\u001b\u0001\n",
      "...\n",
      "\u0000zm\u0000^\u0016\n",
      "^\u001b\u00013\n",
      "7775: (7.41)\n",
      "Here the \f",
      "rst occurrence of \f",
      "should be technically interpreted as a vector\n",
      "with all the entries being \f",
      ". in We also note that ^ \u0016and ^\u001bare also functions\n",
      "ofzand shouldn't be treated as constants when computing the derivatives of\n",
      "are learnable parameters and thus layernorm\n",
      "is a parameterized module (as opposed to the activation layer which doesn't\n",
      "have any parameters.)\n",
      "Scaling-invariant property. One important property of layer normalization\n",
      "is that it will make the model invariant to scaling of the parameters in the\n",
      "following sense. Suppose we consider composing LN with MM W;band get\n",
      "a subnetwork LN(MM W;b(z)). Then, we have that the output of this sub-\n",
      "network does not change when the parameter in MM W;bis scaled:\n",
      "LN(MM \u000b",
      "W;\u000b",
      "b (z)) = LN(MM W;b(z));8\u000b",
      ">0: (7.42)\n",
      "To see this, we \f",
      "rst know that LN-S( \u0001) is scale-invariant\n",
      "LN-S(\u000b",
      "z) =2\n",
      "6664\u000b",
      "z1\u0000\u000b",
      "^\u0016\n",
      "\u000b",
      "^\u001b\u000b",
      "z2\u0000\u000b",
      "^\u0016\n",
      "\u000b",
      "^\u001b...\n",
      "\u000b",
      "zm\u0000\u000b",
      "^\u0016\n",
      "\u000b",
      "^\u001b3\n",
      "7775=2\n",
      "6664z1\u0000^\u0016\n",
      "^\u001bz2\u0000^\u0016\n",
      "^\u001b...\n",
      "zm\u0000^\u0016\n",
      "^\u001b3\n",
      "7775= LN-S(z): (7.43)\n",
      "Then we have\n",
      "LN-S(MM \u000b",
      "W;\u000b",
      "b (z)) (7.44)\n",
      "LN-S(\u000b",
      "MMW;b(z)) (7.45)\n",
      "LN-S(MM W;b(z)) (7.46)\n",
      "= LN(MM W;b(z)): (7.47)\n",
      "Due to this property, most of the modern DL architectures for large-scale\n",
      "computer vision and language applications have the following scale-invariant\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "96\n",
      "property w.r.t all the weights that are not at the last layer. Suppose the\n",
      "networkfhas last layer' weights Wlast, and all the rest of the weights are\n",
      "denote byW. Then, we have fWlast;\u000b",
      "W(x) =fWlast;W(x) for all\u000b",
      ">0. Here,\n",
      "the last layers weights are special because there are typically no layernorm\n",
      "or batchnorm after the last layer's weights.\n",
      "Other normalization layers. There are several other normalization layers that\n",
      "aim to normalize the intermediate layers of the neural networks to a more\n",
      "\f",
      "xed and controllable scaling, such as batch-normalization [Io\u000b",
      "e and Szegedy,\n",
      "2015], and group normalization [Wu and He, 2018]. Batch normalization and\n",
      "group normalization are more often used in computer vision applications\n",
      "whereas layer norm is used more often in language applications.\n",
      "Convolutional Layers. Convolutional Neural Networks are neural net-\n",
      "works that consist of convolution layers (and many other modules), and are\n",
      "particularly useful for computer vision applications. For the simplicity of\n",
      "y mentionn, we focus on 1-D convolution in this text and only brie\n",
      "2-D convolution informally at the end of this subsection. (2-D convolution\n",
      "is more suitable for images which have two dimensions. 1-D convolution is\n",
      "also used in natural language processing.)\n",
      "We start by introducing a simpli\f",
      "ed version of the 1-D convolution layer,\n",
      "denoted by Conv1D-S( \u0001) which is a type of matrix multiplication layer with\n",
      "a special structure. The parameters of Conv1D-S are a \f",
      "lter vector w2Rk\n",
      "wherekis called the \f",
      "lter size (oftentimes k\u001c",
      "m), and a bias scalar b.\n",
      "Oftentimes the \f",
      "lter is also called a kernel (but it does not have much to do\n",
      "with the kernel in kernel method.) For simplicity, we assume k= 2`+ 1 is\n",
      "an odd number. We \f",
      "rst pad zeros to the input vector zin the sense that we\n",
      "letz1\u0000`=z1\u0000`+1=::=z0= 0 andzm+1=zm+2=::=zm+`= 0, and treat\n",
      "zas an (m+ 2`)-dimension vector. Conv1D-S outputs a vector of dimension\n",
      "Rmwhere each output dimension is a linear combination of subsets of zj's\n",
      "with coe\u000ecients from w,\n",
      "Conv1D-S(z)i=w1zi\u0000`+w2zi\u0000`+1+\u0001\u0001\u0001+w2`+1zi+`=2`+1X\n",
      "j=1wjzi\u0000`+(j\u00001):\n",
      "(7.48)\n",
      "Therefore, one can view Conv1D-S as a matrix multiplication with shared\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "97\n",
      "parameters: Conv1D-S( z) =Qz, where\n",
      "Q=2\n",
      "666666666666666666664w`+1\u0001\u0001\u0001w2`+1 0 0\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 0\n",
      "w`\u0001\u0001\u0001w2`w2`+1 0\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 0\n",
      "...\n",
      "w1\u0001\u0001\u0001w`+1\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 w2`+1 0\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 0\n",
      "0w1\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 w2`w2`+1 0\u0001\u0001\u0001 \u0001\u0001\u0001 0\n",
      "...\n",
      "...\n",
      "0\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 0w1\u0001\u0001\u0001 \u0001\u0001\u0001 w2`+1\n",
      "...\n",
      "0\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 0w1\u0001\u0001\u0001w`+13\n",
      "777777777777777777775:(7.49)\n",
      "Note thatQi;j=Qi\u00001;j\u00001for alli;j2f2;:::;mg, and thus convoluation is a\n",
      "matrix multiplication with parameter sharing. We also note that computing\n",
      "the convolution only takes O(km) times but computing a generic matrix\n",
      "multiplication takes O(m2) time. Convolution has kparameters but generic\n",
      "matrix multiplication will have m2parameters. Thus convolution is supposed\n",
      "to be much more e\u000ecient than a generic matrix multiplication (as long as\n",
      "exibility of the modelre imposed does not hurt the \n",
      "to \f",
      "t the data).\n",
      "We also note that in practice there are many variants of the convolutional\n",
      "layers that we de\f",
      "ne here, e.g., there are other ways to pad zeros or sometimes\n",
      "the dimension of the output of the convolutional layers could be di\u000b",
      "erent from\n",
      "the input. We omit some of this subtleties here for simplicity.\n",
      "The convolutional layers used in practice have also many \\channels\" and\n",
      "the simpli\f",
      "ed version above corresponds to the 1-channel version. Formally,\n",
      "Conv1D takes in Cvectorsz1;:::;zC2Rmas inputs, where Cis referred\n",
      "to as the number of channels. In other words, the more general version,\n",
      "denoted by Conv1D, takes in a matrix as input, which is the concatenation\n",
      "ofz1;:::;zCand has dimension m\u0002C. It can output C0vectors of dimension\n",
      "m, denoted by Conv1D( z)1;:::; Conv1D(z)C0, whereC0is referred to as the\n",
      "output channel, or equivalently a matrix of dimension m\u0002C0. Each of the\n",
      "output is a sum of the simpli\f",
      "ed convolutions applied on various channels.\n",
      "8i2[C0];Conv1D(z)i=CX\n",
      "j=1Conv1D-S i;j(zj): (7.50)\n",
      "Note that each Conv1D-S i;jare modules with di\u000b",
      "erent parameters, and\n",
      "thus the total number of parameters is k(the number of parameters in a\n",
      "Conv1D-S)\u0002CC0(the number of Conv1D-S i:j's) =kCC0. In contrast, a\n",
      "generic linear mapping from Rm\u0002CandRm\u0002C0hasm2CC0parameters. The\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "98\n",
      "parameters can also be represented as a three-dimensional tensor of dimen-\n",
      "sionk\u0002C\u0002C0.\n",
      "2-D convolution (brief). A 2-D convolution with one channel, denoted by\n",
      "Conv2D-S, is analogous to the Conv1D-S, but takes a 2-dimensional input\n",
      "z2Rm\u0002mand applies a \f",
      "lter of size k\u0002k, and outputs Conv2D-S( z)2\n",
      "Rm\u0002m. The full 2-D convolutional layer, denoted by Conv2D, takes in\n",
      "a sequence of matrices z1;:::;zC2Rm\u0002m, or equivalently a 3-D ten-\n",
      "sorz= (z1;:::;zC)2Rm\u0002m\u0002Cand outputs a sequence of matrices,\n",
      "Conv2D(z)1;:::; Conv2D(z)C02Rm\u0002m, which can also be viewed as a 3D\n",
      "tensor in Rm\u0002m\u0002C0. Each channel of the output is sum of the outcomes of\n",
      "applying Conv2D-S layers on all the input channels.\n",
      "8i2[C0];Conv2D(z)i=CX\n",
      "j=1Conv2D-S i;j(zj): (7.51)\n",
      "Because there are CC0number of Conv2D-S modules and each of the\n",
      "Conv2D-S module has k2parameters, the total number of parameters is\n",
      "CC0k2. The parameters can also be viewed as a 4D tensor of dimension\n",
      "C\u0002C0\u0002k\u0002k.\n",
      "7.4 Backpropagation\n",
      "In this section, we introduce backpropgation or auto-di\u000b",
      "erentiation, which\n",
      "computes the gradient of the loss rJ(\u0012) e\u000eciently. We will start with an\n",
      "informal theorem that states that as long as a real-valued function fcan be\n",
      "e\u000eciently computed/evaluated by a di\u000b",
      "erentiable network or circuit, then its\n",
      "gradient can be e\u000eciently computed in a similar time. We will then show\n",
      "how to do this concretely for neural networks.\n",
      "Because the formality of the general theorem is not the main focus here,\n",
      "we will introduce the terms with informal de\f",
      "nitions. By a di\u000b",
      "erentiable\n",
      "circuit or a di\u000b",
      "erentiable network, we mean a composition of a sequence of\n",
      "di\u000b",
      "erentiable arithmetic operations (additions, subtraction, multiplication,\n",
      "divisions, etc) and elementary di\u000b",
      "erentiable functions (ReLU, exp, log, sin,\n",
      "cos, etc.). Let the size of the circuit be the total number of such operations\n",
      "and elementary functions. We assume that each of the operations and func-\n",
      "tions, and their derivatives or partial derivatives ecan be computed in O(1)\n",
      "time.\n",
      "Theorem 7.4.1: [backpropagation or auto-di\u000b",
      "erentiation, informally stated]\n",
      "Suppose a di\u000b",
      "erentiable circuit of size Ncomputes a real-valued function\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "99\n",
      "f:R`!R. Then, the gradient rfcan be computed in time O(N), by a\n",
      "circuit of size O(N).5\n",
      "We note that the loss function J(j)(\u0012) forj-th example can be indeed\n",
      "computed by a sequence of operations and functions involving additions,\n",
      "subtraction, multiplications, and non-linear activations. Thus the theorem\n",
      "suggests that we should be able to compute the rJ(j)(\u0012) in a similar time\n",
      "to that for computing J(j)(\u0012) itself. This does not only apply to the fully-\n",
      "connected neural network introduced in the Section 7.2, but also many other\n",
      "types of neural networks that uses more advance modules.\n",
      "We remark that auto-di\u000b",
      "erentiation or backpropagation is already imple-\n",
      "ow and pytorch, andeep learning packages such as tensor\n",
      "thus in practice, in most of cases a researcher does not need to write their\n",
      "backpropagation algorithms. However, understanding it is very helpful for\n",
      "gaining insights into the working of deep learning.\n",
      "Organization of the rest of the section. In Section 7.4.1, we will start review-\n",
      "ing the basic Chain rule with a new perspective that is particularly useful\n",
      "for understanding backpropgation. Section 7.4.2 will introduce the general\n",
      "strategy for backpropagation. Section 7.4.2 will discuss how to compute the\n",
      "so-called backward function for basic modules used in neural networks, and\n",
      "Section 7.4.4 will put everything together to get a concrete backprop algo-\n",
      "rithm for MLPs.\n",
      "7.4.1 Preliminaries on partial derivatives\n",
      "Suppose a scalar variable Jdepend on some variables z(which could be a\n",
      "scalar, matrix, or high-order tensor), we write@J\n",
      "@zas the partial derivatives\n",
      "ofJw.r.t to the variable z. We stress that the convention here is that@J\n",
      "@z\n",
      "has exactly the same dimension as zitself. For example, if z2Rm\u0002n, then\n",
      "@J\n",
      "@z2Rm\u0002n, and the (i;j)-entry of@J\n",
      "@zis equal to@J\n",
      "@zij.\n",
      "Remark 7.4.2: When both Jandzare not scalars, the partial derivatives of\n",
      "Jw.r.tzbecomes either a matrix or tensor and the notation becomes some-\n",
      "what tricky. Besides the mathematical or notational challenges in dealing\n",
      "5We note if the output of the function fdoes not depend on some of the input co-\n",
      "ordinates, then we set by default the gradient w.r.t that coordinate to zero. Setting to\n",
      "zero does not count towards the total runtime here in our accounting scheme. This is why\n",
      "whenN\u0014`, we can compute the gradient in O(N) time, which might be potentially even\n",
      "less than`.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100\n",
      "with these partial derivatives of multi-variate functions, they are also expen-\n",
      "sive to compute and store, and thus rarely explicitly constructed empirically.\n",
      "The experience of authors of this note is that it's generally more productive\n",
      "to think only about derivatives of scalar function w.r.t to vector, matrices,\n",
      "or tensors. For example, in this note, we will not deal with derivatives of\n",
      "multi-variate functions.\n",
      "Chain rule. We review the chain rule in calculus but with a perspective\n",
      "and notions that are more relevant for auto-di\u000b",
      "erentiation.\n",
      "Consider a scalar variable Jwhich is obtained by the composition of f\n",
      "andgon some variable z,\n",
      "z2Rm\n",
      "u=g(z)2Rn\n",
      "J=f(u)2R: (7.52)\n",
      "The same derivations below can be easily extend to the cases when zandu\n",
      "are matrices or tensors; but we insist that the \f",
      "nal variable Jis a scalar. (See\n",
      "also Remark 7.4.2.) Let u= (u1;:::;un) and letg(z) = (g1(z);\u0001\u0001\u0001;gn(z)):\n",
      "Then, the standard chain rule gives us that\n",
      "8i2f1;:::;mg;@J\n",
      "@zi=nX\n",
      "j=1@J\n",
      "@uj\u0001@gj\n",
      "@zi: (7.53)\n",
      "Alternatively, when zanduare both vectors, in a vectorized notation:\n",
      "@J\n",
      "@z=2\n",
      "64@g1\n",
      "@z1\u0001\u0001\u0001@gn\n",
      "@z1.........\n",
      "@g1\n",
      "@zm\u0001\u0001\u0001@gn\n",
      "@zm3\n",
      "75\u0001@J\n",
      "@u: (7.54)\n",
      "In other words, the backward function is always a linear map from@J\n",
      "@uto\n",
      "@J\n",
      "@z, though note that the mapping itself can depend on zin complex ways.\n",
      "The matrix on the RHS of (7.54) is actually the transpose of the Jacobian\n",
      "matrix of the function g. However, we do not discuss in-depth about Jacobian\n",
      "matrices to avoid complications. Part of the reason is that when zis a matrix\n",
      "atten zsor), to write an analog of equation (7.54), one has to either \n",
      "into a vector or introduce additional notations on tensor-matrix product. In\n",
      "this sense, equation (7.53) is more convenient and e\u000b",
      "ective to use in all cases.\n",
      "For example, when z2Rr\u0002sis a matrix, we can easily rewrite equation (7.53)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "101\n",
      "to\n",
      "8i;k;@J\n",
      "@zik=nX\n",
      "j=1@J\n",
      "@uj\u0001@gj\n",
      "@zik: (7.55)\n",
      "which will indeed be used in some of the derivations in Section 7.4.3.\n",
      "Key interpretation of the chain rule. We can view the formula above (equa-\n",
      "tion (7.53) or (7.54)) as a way to compute@J\n",
      "@zfrom@J\n",
      "@u. Consider the following\n",
      "abstract problem. Suppose Jdepends on zviauas de\f",
      "ned in equation (7.52).\n",
      "However, suppose the function fis not given or the function fis complex,\n",
      "but we are given the value of@J\n",
      "@u. Then, the formula in equation (7.54) gives\n",
      "us a way to compute@J\n",
      "@zfrom@J\n",
      "@u.\n",
      "@J\n",
      "@uchain rule, formula (7.54)= = = = = = = = = = = = = = = = = = = = )\n",
      "only requires info about g(\u0001) andz@J\n",
      "@z: (7.56)\n",
      "Moreover, this formula only involves knowledge about g(more precisely@gj\n",
      "@zi).\n",
      "We will repeatedly use this fact in situations where gis a building blocks of\n",
      "a complex network f.\n",
      "Empirically, it's often useful to modularized the mapping in (7.53) or\n",
      "(7.54) into a black-box, and mathematically it's also convenient to de\f",
      "ne a\n",
      "notation for it.6We useB[g;z] to de\f",
      "ne the function that maps@J\n",
      "@uto@J\n",
      "@z,\n",
      "and write\n",
      "@J\n",
      "@z=B[g;z]\u0012@J\n",
      "@u\u0013\n",
      ": (7.57)\n",
      "We callB[g;z] thebackward function for the module g. Note that when z\n",
      "is \f",
      "xed,B[g;z] is merely a linear map from RntoRm. Using equation (7.53),\n",
      "we have\n",
      "(B[g;z](v))i=mX\n",
      "j=1@gj\n",
      "@zi\u0001vj: (7.58)\n",
      "Or in vectorized notation, using (7.54), we have\n",
      "B[g;z](v) =2\n",
      "64@g1\n",
      "@z1\u0001\u0001\u0001@gn\n",
      "@z1.........\n",
      "@g1\n",
      "@zm\u0001\u0001\u0001@gn\n",
      "@zm3\n",
      "75\u0001v: (7.59)\n",
      "6e.g., the function is the .backward() method of the module in pytorch.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "102\n",
      "and thereforeB[g;z] can be viewed as a matrix. However, in reality, zwill be\n",
      "changing and thus the backward mapping has to be recomputed for di\u000b",
      "erent\n",
      "z's whilegis often \f",
      "xed. Thus, empirically, the backward function B[g;z](v)\n",
      "is often viewed as a function which takes in z(=the input to g) andv(=a\n",
      "vector that is supposed to be the gradient of some variable Jw.r.t to the\n",
      "output ofg) as the inputs, and outputs a vector that is supposed to be the\n",
      "gradient of Jw.r.t toz.\n",
      "7.4.2 General strategy of backpropagation\n",
      "We discuss the general strategy of auto-di\u000b",
      "erentiation in this section to build\n",
      "a high-level understanding. Then, we will instantiate the approach to con-\n",
      "crete neural networks. We take the viewpoint that neural networks are com-\n",
      "plex compositions of small building blocks such as MM, \u001b, Conv2D, LN,\n",
      "etc., de\f",
      "ned in Section 7.3. Note that the losses (e.g., mean-squared loss, or\n",
      "the cross-entropy loss) can also be abstractly viewed as additional modules.\n",
      "Thus, we can abstractly write the loss function J(on a single example ( x;y))\n",
      "as a composition of many modules:7\n",
      "J=Mk(Mk\u00001(\u0001\u0001\u0001M1(x))): (7.60)\n",
      "For example, for a binary classi\f",
      "cation problem with a MLP \u0016h\u0012(x) (de-\n",
      "\f",
      "ned in equation (7.36) and (7.37)), the loss function has ber written in the\n",
      "form of equation (7.60) with M1= MMW[1];b[1],M2=\u001b,M3= MMW[2];b[2],\n",
      ":::, andMk\u00001= MMW[r];b[r]andMk=`logistic .\n",
      "We can see from this example that some modules involve parameters, and\n",
      "other modules might only involve a \f",
      "xed set of operations. For generality,\n",
      "we assume that eachj Miinvolves a set of parameters \u0012[i], though\u0012[i]could\n",
      "possibly be an empty set when Miis a \f",
      "xed operation such as the nonlinear\n",
      "activations. We will discuss more on the granularity of the modularization,\n",
      "but so far we assume all the modules Mi's are simple enough.\n",
      "We introduce the intermediate variables for the computation in (7.60).\n",
      "7Technically, we should write J=Mk(Mk\u00001(\u0001\u0001\u0001M1(x));y). However, yis treated as a\n",
      "constant for the purpose of computing the derivatives w.r.t to the parameters, and thus\n",
      "we can view it as part of Mkfor the sake of simplicity of notations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "103\n",
      "Let\n",
      "u[0]=x\n",
      "u[1]=M1(u[0])\n",
      "u[2]=M2(u[1])\n",
      "...\n",
      "J=u[k]=Mk(u[k\u00001]): (F)\n",
      "Backpropgation consists of two passes, the forward pass and backward\n",
      "pass. In the forward pass, the algorithm simply computes u[1];:::;u[k]from\n",
      "i= 1;:::;k , sequentially using the de\f",
      "nition in (F), and save all the in-\n",
      "termediate variables u[i]'s in the memory.\n",
      "In the backward pass , we \f",
      "rst compute the derivatives w.r.t to the\n",
      "intermediate variables, that is,@J\n",
      "@u[k];:::;@J\n",
      "@u[1], sequentially in this backward\n",
      "order, and then compute the derivatives of the parameters@J\n",
      "@\u0012[i]from@J\n",
      "@u[i]and\n",
      "u[i\u00001]. These two type of computations can be also interleaved with each\n",
      "other because@J\n",
      "@\u0012[i]only depends on@J\n",
      "@u[i]andu[i\u00001]but not any@J\n",
      "@u[k]with\n",
      "k<i .\n",
      "We \f",
      "rst see why@J\n",
      "@u[i\u00001]can be computed e\u000eciently from@J\n",
      "@u[i]andu[i\u00001]\n",
      "by invoking the discussion in Section 7.4.1 on the chain rule. We in-\n",
      "stantiate the discussion by setting u=u[i]andz=u[i\u00001], andf(u) =\n",
      "Mk(Mk\u00001(\u0001\u0001\u0001Mi+1(u[i]))), andg(\u0001) =Mi(\u0001). Note that fis very complex\n",
      "but we don't need any concrete information about f. Then, the conclusive\n",
      "equation (7.56) corresponds to\n",
      "@J\n",
      "@u[i]chain rule= = = = = = = = = = = = = = = = = = = = = = = = = = )\n",
      "only requires info about Mi(\u0001)andu[i\u00001]@J\n",
      "@u[i\u00001]: (7.61)\n",
      "More precisely, we can write, following equation (7.57)\n",
      "@J\n",
      "@u[i\u00001]=B[Mi;u[i\u00001]]\u0012@J\n",
      "@u[i]\u0013\n",
      ": (B1)\n",
      "Instantiating the chain rule with z=\u0012[i]andu=u[i], we also have\n",
      "@J\n",
      "@\u0012[i]=B[Mi;\u0012[i]]\u0012@J\n",
      "@u[i]\u0013\n",
      ": (B2)\n",
      "See Figure 7.5 for an illustration of the algorithm.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "104\n",
      "𝑥...𝑀!𝐽...𝑀\"\n",
      "𝑢[!]𝑢[\"%!]𝜕𝐽𝜕𝐽ℬ[𝑀\",𝑢[\"%!]]𝜕𝐽𝜕𝑢!\"#𝑢[&]𝑀&𝑢[&%!]ℬ[𝑀&,𝑢[&%!]]𝜕𝐽𝜕𝑢$\"#𝜕𝐽𝜕𝑢$...\n",
      "𝜕𝐽𝜕𝑢#...\n",
      "Forward passBackward passℬ[𝑀!,𝜃!]𝜕𝐽𝜕𝜃#ℬ[𝑀&,𝜃&]𝜕𝐽𝜕𝜃$ℬ[𝑀\",𝜃\"]𝜕𝐽𝜕𝜃!\n",
      "Figure 7.5: Back-propagation.\n",
      "Remark 7.4.3: [Computational e\u000eciency and granularity of the modules]\n",
      "The main underlying purpose of treating a complex network as compositions\n",
      "of small modules is that small modules tend to have e\u000eciently implementable\n",
      "backward function. In fact, the backward functions of all the atomic modules\n",
      "such as addition, multiplication and ReLU can be computed as e\u000eciently as\n",
      "the the evaluation of these modules (up to multiplicative constant factor).\n",
      "Using this fact, we can prove Theorem 7.4.1 by viewing neural networks as\n",
      "compositions of many atomic operations, and invoking the backpropagation\n",
      "discussed above. However, in practice, it's oftentimes more convenient to\n",
      "modularize the networks using modules on the level of matrix multiplication,\n",
      "layernorm, etc. As we will see, naive implementation of these operations'\n",
      "backward functions also have the same runtime as the evaluation of these\n",
      "functions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "105\n",
      "7.4.3 Backward functions for basic modules\n",
      "Using the general strategy in Section 7.4.2, it su\u000eces to compute the back-\n",
      "ward function for all modules Mi's used in the networks. We compute the\n",
      "backward function for the basic module MM, activations \u001b, and loss functions\n",
      "in this section.\n",
      "Backward function for MM.Suppose MM W;b(z) =Wz+bis a matrix multi-\n",
      "plication module where z2RmandW2Rn\u0002m. Then, using equation (7.59),\n",
      "we have for v2Rn\n",
      "B[MM;z](v) =2\n",
      "64@(Wz+b)1\n",
      "@z1\u0001\u0001\u0001@(Wz+b)n\n",
      "@z1.........\n",
      "@(Wz+b)1\n",
      "@zm\u0001\u0001\u0001@(Wz+b)n\n",
      "@zm3\n",
      "75v: (7.62)\n",
      "Using the fact that 8i2[m];j2[n],@(Wz+b)j\n",
      "@zi=@bj+Pm\n",
      "k=1Wjkzk\n",
      "@zi=Wji, we\n",
      "have\n",
      "B[MM;z](v) =W>v2Rm: (7.63)\n",
      "In the derivation above, we have treated MM as a function of z. If we treat\n",
      "MM as a function of Wandb, then we can also compute the backward\n",
      "function for the parameter variables Wandb. It's less convenient to use\n",
      "equation (7.59) because the variable Wis a matrix and the matrix in (7.59)\n",
      "will be a 4-th order tensor that is challenging for us to mathematically write\n",
      "down. We use (7.58) instead:\n",
      "(B[MM;W](v))ij=mX\n",
      "k=1@(Wz+b)k\n",
      "@Wij\u0001vk=mX\n",
      "k=1@Pm\n",
      "s=1Wkszs\n",
      "@Wij\u0001vk=vizj:\n",
      "(7.64)\n",
      "In vectorized notation, we have\n",
      "B[MM;W](v) =vz>2Rn\u0002\u0002m: (7.65)\n",
      "Using equation (7.59) for the variable b, we have,\n",
      "B[MM;b](v) =2\n",
      "64@(Wz+b)1\n",
      "@b1\u0001\u0001\u0001@(Wz+b)n\n",
      "@b1.........\n",
      "@(Wz+b)1\n",
      "@bn\u0001\u0001\u0001@(Wz+b)n\n",
      "@bn3\n",
      "75v=v: (7.66)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "106\n",
      "Here we used that@(Wz+b)j\n",
      "@bi= 0 ifi6=jand@(Wz+b)j\n",
      "@bi= 1 ifi=j.\n",
      "The computational e\u000eciency for computing the backward function is\n",
      "O(mn), the same as evaluating the result of matrix multiplication up to\n",
      "constant factor.\n",
      "Backward function for the activations. SupposeM(z) =\u001b(z) where\u001bis an\n",
      "element-wise activation function and z2Rm. Then, using equation (7.59),\n",
      "we have\n",
      "B[\u001b;z](v) =2\n",
      "64@\u001b(z1)\n",
      "@z1\u0001\u0001\u0001@\u001b(zm)\n",
      "@z1.........\n",
      "@\u001b(z1)\n",
      "@zm\u0001\u0001\u0001@\u001b(zm)\n",
      "@zm3\n",
      "75v (7.67)\n",
      "= diag(\u001b0(z1);\u0001\u0001\u0001;\u001b0(zm))v (7.68)\n",
      "=\u001b0(z)\f",
      "v2Rm: (7.69)\n",
      "Here, we used the fact that@\u001b(zj)\n",
      "@zi= 0 whenj6=i, diag(\u00151;:::;\u0015m) denotes\n",
      "the diagonal matrix with \u00151;:::;\u0015mon the diagonal, and \f",
      "denotes the\n",
      "element-wise product of two vectors with the same dimension, and \u001b0(\u0001) is\n",
      "the element-wise application of the derivative of the activation function \u001b.\n",
      "Regarding computation e\u000eciency, we note that at the \f",
      "rst sight, equa-\n",
      "tion (7.67) appears to indicate the backward function takes O(m2) time, but\n",
      "equation (7.69) shows that it's implementable in O(m) time (which is the\n",
      "same as the time for evaluating of the function.) We are not supposed to be\n",
      "surprised by that the possibility of simplifying equation (7.67) to (7.69)|if\n",
      "we use smaller modules, that is, treating the vector-to-vector nonlinear ac-\n",
      "tivation as mscalar-to-scalar non-linear activation, then it's more obvious\n",
      "that the backward pass should have similar time to the forward pass.\n",
      "Backward function for loss functions. When a module Mtakes in a vector\n",
      "zand outputs a scalar, by equation (7.59), the backward function takes in a\n",
      "scalarvand outputs a vector with entries ( B[M;z](v))i=@M\n",
      "@ziv. Therefore,\n",
      "in vectorized notation, B[M;z](v) =@M\n",
      "@z\u0001v.\n",
      "Recall that squared loss `MSE(z;y) =1\n",
      "2(z\u0000y)2:Thus,B[`MSE;z](v) =\n",
      "@1\n",
      "2(z\u0000y)2\n",
      "@z\u0001v= (z\u0000y)\u0001v.\n",
      "For logistics loss, by equation (2.6), we have\n",
      "B[`logistic;t](v) =@`logistic (t;y)\n",
      "@t\u0001v= (1=(1 + exp(\u0000t))\u0000y)\u0001v: (7.70)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "107\n",
      "For cross-entropy loss, by equation (2.17), we have\n",
      "B[`ce;t](v) =@`ce(t;y)\n",
      "@t\u0001v= (\u001e",
      "\u0000ey)\u0001v; (7.71)\n",
      "where\u001e",
      "= softmax( t).\n",
      "7.4.4 Back-propagation for MLPs\n",
      "Given the backward functions for every module needed in evaluating the loss\n",
      "of an MLP, we follow the strategy in Section 7.4.2 to compute the gradient\n",
      "of the loss w.r.t to the hidden activations and the parameters.\n",
      "We consider the an r-layer MLP with a logistic loss. The loss function\n",
      "can be computed via a sequence of operations (that is, the forward pass),\n",
      "z[1]= MMW[1];b[1](x);\n",
      "a[1]=\u001b(z[1])\n",
      "z[2]= MMW[2];b[2](a[1])\n",
      "a[2]=\u001b(z[2])\n",
      "...\n",
      "z[r]= MMW[r];b[r](a[r\u00001])\n",
      "J=`logistic (z[r];y): (7.72)\n",
      "We apply the backward function sequentially in a backward order. First, we\n",
      "have that\n",
      "@J\n",
      "@z[r]=B[`logistic;z[r]]\u0012@J\n",
      "@J\u0013\n",
      "=B[`logistic;z[r]](1): (7.73)\n",
      "Then, we iteratively compute@J\n",
      "@a[i]and@J\n",
      "@z[i]'s by repeatedly invoking the chain\n",
      "rule (equation (7.58)),\n",
      "@J\n",
      "@a[r\u00001]=B[MM;a[r\u00001]]\u0012@J\n",
      "@z[r]\u0013\n",
      "@J\n",
      "@z[r\u00001]=B[\u001b;z[r\u00001]]\u0012@J\n",
      "@a[r\u00001]\u0013\n",
      "...\n",
      "@J\n",
      "@z[1]=B[\u001b;z[1]]\u0012@J\n",
      "@a[1]\u0013\n",
      ": (7.74)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "108\n",
      "Numerically, we compute these quantities by repeatedly invoking equa-\n",
      "tions (7.69) and (7.63) with di\u000b",
      "erent choices of variables.\n",
      "We note that the intermediate values of a[i]andz[i]are used in the back-\n",
      "propagation (equation (7.74)), and therefore these values need to be stored\n",
      "in the memory after the forward pass.\n",
      "Next, we compute the gradient of the parameters by invoking equa-\n",
      "tions (7.65) and (7.66),\n",
      "@J\n",
      "@W[r]=B[MM;W[r]]\u0012@J\n",
      "@z[r]\u0013\n",
      "@J\n",
      "@b[r]=B[MM;b[r]]\u0012@J\n",
      "@z[r]\u0013\n",
      "...\n",
      "@J\n",
      "@W[1]=B[MM;W[1]]\u0012@J\n",
      "@z[1]\u0013\n",
      "@J\n",
      "@b[1]=B[MM;b[1]]\u0012@J\n",
      "@z[1]\u0013\n",
      ": (7.75)\n",
      "We also note that the block of computations in equations (7.75) can be\n",
      "interleaved with the block of computation in equations (7.74) because the\n",
      "@J\n",
      "@W[i]and@J\n",
      "@b[i]can be computed as soon as@J\n",
      "@z[i]is computed.\n",
      "Putting all of these together, and explicitly invoking the equa-\n",
      "tions (7.72), (7.74) and (7.75), we have the following algorithm (Algorithm 3).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "109\n",
      "Algorithm 3 Back-propagation for multi-layer neural networks.\n",
      "1:Forward pass. Compute and store the values of a[k]'s,z[k]'s, andJ\n",
      "using the equations (7.72).\n",
      "2:Backward pass. Compute the gradient of loss Jwith respect to z[r]:\n",
      "@J\n",
      "@z[r]=B[`logistic;z[r]](1) =\u0000\n",
      "1=(1 + exp(\u0000z[r]))\u0000y\u0001\n",
      ": (7.76)\n",
      "3:fork=r\u00001 to 0 do\n",
      "4: Compute the gradient with respect to parameters W[k+1]andb[k+1].\n",
      "@J\n",
      "@W[k+1]=B[MM;W[k+1]]\u0012@J\n",
      "@z[k+1]\u0013\n",
      "=@J\n",
      "@z[k+1]a[k]>: (7.77)\n",
      "@J\n",
      "@b[k+1]=B[MM;b[k+1]]\u0012@J\n",
      "@z[k+1]\u0013\n",
      "=@J\n",
      "@z[k+1]: (7.78)\n",
      "5: Whenk\u00151, compute the gradient with respect to z[k]anda[k].\n",
      "@J\n",
      "@a[k]=B[\u001b;a[k]]\u0012@J\n",
      "@z[k+1]\u0013\n",
      "=W[k+1]>@J\n",
      "@z[k+1]: (7.79)\n",
      "@J\n",
      "@z[k]=B[\u001b;z[k]]\u0012@J\n",
      "@a[k]\u0013\n",
      "=\u001b0(z[k])\f",
      "@J\n",
      "@a[k]: (7.80)\n",
      "7.5 Vectorization over training examples\n",
      "As we discussed in Section 7.1, in the implementation of neural networks,\n",
      "we will leverage the parallelism across the multiple examples. This means\n",
      "that we will need to write the forward pass (the evaluation of the outputs)\n",
      "of the neural network and the backward pass (backpropagation) for multiple\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "110\n",
      "training examples in matrix notation.\n",
      "The basic idea. The basic idea is simple. Suppose you have a training\n",
      "set with three examples x(1);x(2);x(3). The \f",
      "rst-layer activations for each\n",
      "example are as follows:\n",
      "z[1](1)=W[1]x(1)+b[1]\n",
      "z[1](2)=W[1]x(2)+b[1]\n",
      "z[1](3)=W[1]x(3)+b[1]\n",
      "Note the di\u000b",
      "erence between square brackets [ \u0001], which refer to the layer num-\n",
      "ber, and parenthesis ( \u0001), which refer to the training example number. In-\n",
      "tuitively, one would implement this using a for loop. It turns out, we can\n",
      "vectorize these operations as well. First, de\f",
      "ne:\n",
      "X=2\n",
      "4j j j\n",
      "x(1)x(2)x(3)\n",
      "j j j3\n",
      "52Rd\u00023(7.81)\n",
      "Note that we are stacking training examples in columns and notrows. We\n",
      "can then combine this into a single uni\f",
      "ed formulation:\n",
      "Z[1]=2\n",
      "4j j j\n",
      "z[1](1)z[1](2)z[1](3)\n",
      "j j j3\n",
      "5=W[1]X+b[1](7.82)\n",
      "You may notice that we are attempting to add b[1]2R4\u00021toW[1]X2\n",
      "R4\u00023. Strictly following the rules of linear algebra, this is not allowed. In\n",
      "practice however, this addition is performed using broadcasting . We create\n",
      "an intermediate ~b[1]2R4\u00023:\n",
      "~b[1]=2\n",
      "4j j j\n",
      "b[1]b[1]b[1]\n",
      "j j j3\n",
      "5 (7.83)\n",
      "We can then perform the computation: Z[1]=W[1]X+~b[1]. Often times, it\n",
      "is not necessary to explicitly construct ~b[1]. By inspecting the dimensions in\n",
      "(7.82), you can assume b[1]2R4\u00021is correctly broadcast to W[1]X2R4\u00023.\n",
      "The matricization approach as above can easily generalize to multiple\n",
      "layers, with one subtlety though, as discussed below.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "111\n",
      "Complications/Subtlety in the Implementation. All the deep learn-\n",
      "ing packages or implementations put the data points in the rows of a data\n",
      "matrix. (If the data point itself is a matrix or tensor, then the data are con-\n",
      "centrated along the zero-th dimension.) However, most of the deep learning\n",
      "papers use a similar notation to these notes where the data points are treated\n",
      "as column vectors.8There is a simple conversion to deal with the mismatch:\n",
      "in the implementation, all the columns become row vectors, row vectors be-\n",
      "come column vectors, all the matrices are transposed, and the orders of the\n",
      "ipped. In the example above, using the row ma-\n",
      "jor convention, the data matrix is X2R3\u0002d, the \f",
      "rst layer weight matrix\n",
      "has dimensionality d\u0002m(instead of m\u0002das in the two layer neural net\n",
      "section), and the bias vector b[1]2R1\u0002m. The computation for the hidden\n",
      "activation becomes\n",
      "Z[1]=XW[1]+b[1]2R3\u0002m(7.84)\n",
      "8The instructor suspects that this is mostly because in mathematics we naturally mul-\n",
      "tiply a matrix to a vector on the left hand side.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Part III\n",
      "Generalization and\n",
      "regularization\n",
      "112\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 8\n",
      "Generalization\n",
      "This chapter discusses tools to analyze and understand the generaliza-\n",
      "tion of machine learning models, i.e, their performances on unseen test\n",
      "examples. Recall that for supervised learning problems, given a train-\n",
      "ing datasetf(x(i);y(i))gn\n",
      "i=1, we typically learn a model h\u0012by minimizing a\n",
      "loss/cost function J(\u0012), which encourages h\u0012to \f",
      "t the data. E.g., when\n",
      "the loss function is the least square loss (aka mean squared error), we have\n",
      "J(\u0012) =1\n",
      "nPn\n",
      "i=1(y(i)\u0000h\u0012(x(i)))2. This loss function for training purposes is\n",
      "oftentimes referred to as the training loss/error/cost.\n",
      "However, minimizing the training loss is not our ultimate goal|it is\n",
      "merely our approach towards the goal of learning a predictive model. The\n",
      "most important evaluation metric of a model is the loss on unseen test exam-\n",
      "ples, which is oftentimes referred to as the test error. Formally, we sample a\n",
      "test example ( x;y) from the so-called test distribution D, and measure the\n",
      "model's error on it, by, e.g., the mean squared error, ( h\u0012(x)\u0000y)2. The ex-\n",
      "pected loss/error over the randomness of the test example is called the test\n",
      "loss/error,1\n",
      "L(\u0012) =E(x;y)\u0018D[(y\u0000h\u0012(x))2] (8.1)\n",
      "Note that the measurement of the error involves computing the expectation,\n",
      "and in practice, it can be approximated by the average error on many sampled\n",
      "test examples, which are referred to as the test dataset. Note that the key\n",
      "di\u000b",
      "erence here between training and test datasets is that the test examples\n",
      "1In theoretical and statistical literature, we oftentimes call the uniform distribution\n",
      "over the training set f(x(i);y(i))gn\n",
      "i=1, denoted by bD, an empirical distribution, and call\n",
      "Dthe population distribution. Partly because of this, the training loss is also referred\n",
      "to as the empirical loss/risk/error, and the test loss is also referred to as the population\n",
      "loss/risk/error.\n",
      "113\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "114\n",
      "areunseen , in the sense that the training procedure has not used the test\n",
      "examples. In classical statistical learning settings, the training examples are\n",
      "also drawn from the same distribution as the test distribution D, but still\n",
      "the test examples are unseen by the learning procedure whereas the training\n",
      "examples are seen.2\n",
      "Because of this key di\u000b",
      "erence between training and test datasets, even\n",
      "if they are both drawn from the same distribution D, the test error is not\n",
      "necessarily always close to the training error.3As a result, successfully min-\n",
      "imizing the training error may not always lead to a small test error. We\n",
      "typically say the model over\f",
      "ts the data if the model predicts accurately on\n",
      "the training dataset but doesn't generalize well to other test examples, that\n",
      "is, if the training error is small but the test error is large. We say the model\n",
      "under\f",
      "ts the data if the training error is relatively large4(and in this case,\n",
      "typically the test error is also relatively large.)\n",
      "uenced by the learning pro-e test error is in\n",
      "cedure, especially the choice of model parameterizations. We will decompose\n",
      "the test error into \\bias\" and \\variance\" terms and study how each of them is\n",
      "a\u000b",
      "ected by the choice of model parameterizations and their tradeo\u000b",
      "s. Using\n",
      "the bias-variance tradeo\u000b",
      ", we will discuss when over\f",
      "tting and under\f",
      "tting\n",
      "will occur and be avoided. We will also discuss the double descent phe-\n",
      "nomenon in Section 8.2 and some classical theoretical results in Section 8.3.\n",
      "2These days, researchers have increasingly been more interested in the setting with\n",
      "\\domain shift\", that is, the training distribution and test distribution are di\u000b",
      "erent.\n",
      "3the di\u000b",
      "erence between test error and training error is often referred to as the gener-\n",
      "alization gap. The term generalization error in some literature means the test error, and\n",
      "in some other literature means the generalization gap.\n",
      "4e.g., larger than the intrinsic noise level of the data in regression problems.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "115\n",
      "8.1 Bias-variance tradeo\u000b",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining dataset\n",
      "training data\n",
      "ground truth h*\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytest dataset\n",
      "test data\n",
      "ground truth h*\n",
      "Figure 8.1: A running example of training and test dataset for this section.\n",
      "As an illustrating example, we consider the following training dataset and\n",
      "test dataset, which are also shown in Figure 8.1. The training inputs x(i)'s are\n",
      "randomly chosen and the outputs y(i)are generated by y(i)=h?(x(i)) +\u0018(i)\n",
      "where the function h?(\u0001) is a quadratic function and is shown in Figure 8.1\n",
      "as the solid line, and \u0018(i)is the a observation noise assumed to be generated\n",
      "from\u0018N(0;\u001b2). A test example ( x;y) also has the same input-output\n",
      "relationship y=h?(x) +\u0018where\u0018\u0018N(0;\u001b2). It's impossible to predict the\n",
      "noise\u0018, and therefore essentially our goal is to recover the function h?(\u0001).\n",
      "We will consider the test error of learning various types of models. When\n",
      "talking about linear regression, we discussed the problem of whether to \f",
      "t\n",
      "a \\simple\" model such as the linear \\ y=\u00120+\u00121x,\" or a more \\complex\"\n",
      "model such as the polynomial \\ y=\u00120+\u00121x+\u0001\u0001\u0001\u00125x5.\"\n",
      "We start with \f",
      "tting a linear model, as shown in Figure 8.2. The best\n",
      "\f",
      "tted linear model cannot predict yfromxaccurately even on the training\n",
      "dataset, let alone on the test dataset. This is because the true relationship\n",
      "betweenyandxis not linear|any linear model is far away from the true\n",
      "functionh?(\u0001). As a result, the training error is large and this is a typical\n",
      "situation of under\f",
      "tting .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "116\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining data\n",
      "best fit linear model\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytest data\n",
      "best fit linear model\n",
      "Figure 8.2: The best \f",
      "t linear model has large training and test errors.\n",
      "The issue cannot be mitigated with more training examples|even with\n",
      "a very large amount of, or even in\f",
      "nite training examples, the best \f",
      "tted\n",
      "linear model is still inaccurate and fails to capture the structure of the data\n",
      "(Figure 8.3). Even if the noise is not present in the training data, the issue\n",
      "still occurs (Figure 8.4). Therefore, the fundamental bottleneck here is the\n",
      "linear model family's inability to capture the structure in the data|linear\n",
      "models cannot represent the true quadratic function h?|, but not the lack of\n",
      "the data. Informally, we de\f",
      "ne the bias of a model to be the test error even\n",
      "if we were to \f",
      "t it to a very (say, in\f",
      "nitely) large training dataset. Thus, in\n",
      "this case, the linear model su\u000b",
      "ers from large bias, and under\f",
      "ts (i.e., fails to\n",
      "capture structure exhibited by) the data.\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5yfitting linear models on a large dataset\n",
      "training data\n",
      "ground truth h*\n",
      "best fit linear model\n",
      "Figure 8.3: The best \f",
      "t linear\n",
      "model on a much larger dataset\n",
      "still has a large training error.\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5yfitting linear models on a noiseless dataset\n",
      "training data\n",
      "ground truth h*\n",
      "best fit linear modelFigure 8.4: The best \f",
      "t linear\n",
      "model on a noiseless dataset also\n",
      "has a large training/test error.\n",
      "Next, we \f",
      "t a 5th-degree polynomial to the data. Figure 8.5 shows that\n",
      "it fails to learn a good model either. However, the failure pattern is di\u000b",
      "erent\n",
      "from the linear model case. Speci\f",
      "cally, even though the learnt 5th-degree\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "117\n",
      "polynomial did a very good job predicting y(i)'s fromx(i)'s for training ex-\n",
      "amples, it does not work well on test examples (Figure 8.5). In other words,\n",
      "the model learnt from the training set does not generalize well to other test\n",
      "examples|the test error is high. Contrary to the behavior of linear models,\n",
      "the bias of the 5-th degree polynomials is small|if we were to \f",
      "t a 5-th de-\n",
      "gree polynomial to an extremely large dataset, the resulting model would be\n",
      "close to a quadratic function and be accurate (Figure 8.6). This is because\n",
      "the family of 5-th degree polynomials contains all the quadratic functions\n",
      "(setting\u00125=\u00124=\u00123= 0 results in a quadratic function), and, therefore,\n",
      "5-th degree polynomials are in principle capable of capturing the structure\n",
      "of the data.\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining data\n",
      "best fit 5-th degree model\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytest data\n",
      "ground truth h*\n",
      "best fit 5-th degree model\n",
      "Figure 8.5: Best \f",
      "t 5-th degree polynomial has zero training error, but still\n",
      "has a large test error and does not recover the the ground truth. This is a\n",
      "classic situation of over\f",
      "tting.\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining data\n",
      "best fit 5-th degree model\n",
      "ground truth h*fitting 5-th degree model on large dataset\n",
      "Figure 8.6: The best \f",
      "t 5-th degree polynomial on a huge dataset nearly\n",
      "recovers the ground-truth|suggesting that the culprit in Figure 8.5 is the\n",
      "variance (or lack of data) but not bias.\n",
      "The failure of \f",
      "tting 5-th degree polynomials can be captured by another\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "118\n",
      "component of the test error, called variance of a model \f",
      "tting procedure.\n",
      "Speci\f",
      "cally, when \f",
      "tting a 5-th degree polynomial as in Figure 8.7, there is a\n",
      "large risk that we're \f",
      "tting patterns in the data that happened to be present\n",
      "ect the wider pattern ofning set, but that do not re\n",
      "the relationship between xandy. These \\spurious\" patterns in the training\n",
      "set are (mostly) due to the observation noise \u0018(i), and \f",
      "tting these spurious\n",
      "patters results in a model with large test error. In this case, we say the model\n",
      "has a large variance.\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining data\n",
      "best fit 5-th degree model\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining data\n",
      "best fit 5-th degree model\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining data\n",
      "best fit 5-th degree modelfitting 5-th degree model on different datasets\n",
      "Figure 8.7: The best \f",
      "t 5-th degree models on three di\u000b",
      "erent datasets gen-\n",
      "erated from the same distribution behave quite di\u000b",
      "erently, suggesting the\n",
      "existence of a large variance.\n",
      "The variance can be intuitively (and mathematically, as shown in Sec-\n",
      "tion 8.1.1) characterized by the amount of variations across models learnt\n",
      "on multiple di\u000b",
      "erent training datasets (drawn from the same underlying dis-\n",
      "tribution). The \\spurious patterns\" are speci\f",
      "c to the randomness of the\n",
      "noise (and inputs) in a particular dataset, and thus are di\u000b",
      "erent across mul-\n",
      "tiple training datasets. Therefore, over\f",
      "tting to the \\spurious patterns\" of\n",
      "multiple datasets should result in very di\u000b",
      "erent models. Indeed, as shown\n",
      "in Figure 8.7, the models learned on the three di\u000b",
      "erent training datasets are\n",
      "quite di\u000b",
      "erent, over\f",
      "tting to the \\spurious patterns\" of each datasets.\n",
      "Often, there is a tradeo\u000b",
      " between bias and variance. If our model is too\n",
      "\\simple\" and has very few parameters, then it may have large bias (but small\n",
      "variance), and it typically may su\u000b",
      "er from under\f",
      "ttng. If it is too \\complex\"\n",
      "and has very many parameters, then it may su\u000b",
      "er from large variance (but\n",
      "have smaller bias), and thus over\f",
      "tting. See Figure 8.8 for a typical tradeo\u000b",
      "\n",
      "between bias and variance.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "119\n",
      "Model ComplexityError\n",
      "Bias2VarianceTest Error (= Bias2+Variance) Optimal Tradeoff\n",
      "Figure 8.8: An illustration of the typical bias-variance tradeo\u000b",
      ".\n",
      "As we will see formally in Section 8.1.1, the test error can be decomposed\n",
      "as a summation of bias and variance. This means that the test error will\n",
      "have a convex curve as the model complexity increases, and in practice we\n",
      "should tune the model complexity to achieve the best tradeo\u000b",
      ". For instance,\n",
      "in the example above, \f",
      "tting a quadratic function does better than either of\n",
      "the extremes of a \f",
      "rst or a 5-th degree polynomial, as shown in Figure 8.9.\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytraining data\n",
      "best fit quadratic model\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "x0.00.51.01.5ytest data\n",
      "best fit quadratic model\n",
      "ground truth h*\n",
      "Figure 8.9: Best \f",
      "t quadratic model has small training and test error because\n",
      "quadratic model achieves a better tradeo\u000b",
      ".\n",
      "Interestingly, the bias-variance tradeo\u000b",
      " curves or the test error curves\n",
      "do not universally follow the shape in Figure 8.8, at least not universally\n",
      "when the model complexity is simply measured by the number of parameters.\n",
      "(We will discuss the so-called double descent phenomenon in Section 8.2.)\n",
      "Nevertheless, the principle of bias-variance tradeo\u000b",
      " is perhaps still the \f",
      "rst\n",
      "resort when analyzing and predicting the behavior of test errors.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "120\n",
      "8.1.1 A mathematical decomposition (for regression)\n",
      "To formally state the bias-variance tradeo\u000b",
      " for regression problems, we con-\n",
      "sider the following setup (which is an extension of the beginning paragraph\n",
      "of Section 8.1).\n",
      "•Draw a training dataset S=fx(i);y(i)gn\n",
      "i=1such thaty(i)=h?(x(i)) +\u0018(i)\n",
      "where\u0018(i)2N(0;\u001b2).\n",
      "•Train a model on the dataset S, denoted by ^hS.\n",
      "•Take a test example ( x;y) such that y=h?(x) +\u0018where\u0018\u0018N(0;\u001b2),\n",
      "and measure the expected test error (averaged over the random draw of\n",
      "the training set Sand the randomness of \u0018)56\n",
      "MSE(x) =ES;\u0018[(y\u0000hS(x))2] (8.2)\n",
      "We will decompose the MSE into a bias and variance term. We start by\n",
      "stating a following simple mathematical tool that will be used twice below.\n",
      "Claim 8.1.1: SupposeAandBare two independent real random variables\n",
      "andE[A] = 0. Then, E[(A+B)2] =E[A2] +E[B2].\n",
      "As a corollary, because a random variable Ais independent with a con-\n",
      "stantc, when E[A] = 0, we have E[(A+c)2] =E[A2] +c2.\n",
      "The proof of the claim follows from expanding the square: E[(A+B)2] =\n",
      "E[A2] +E[B2] + 2E[AB] =E[A2] +E[B2]. Here we used the independence to\n",
      "show that E[AB] =E[A]E[B] = 0.\n",
      "Using Claim 8.1.1 with A=\u0018andB=h?(x)\u0000^hS(x), we have\n",
      "MSE(x) =E[(y\u0000hS(x))2] =E[(\u0018+ (h?(x)\u0000hS(x)))2] (8.3)\n",
      "=E[\u00182] +E[(h?(x)\u0000hS(x))2] (by Claim 8.1.1)\n",
      "=\u001b2+E[(h?(x)\u0000hS(x))2] (8.4)\n",
      "Then, let's de\f",
      "ne havg(x) =ES[hS(x)] as the \\average model\"|the model\n",
      "obtained by drawing an in\f",
      "nite number of datasets, training on them, and\n",
      "averaging their predictions on x. Note that havgis a hypothetical model for\n",
      "analytical purposes that can not be obtained in reality (because we don't\n",
      "5For simplicity, the test input xis considered to be \f",
      "xed here, but the same conceptual\n",
      "message holds when we average over the choice of x's.\n",
      "6The subscript under the expectation symbol is to emphasize the variables that are\n",
      "considered as random by the expectation operation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "121\n",
      "have in\f",
      "nite number of datasets). It turns out that for many cases, havg\n",
      "is (approximately) equal to the the model obtained by training on a single\n",
      "dataset with in\f",
      "nite samples. Thus, we can also intuitively interpret havgthis\n",
      "way, which is consistent with our intuitive de\f",
      "nition of bias in the previous\n",
      "subsection.\n",
      "We can further decompose MSE( x) by letting c=h?(x)\u0000havg(x) (which is\n",
      "a constant that does not depend on the choice of S!) andA=havg(x)\u0000hS(x)\n",
      "in the corollary part of Claim 8.1.1:\n",
      "MSE(x) =\u001b2+E[(h?(x)\u0000hS(x))2] (8.5)\n",
      "=\u001b2+ (h?(x)\u0000havg(x))2+E[(havg\u0000hS(x))2] (8.6)\n",
      "=\u001b2\n",
      "|{z}\n",
      "unavoidable+ (h?(x)\u0000havg(x))2\n",
      "|{z}\n",
      ",bias2+ var(hS(x))|{z}\n",
      ",variance(8.7)\n",
      "We call the second term the bias (square) and the third term the variance. As\n",
      "discussed before, the bias captures the part of the error that are introduced\n",
      "due to the lack of expressivity of the model. Recall that havgcan be thought\n",
      "of as the best possible model learned even with in\f",
      "nite data. Thus, the bias is\n",
      "not due to the lack of data, but is rather caused by that the family of models\n",
      "fundamentally cannot approximate the h?. For example, in the illustrating\n",
      "example in Figure 8.2, because any linear model cannot approximate the\n",
      "true quadratic function h?, neither can havg, and thus the bias term has to\n",
      "be large.\n",
      "The variance term captures how the random nature of the \f",
      "nite dataset\n",
      "introduces errors in the learned model. It measures the sensitivity of the\n",
      "learned model to the randomness in the dataset. It often decreases as the\n",
      "size of the dataset increases.\n",
      "There is nothing we can do about the \f",
      "rst term \u001b2as we can not predict\n",
      "the noise\u0018by de\f",
      "nition.\n",
      "Finally, we note that the bias-variance decomposition for classi\f",
      "cation\n",
      "is much less clear than for regression problems. There have been several\n",
      "proposals, but there is as yet no agreement on what is the \\right\" and/or\n",
      "the most useful formalism.\n",
      "8.2 The double descent phenomenon\n",
      "Model-wise double descent. Recent works have demonstrated that the\n",
      "test error can present a \\double descent\" phenomenon in a range of machine\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "122\n",
      "learning models including linear models and deep neural networks.7The\n",
      "conventional wisdom, as discussed in Section 8.1, is that as we increase the\n",
      "model complexity, the test error \f",
      "rst decreases and then increases, as illus-\n",
      "trated in Figure 8.8. However, in many cases, we empirically observe that\n",
      "the test error can have a second descent|it \f",
      "rst decreases, then increases\n",
      "to a peak around when the model size is large enough to \f",
      "t all the training\n",
      "data very well, and then decreases again in the so-called overparameterized\n",
      "regime, where the number of parameters is larger than the number of data\n",
      "points. See Figure 8.10 for an illustration of the typical curves of test errors\n",
      "against model complexity (measured by the number of parameters). To some\n",
      "extent, the overparameterized regime with the second descent is considered as\n",
      "new to the machine learning community|partly because lightly-regularized,\n",
      "overparameterized models are only extensively used in the deep learning era.\n",
      "A practical implication of the phenomenon is that one should not hold back\n",
      "from scaling into and experimenting with over-parametrized models because\n",
      "the test error may well decrease again to a level even smaller than the previ-\n",
      "ous lowest point. Actually, in many cases, larger overparameterized models\n",
      "always lead to a better test performance (meaning there won't be a second\n",
      "ascent after the second descent).\n",
      "# parameterstest errortypically when # parameters\n",
      "is sufficient to fit the dataclassical regime:\n",
      "bias-variance tradeoffmodern regime:\n",
      "over-parameterization\n",
      "Figure 8.10: A typical model-wise double descent phenomenon. As the num-\n",
      "ber of parameters increases, the test error \f",
      "rst decreases when the number of\n",
      "parameters is smaller than the training data. Then in the overparameterized\n",
      "regime, the test error decreases again.\n",
      "7The discovery of the phenomenon perhaps dates back to Opper [1995, 2001], and has\n",
      "been recently popularized by Belkin et al. [2020], Hastie et al. [2019], etc.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "123\n",
      "Sample-wise double descent. A priori, we would expect that more\n",
      "training examples always lead to smaller test errors|more samples give\n",
      "strictly more information for the algorithm to learn from. However, recent\n",
      "work [Nakkiran, 2019] observes that the test error is not monotonically de-\n",
      "creasing as we increase the sample size. Instead, as shown in Figure 8.11, the\n",
      "test error decreases, and then increases and peaks around when the number\n",
      "of examples (denoted by n) is similar to the number of parameters (denoted\n",
      "byd), and then decreases again. We refer to this as the sample-wise dou-\n",
      "ble descent phenomenon. To some extent, sample-wise double descent and\n",
      "model-wise double descent are essentially describing similar phenomena|the\n",
      "test error is peaked when n\u0019d.\n",
      "Explanation and mitigation strategy. The sample-wise double descent,\n",
      "or, in particular, the peak of test error at n\u0019d, suggests that the existing\n",
      "training algorithms evaluated in these experiments are far from optimal when\n",
      "n\u0019d. We will be better o\u000b",
      " by tossing away some examples and run the\n",
      "algorithms with a smaller sample size to steer clear of the peak. In other\n",
      "words, in principle, there are other algorithms that can achieve smaller test\n",
      "error when n\u0019d, but the algorithms evaluated in these experiments fail to\n",
      "do so. The sub-optimality of the learning procedure appears to be the culprit\n",
      "of the peak in both sample-wise and model-wise double descent.\n",
      "Indeed, with an optimally-tuned regularization (which will be discussed\n",
      "more in Section 9), the test error in the n\u0019dregime can be dramatically\n",
      "improved, and the model-wise and sample-wise double descent are both mit-\n",
      "igated. See Figure 8.11.\n",
      "The intuition above only explains the peak in the model-wise and sample-\n",
      "wise double descent, but does not explain the second descent in the model-\n",
      "wise double descent|why overparameterized models are able to generalize\n",
      "so well. The theoretical understanding of overparameterized models is an ac-\n",
      "tive research area with many recent advances. A typical explanation is that\n",
      "the commonly-used optimizers such as gradient descent provide an implicit\n",
      "regularization e\u000b",
      "ect (which will be discussed in more detail in Section 9.2).\n",
      "In other words, even in the overparameterized regime and with an unregular-\n",
      "ized loss function, the model is still implicitly regularized, and thus exhibits\n",
      "a better test performance than an arbitrary solution that \f",
      "ts the data. For\n",
      "example, for linear models, when n\u001c",
      "d, the gradient descent optimizer with\n",
      "zero initialization \f",
      "nds the minimum norm solution that \f",
      "ts the data (in-\n",
      "stead of an arbitrary solution that \f",
      "ts the data), and the minimum norm reg-\n",
      "ularizer turns out to be a su\u000eciently good for the overparameterized regime\n",
      "(but it's not a good regularizer when n\u0019d, resulting in the peak of test\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "124\n",
      "error).\n",
      "0 200 400 600 800 1000\n",
      "Num Samples0.000.250.500.751.001.251.501.752.00T est ErrorT est Error vs. # Samples\n",
      "T est Error\n",
      "Figure 8.11: Left: The sample-wise double descent phenomenon for linear\n",
      "models. Right: The sample-wise double descent with di\u000b",
      "erent regularization\n",
      "strength for linear models. Using the optimal regularization parameter \u0015\n",
      "(optimally tuned for each n, shown in green solid curve) mitigates double\n",
      "descent. Setup: The data distribution of ( x;y) isx\u0018N (0;Id) andy\u0018\n",
      "x>\f",
      "+N(0;\u001b2) whered= 500;\u001b= 0:5 andk\f",
      "k2= 1.8\n",
      "Finally, we also remark that the double descent phenomenon has been\n",
      "mostly observed when the model complexity is measured by the number of\n",
      "parameters. It is unclear if and when the number of parameters is the best\n",
      "complexity measure of a model. For example, in many situations, the norm\n",
      "of the models is used as a complexity measure. As shown in Figure 8.12\n",
      "right, for a particular linear case, if we plot the test error against the norm\n",
      "of the learnt model, the double descent phenomenon no longer occurs. This\n",
      "is partly because the norm of the learned model is also peaked around n\u0019d\n",
      "(See Figure 8.12 (middle) or Belkin et al. [2019], Mei and Montanari [2022],\n",
      "and discussions in Section 10.8 of James et al. [2021]). For deep neural\n",
      "networks, the correct complexity measure is even more elusive. The study of\n",
      "double descent phenomenon is an active research topic.\n",
      "8The \f",
      "gure is reproduced from Figure 1 of Nakkiran et al. [2020]. Similar phenomenon\n",
      "are also observed in Hastie et al. [2022], Mei and Montanari [2022]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "125\n",
      "0 250 500 750 1000\n",
      "# parameters0.00.20.40.60.81.0test errortest error vs. # params\n",
      "0 200 400 600 800 1000\n",
      "# parameters010203040normnorm vs. # params\n",
      "0 10 20 30 40\n",
      "norm0.00.20.40.60.81.0test errord=n\n",
      "# parameterstest error vs. norm\n",
      "02004006008001000\n",
      "Figure 8.12: Left: The double descent phenomenon, where the number of pa-\n",
      "rameters is used as the model complexity. Middle: The norm of the learned\n",
      "model is peaked around n\u0019d.Right: The test error against the norm of\n",
      "the learnt model. The color bar indicate the number of parameters and the\n",
      "arrows indicates the direction of increasing model size. Their relationship\n",
      "are closer to the convention wisdom than to a double descent. Setup: We\n",
      "consider a linear regression with a \f",
      "xed dataset of size n= 500:The input\n",
      "xis a random ReLU feature on Fashion-MNIST, and output y2R10is the\n",
      "one-hot label. This is the same setting as in Section 5.2 of Nakkiran et al.\n",
      "[2020].\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "126\n",
      "8.3 Sample complexity bounds (optional\n",
      "readings)\n",
      "8.3.1 Preliminaries\n",
      "In this set of notes, we begin our foray into learning theory. Apart from\n",
      "being interesting and enlightening in its own right, this discussion will also\n",
      "help us hone our intuitions and derive rules of thumb about how to best\n",
      "apply learning algorithms in di\u000b",
      "erent settings. We will also seek to answer\n",
      "a few questions: First, can we make formal the bias/variance tradeo\u000b",
      " that\n",
      "was just discussed? This will also eventually lead us to talk about model\n",
      "selection methods, which can, for instance, automatically decide what order\n",
      "polynomial to \f",
      "t to a training set. Second, in machine learning it's really\n",
      "generalization error that we care about, but most learning algorithms \f",
      "t their\n",
      "models to the training set. Why should doing well on the training set tell us\n",
      "anything about generalization error? Speci\f",
      "cally, can we relate error on the\n",
      "training set to generalization error? Third and \f",
      "nally, are there conditions\n",
      "under which we can actually prove that learning algorithms will work well?\n",
      "We start with two simple but very useful lemmas.\n",
      "Lemma. (The union bound). Let A1;A2;:::;Akbekdi\u000b",
      "erent events (that\n",
      "may not be independent). Then\n",
      "P(A1[\u0001\u0001\u0001[Ak)\u0014P(A1) +:::+P(Ak):\n",
      "In probability theory, the union bound is usually stated as an axiom\n",
      "(and thus we won't try to prove it), but it also makes intuitive sense: The\n",
      "probability of any one of kevents happening is at most the sum of the\n",
      "probabilities of the kdi\u000b",
      "erent events.\n",
      "Lemma. (Hoe\u000b",
      "ding inequality) Let Z1;:::;Znbenindependent and iden-\n",
      "tically distributed (iid) random variables drawn from a Bernoulli( \u001e",
      ") distri-\n",
      "bution. I.e., P(Zi= 1) =\u001e",
      ", andP(Zi= 0) = 1\u0000\u001e",
      ". Let ^\u001e",
      "= (1=n)Pn\n",
      "i=1Zi\n",
      " >0 be \f",
      "xed. Thenese random variables, and let any \n",
      "2n) exp(\u00002\n",
      "This lemma (which in learning theory is also called the Cherno\u000b",
      " bound )\n",
      "says that if we take ^\u001e",
      "|the average of nBernoulli(\u001e",
      ") random variables|to\n",
      "be our estimate of \u001e",
      ", then the probability of our being far from the true value\n",
      "is small, so long as nis large. Another way of saying this is that if you have\n",
      "a biased coin whose chance of landing on heads is \u001e",
      ", then if you toss it n\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "127\n",
      "times and calculate the fraction of times that it came up heads, that will be\n",
      "a good estimate of \u001e",
      "with high probability (if nis large).\n",
      "Using just these two lemmas, we will be able to prove some of the deepest\n",
      "and most important results in learning theory.\n",
      "To simplify our exposition, let's restrict our attention to binary classi\f",
      "ca-\n",
      "tion in which the labels are y2f0;1g. Everything we'll say here generalizes\n",
      "to other problems, including regression and multi-class classi\f",
      "cation.\n",
      "We assume we are given a training set S=f(x(i);y(i));i= 1;:::;ngof size\n",
      "n, where the training examples ( x(i);y(i)) are drawn iid from some probability\n",
      "distributionD. For a hypothesis h, we de\f",
      "ne the training error (also called\n",
      "theempirical risk orempirical error in learning theory) to be\n",
      "^\"(h) =1\n",
      "nnX\n",
      "i=11fh(x(i))6=y(i)g:\n",
      "This is just the fraction of training examples that hmisclassi\f",
      "es. When we\n",
      "want to make explicit the dependence of ^ \"(h) on the training set S, we may\n",
      "also write this a ^ \"S(h). We also de\f",
      "ne the generalization error to be\n",
      "\"(h) =P(x;y)\u0018D(h(x)6=y):\n",
      "I.e. this is the probability that, if we now draw a new example ( x;y) from\n",
      "the distributionD,hwill misclassify it.\n",
      "Note that we have assumed that the training data was drawn from the\n",
      "same distributionDwith which we're going to evaluate our hypotheses (in\n",
      "the de\f",
      "nition of generalization error). This is sometimes also referred to as\n",
      "one of the PAC assumptions.9\n",
      "Consider the setting of linear classi\f",
      "cation, and let h\u0012(x) = 1f\u0012Tx\u00150g.\n",
      "What's a reasonable way of \f",
      "tting the parameters \u0012? One approach is to try\n",
      "to minimize the training error, and pick\n",
      "^\u0012= arg min\n",
      "\u0012^\"(h\u0012):\n",
      "We call this process empirical risk minimization (ERM), and the resulting\n",
      "hypothesis output by the learning algorithm is ^h=h^\u0012. We think of ERM\n",
      "as the most \\basic\" learning algorithm, and it will be this algorithm that we\n",
      "9PAC stands for \\probably approximately correct,\" which is a framework and set of\n",
      "assumptions under which numerous results on learning theory were proved. Of these, the\n",
      "assumption of training and testing on the same distribution, and the assumption of the\n",
      "independently drawn training examples, were the most important.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "128\n",
      "focus on in these notes. (Algorithms such as logistic regression can also be\n",
      "viewed as approximations to empirical risk minimization.)\n",
      "In our study of learning theory, it will be useful to abstract away from\n",
      "the speci\f",
      "c parameterization of hypotheses and from issues such as whether\n",
      "we're using a linear classi\f",
      "er. We de\f",
      "ne the hypothesis class Hused by a\n",
      "learning algorithm to be the set of all classi\f",
      "ers considered by it. For linear\n",
      "classi\f",
      "cation,H=fh\u0012:h\u0012(x) = 1f\u0012Tx\u00150g;\u00122Rd+1gis thus the set of\n",
      "all classi\f",
      "ers over X(the domain of the inputs) where the decision boundary\n",
      "is linear. More broadly, if we were studying, say, neural networks, then we\n",
      "could letHbe the set of all classi\f",
      "ers representable by some neural network\n",
      "architecture.\n",
      "Empirical risk minimization can now be thought of as a minimization over\n",
      "the class of functions H, in which the learning algorithm picks the hypothesis:\n",
      "^h= arg min\n",
      "h2H^\"(h)\n",
      "8.3.2 The case of \f",
      "nite H\n",
      "Let's start by considering a learning problem in which we have a \f",
      "nite hy-\n",
      "pothesis classH=fh1;:::;hkgconsisting of khypotheses. Thus, His just a\n",
      "set ofkfunctions mapping from Xtof0;1g, and empirical risk minimization\n",
      "selects ^hto be whichever of these kfunctions has the smallest training error.\n",
      "We would like to give guarantees on the generalization error of ^h. Our\n",
      "strategy for doing so will be in two parts: First, we will show that ^ \"(h) is a\n",
      "reliable estimate of \"(h) for allh. Second, we will show that this implies an\n",
      "upper-bound on the generalization error of ^h.\n",
      "Take any one, \f",
      "xed, hi2H. Consider a Bernoulli random variable Z\n",
      "whose distribution is de\f",
      "ned as follows. We're going to sample ( x;y)\u0018D.\n",
      "Then, we set Z= 1fhi(x)6=yg. I.e., we're going to draw one example,\n",
      "and letZindicate whether himisclassi\f",
      "es it. Similarly, we also de\f",
      "ne Zj=\n",
      "1fhi(x(j))6=y(j)g. Since our training set was drawn iid from D,Zand the\n",
      "Zj's have the same distribution.\n",
      "We see that the misclassi\f",
      "cation probability on a randomly drawn\n",
      "example|that is, \"(h)|is exactly the expected value of Z(andZj). More-\n",
      "over, the training error can be written\n",
      "^\"(hi) =1\n",
      "nnX\n",
      "j=1Zj:\n",
      "Thus, ^\"(hi) is exactly the mean of the nrandom variables Zjthat are drawn\n",
      "iid from a Bernoulli distribution with mean \"(hi). Hence, we can apply the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "129\n",
      "Hoe\u000b",
      "ding inequality, and obtain\n",
      "2n):exp(\u00002\"(hi)j>\n",
      "This shows that, for our particular hi, training error will be close to\n",
      "generalization error with high probability, assuming nis large. But we don't\n",
      "just want to guarantee that \"(hi) will be close to ^ \"(hi) (with high probability)\n",
      "for just only one particular hi. We want to prove that this will be true\n",
      "simultaneously for allh2H. To do so, let Aidenote the event that j\"(hi)\u0000\n",
      ". We've already shown that, for any particular Ai, it holds true\n",
      "2n). Thus, using the union bound, we have that\n",
      ") =P(A1[\u0001\u0001\u0001[Ak)\"(hi)j>\n",
      "\u0014kX\n",
      "i=1P(Ai)\n",
      "\u0014kX\n",
      "2n)2 exp(\u00002\n",
      "2n)kexp(\u00002\n",
      "If we subtract both sides from 1, we \f",
      "nd that\n",
      ") =P(8h2H:j\"(hi)\u0000^\"(hi)j\u0014\n",
      "2n)2kexp(\u00002\n",
      "(The \\:\" symbol means \\not.\") So, with probability at least 1 \u0000\n",
      "of ^\"(h) for allh2H.h) will be within \n",
      "This is called a uniform convergence result, because this is a bound that\n",
      "holds simultaneously for all (as opposed to just one) h2H.\n",
      "In the discussion above, what we did was, for particular values of nand\n",
      ". give a bound on the probability that for some h2H,j\"(h)\u0000^\"(h)j> \n",
      ", and the probability of error;nterest here: n,\n",
      "we can bound either one in terms of the other two.\n",
      "and some\u000e>0,, we can ask the following question: Given \n",
      "how large must nbe before we can guarantee that with probability at least\n",
      "of generalization error? By setting\n",
      "2n) and solving for n, [you should convince yourself this is\n",
      "the right thing to do!], we \f",
      "nd that if\n",
      "n\u00151\n",
      "2log2k\n",
      "\u000e;\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "130\n",
      "for allth probability at least 1 \u0000\u000e, we have thatj\"(h)\u0000^\"(h)j\u0014\n",
      "h2H. (Equivalently, this shows that the probability that j\"(h)\u0000^\"(h)j>\n",
      "for someh2 H is at most \u000e.) This bound tells us how many training\n",
      "examples we need in order make a guarantee. The training set size nthat\n",
      "a certain method or algorithm requires in order to achieve a certain level of\n",
      "performance is also called the algorithm's sample complexity .\n",
      "The key property of the bound above is that the number of training\n",
      "examples needed to make this guarantee is only logarithmic ink, the number\n",
      "of hypotheses inH. This will be important later.\n",
      "in the previousan also hold nand\u000e\f",
      "xed and solve for \n",
      "equation, and show [again, convince yourself that this is right!] that with\n",
      "probability 1\u0000\u000e, we have that for all h2H,\n",
      "j^\"(h)\u0000\"(h)j\u0014r\n",
      "1\n",
      "2nlog2k\n",
      "\u000e:\n",
      "Now, let's assume that uniform convergence holds, i.e., that j\"(h)\u0000^\"(h)j\u0014\n",
      "for allh2H. What can we prove about the generalization of our learning\n",
      "algorithm that picked ^h= arg min h2H^\"(h)?\n",
      "De\f",
      "neh\u0003= arg min h2H\"(h) to be the best possible hypothesis in H. Note\n",
      "thath\u0003is the best that we could possibly do given that we are using H, so\n",
      "it makes sense to compare our performance to that of h\u0003. We have:\n",
      "\"(^h)\u0014^\"(^h) +\n",
      "\u0014^\"(h\u0003) +\n",
      "\u0014\"(h\u0003) + 2\n",
      "(by our uniform convergence that j\"(^h)\u0000^\"(^h)j\u0014\n",
      "assumption). The second used the fact that ^hwas chosen to minimize ^ \"(h),\n",
      "and hence ^\"(^h)\u0014^\"(h) for allh, and in particular ^ \"(^h)\u0014^\"(h\u0003). The third\n",
      "line used the uniform convergence assumption again, to show that ^ \"(h\u0003)\u0014\n",
      ". So, what we've shown is the following: If uniform convergence\n",
      "worse than the bestneralization error of ^his at most 2 \n",
      "possible hypothesis in H!\n",
      "Let's put all this together into a theorem.\n",
      "Theorem. LetjHj=k, and let any n;\u000ebe \f",
      "xed. Then with probability at\n",
      "least 1\u0000\u000e, we have that\n",
      "\"(^h)\u0014\u0012\n",
      "min\n",
      "h2H\"(h)\u0013\n",
      "+ 2r\n",
      "1\n",
      "2nlog2k\n",
      "\u000e:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "131\n",
      "equal thep\u0001term, using our previous argu-\n",
      "ment that uniform convergence occurs with probability at least 1 \u0000\u000e, and\n",
      "higher than that uniform convergence implies \"(h) is at most 2 \n",
      "\"(h\u0003) = minh2H\"(h) (as we showed previously).\n",
      "This also quanti\f",
      "es what we were saying previously saying about the\n",
      "bias/variance tradeo\u000b",
      " in model selection. Speci\f",
      "cally, suppose we have some\n",
      "hypothesis classH, and are considering switching to some much larger hy-\n",
      "pothesis classH0\u0013H . If we switch to H0, then the \f",
      "rst term min h\"(h)\n",
      "can only decrease (since we'd then be taking a min over a larger set of func-\n",
      "tions). Hence, by learning using a larger hypothesis class, our \\bias\" can\n",
      "only decrease. However, if k increases, then the second 2p\u0001term would also\n",
      "increase. This increase corresponds to our \\variance\" increasing when we use\n",
      "a larger hypothesis class.\n",
      "and\u000e\f",
      "xed and solving for nlike we did before, we can also\n",
      "obtain the following sample complexity bound:\n",
      "be \f",
      "xed. Then for \"(^h)\u0014 let any \u000e;\n",
      "to hold with probability at least 1 \u0000\u000e, it su\u000eces that\n",
      "n\u00151\n",
      "2log2k\n",
      "\u000e\n",
      "=O\u00121\n",
      "2logk\n",
      "\u000e\u0013\n",
      ";\n",
      "8.3.3 The case of in\f",
      "nite H\n",
      "We have proved some useful theorems for the case of \f",
      "nite hypothesis classes.\n",
      "But many hypothesis classes, including any parameterized by real numbers\n",
      "(as in linear classi\f",
      "cation) actually contain an in\f",
      "nite number of functions.\n",
      "Can we prove similar results for this setting?\n",
      "Let's start by going through something that is notthe \\right\" argument.\n",
      "Better and more general arguments exist , but this will be useful for honing\n",
      "our intuitions about the domain.\n",
      "Suppose we have an Hthat is parameterized by dreal numbers. Since we\n",
      "are using a computer to represent real numbers, and IEEE double-precision\n",
      "oating point num-uble 's in C) uses 64 bits to represent a \n",
      "ber, this means that our learning algorithm, assuming we're using double-\n",
      "oating point, is parameterized by 64 dbits. Thus, our hypothesis\n",
      "class really consists of at most k= 264ddi\u000b",
      "erent hypotheses. From the Corol-\n",
      "lary at the end of the previous section, we therefore \f",
      "nd that, to guarantee\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "132\n",
      ", with to hold with probability at least 1 \u0000\u000e, it su\u000eces that\n",
      "n\u0015O\u0010\n",
      "1\n",
      "2log264d\n",
      "\u000e\u0011\n",
      "=O\u0010\n",
      "d\n",
      "2log1\n",
      "\u000e\u0011\n",
      ";\u000esubscripts indicate\n",
      "and\u000e.) Thus,t big- Ois hiding constants that may depend on \n",
      "the number of training examples needed is at most linear in the parameters\n",
      "of the model.\n",
      "oating point makes this argument not\n",
      "entirely satisfying, but the conclusion is nonetheless roughly correct: If what\n",
      "we try to do is minimize training error, then in order to learn \\well\" using a\n",
      "hypothesis class that has dparameters, generally we're going to need on the\n",
      "order of a linear number of training examples in d.\n",
      "(At this point, it's worth noting that these results were proved for an al-\n",
      "gorithm that uses empirical risk minimization. Thus, while the linear depen-\n",
      "dence of sample complexity on ddoes generally hold for most discriminative\n",
      "learning algorithms that try to minimize training error or some approxima-\n",
      "tion to training error, these conclusions do not always apply as readily to\n",
      "discriminative learning algorithms. Giving good theoretical guarantees on\n",
      "many non-ERM learning algorithms is still an area of active research.)\n",
      "The other part of our previous argument that's slightly unsatisfying is\n",
      "that it relies on the parameterization of H. Intuitively, this doesn't seem like\n",
      "it should matter: We had written the class of linear classi\f",
      "ers as h\u0012(x) =\n",
      "1f\u00120+\u00121x1+\u0001\u0001\u0001\u0012dxd\u00150g, withn+ 1 parameters \u00120;:::;\u0012d. But it could\n",
      "also be written hu;v(x) = 1f(u2\n",
      "0\u0000v2\n",
      "0) + (u2\n",
      "1\u0000v2\n",
      "1)x1+\u0001\u0001\u0001(u2\n",
      "d\u0000v2\n",
      "d)xd\u00150g\n",
      "with 2d+ 2 parameters ui;vi. Yet, both of these are just de\f",
      "ning the same\n",
      "H: The set of linear classi\f",
      "ers in ddimensions.\n",
      "To derive a more satisfying argument, let's de\f",
      "ne a few more things.\n",
      "Given a set S=fx(i);:::;x(D)g(no relation to the training set) of points\n",
      "x(i)2X, we say thatHshattersSifHcan realize any labeling on S.\n",
      "I.e., if for any set of labels fy(1);:::;y(D)g, there exists some h2H so that\n",
      "h(x(i)) =y(i)for alli= 1;:::D.\n",
      "Given a hypothesis class H, we then de\f",
      "ne its Vapnik-Chervonenkis\n",
      "dimension , written VC(H), to be the size of the largest set that is shattered\n",
      "byH. (IfHcan shatter arbitrarily large sets, then VC( H) =1.)\n",
      "For instance, consider the following set of three points:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "133\n",
      "/0 /1\n",
      "/0 /1/0 /1\n",
      "x\n",
      "x12\n",
      "Can the setHof linear classi\f",
      "ers in two dimensions ( h(x) = 1f\u00120+\u00121x1+\n",
      "\u00122x2\u00150g) can shatter the set above? The answer is yes. Speci\f",
      "cally, we\n",
      "see that, for any of the eight possible labelings of these points, we can \f",
      "nd a\n",
      "linear classi\f",
      "er that obtains \\zero training error\" on them:\n",
      "x\n",
      "x12 x\n",
      "x12 x\n",
      "x12 x\n",
      "x12\n",
      "x\n",
      "x12 x\n",
      "x12 x\n",
      "x12 x\n",
      "x12\n",
      "Moreover, it is possible to show that there is no set of 4 points that this\n",
      "hypothesis class can shatter. Thus, the largest set that Hcan shatter is of\n",
      "size 3, and hence VC( H) = 3.\n",
      "Note that the VC dimension of Hhere is 3 even though there may be\n",
      "sets of size 3 that it cannot shatter. For instance, if we had a set of three\n",
      "points lying in a straight line (left \f",
      "gure), then there is no way to \f",
      "nd a linear\n",
      "separator for the labeling of the three points shown below (right \f",
      "gure):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "134\n",
      "x\n",
      "x12/0 /1\n",
      "/0 /1\n",
      "/0 /1x\n",
      "x12\n",
      "In order words, under the de\f",
      "nition of the VC dimension, in order to\n",
      "prove that VC(H) is at least D, we need to show only that there's at least\n",
      "oneset of size DthatHcan shatter.\n",
      "The following theorem, due to Vapnik, can then be shown. (This is, many\n",
      "would argue, the most important theorem in all of learning theory.)\n",
      "Theorem. LetHbe given, and let D= VC(H). Then with probability at\n",
      "least 1\u0000\u000e, we have that for all h2H,\n",
      "j\"(h)\u0000^\"(h)j\u0014O r\n",
      "D\n",
      "nlogn\n",
      "D+1\n",
      "nlog1\n",
      "\u000e!\n",
      ":\n",
      "Thus, with probability at least 1 \u0000\u000e, we also have that:\n",
      "\"(^h)\u0014\"(h\u0003) +O r\n",
      "D\n",
      "nlogn\n",
      "D+1\n",
      "nlog1\n",
      "\u000e!\n",
      ":\n",
      "In other words, if a hypothesis class has \f",
      "nite VC dimension, then uniform\n",
      "convergence occurs as nbecomes large. As before, this allows us to give a\n",
      "bound on\"(h) in terms of \"(h\u0003). We also have the following corollary:\n",
      "to hold for all h2H (and hence \"(^h)\u0014\n",
      ";\u000e(D). probability at least 1 \u0000\u000e, it su\u000eces that n=O\n",
      "In other words, the number of training examples needed to learn \\well\"\n",
      "usingHis linear in the VC dimension of H. It turns out that, for \\most\"\n",
      "hypothesis classes, the VC dimension (assuming a \\reasonable\" parameter-\n",
      "ization) is also roughly linear in the number of parameters. Putting these\n",
      "together, we conclude that for a given hypothesis class H(and for an algo-\n",
      "rithm that tries to minimize training error), the number of training examples\n",
      "needed to achieve generalization error close to that of the optimal classi\f",
      "er\n",
      "is usually roughly linear in the number of parameters of H.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 9\n",
      "Regularization and model\n",
      "selection\n",
      "9.1 Regularization\n",
      "Recall that as discussed in Section 8.1, overftting is typically a result of using\n",
      "too complex models, and we need to choose a proper model complexity to\n",
      "achieve the optimal bias-variance tradeo\u000b",
      ". When the model complexity is\n",
      "measured by the number of parameters, we can vary the size of the model\n",
      "(e.g., the width of a neural net). However, the correct, informative complex-\n",
      "ity measure of the models can be a function of the parameters (e.g., `2norm\n",
      "of the parameters), which may not necessarily depend on the number of pa-\n",
      "rameters. In such cases, we will use regularization, an important technique\n",
      "in machine learning, control the model complexity and prevent over\f",
      "tting.\n",
      "Regularization typically involves adding an additional term, called a reg-\n",
      "ularizer and denoted by R(\u0012) here, to the training loss/cost function:\n",
      "J\u0015(\u0012) =J(\u0012) +\u0015R(\u0012) (9.1)\n",
      "HereJ\u0015is often called the regularized loss, and \u0015\u00150 is called the regular-\n",
      "ization parameter. The regularizer R(\u0012) is a nonnegative function (in almost\n",
      "all cases). In classical methods, R(\u0012) is purely a function of the parameter \u0012,\n",
      "but some modern approach allows R(\u0012) to depend on the training dataset.1\n",
      "The regularizer R(\u0012) is typically chosen to be some measure of the com-\n",
      "plexity of the model \u0012. Thus, when using the regularized loss, we aim to\n",
      "\f",
      "nd a model that both \f",
      "t the data (a small loss J(\u0012)) and have a small\n",
      "1Here our notations generally omit the dependency on the training dataset for\n",
      "simplicity|we write J(\u0012) even though it obviously needs to depend on the training dataset.\n",
      "135\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "136\n",
      "model complexity (a small R(\u0012)). The balance between the two objectives is\n",
      "controlled by the regularization parameter \u0015. When\u0015= 0, the regularized\n",
      "loss is equivalent to the original loss. When \u0015is a su\u000eciently small positive\n",
      "number, minimizing the regularized loss is e\u000b",
      "ectively minimizing the original\n",
      "loss with the regularizer as the tie-breaker. When the regularizer is extremely\n",
      "large, then the original loss is not e\u000b",
      "ective (and likely the model will have a\n",
      "large bias.)\n",
      "The most commonly used regularization is perhaps `2regularization,\n",
      "whereR(\u0012) =1\n",
      "2k\u0012k2\n",
      "2. It encourages the optimizer to \f",
      "nd a model with\n",
      "small`2norm. In deep learning, it's oftentimes referred to as weight de-\n",
      "cay, because gradient descent with learning rate \u0011on the regularized loss\n",
      "R\u0015(\u0012) is equivalent to shrinking/decaying \u0012by a scalar factor of 1 \u0000\u0011\u0015and\n",
      "then applying the standard gradient\n",
      "\u0012 \u0012\u0000\u0011rJ\u0015(\u0012) =\u0012\u0000\u0011\u0015\u0012\u0000\u0011rJ(\u0012)\n",
      "= (1\u0000\u0015\u0011)\u0012|{z}\n",
      "decaying weights\u0000\u0011rJ(\u0012) (9.2)\n",
      "Besides encouraging simpler models, regularization can also impose in-\n",
      "ductive biases or structures on the model parameters. For example, suppose\n",
      "we had a prior belief that the number of non-zeros in the ground-truth model\n",
      "parameters is small,2|which is oftentimes called sparsity of the model|, we\n",
      "can impose a regularization on the number of non-zeros in \u0012, denoted by\n",
      "k\u0012k0, to leverage such a prior belief. Imposing additional structure of the\n",
      "parameters narrows our search space and makes the complexity of the model\n",
      "family smaller,|e.g., the family of sparse models can be thought of as having\n",
      "lower complexity than the family of all models|, and thus tends to lead to a\n",
      "better generalization. On the other hand, imposing additional structure may\n",
      "risk increasing the bias. For example, if we regularize the sparsity strongly\n",
      "but no sparse models can predict the label accurately, we will su\u000b",
      "er from\n",
      "large bias (analogously to the situation when we use linear models to learn\n",
      "data than can only be represented by quadratic functions in Section 8.1.)\n",
      "The sparsity of the parameters is not a continuous function of the param-\n",
      "eters, and thus we cannot optimize it with (stochastic) gradient descent. A\n",
      "common relaxation is to use R(\u0012) =k\u0012k1as a continuous surrogate.3\n",
      "2For linear models, this means the model just uses a few coordinates of the inputs to\n",
      "make an accurate prediction.\n",
      "3There has been a rich line of theoretical work that explains why k\u0012k1is a good sur-\n",
      "rogate for encouraging sparsity, but it's beyond the scope of this course. An intuition is:\n",
      "assuming the parameter is on the unit sphere, the parameter with smallest `1norm also\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "137\n",
      "TheR(\u0012) =k\u0012k1(also called LASSO) and R(\u0012) =1\n",
      "2k\u0012k2\n",
      "2are perhaps\n",
      "among the most commonly used regularizers for linear models. Other norm\n",
      "and powers of norms are sometimes also used. The `2norm regularization is\n",
      "much more commonly used with kernel methods because `1regularization is\n",
      "typically not compatible with the kernel trick (the optimal solution cannot\n",
      "be written as functions of inner products of features.)\n",
      "In deep learning, the most commonly used regularizer is `2regularization\n",
      "or weight decay. Other common ones include dropout, data augmentation,\n",
      "regularizing the spectral norm of the weight matrices, and regularizing the\n",
      "Lipschitzness of the model, etc. Regularization in deep learning is an ac-\n",
      "tive research area, and it's known that there is another implicit source of\n",
      "regularization, as discussed in the next section.\n",
      "9.2 Implicit regularization e\u000b",
      "ect\n",
      "The implicit regularization e\u000b",
      "ect of optimizers, or implicit bias or algorithmic\n",
      "regularization, is a new concept/phenomenon observed in the deep learning\n",
      "era. It largely refers to that the optimizers can implicitly impose structures\n",
      "on parameters beyond what has been imposed by the regularized loss.\n",
      "In most classical settings, the loss or regularized loss has a unique global\n",
      "minimum, and thus any reasonable optimizer should converge to that global\n",
      "minimum and cannot impose any additional preferences. However, in deep\n",
      "learning, oftentimes the loss or regularized loss has more than one (approx-\n",
      "imate) global minima, and di\u000b",
      "erence optimizers may converge to di\u000b",
      "erent\n",
      "global minima. Though these global minima have the same or similar train-\n",
      "ing losses, they may be of di\u000b",
      "erent nature and have dramatically di\u000b",
      "erent\n",
      "generalization performance. See Figures 9.1 and 9.2 and its caption for an\n",
      "illustration and some experiment results. For example, it's possible that one\n",
      "global minimum gives a much more Lipschitz or sparse model than others\n",
      "and thus has a better test error. It turns out that many commonly-used op-\n",
      "timizers (or their components) prefer or bias towards \f",
      "nding global minima\n",
      "of certain properties, leading to a better test performance.\n",
      "happen to be the sparsest parameter with only 1 non-zero coordinate. Thus, sparsity and\n",
      "`1norm gives the same extremal points to some extent.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "138\n",
      "θloss\n",
      "Figure 9.1: An Illustration that di\u000b",
      "erent global minima of the training loss\n",
      "can have di\u000b",
      "erent test performance.\n",
      "Figure 9.2: Left: Performance of neural networks trained by two di\u000b",
      "erent\n",
      "learning rates schedules on the CIFAR-10 dataset. Although both exper-\n",
      "iments used exactly the same regularized losses and the optimizers \f",
      "t the\n",
      "training data perfectly, the models' generalization performance di\u000b",
      "er much.\n",
      "Right: On a di\u000b",
      "erent synthetic dataset, optimizers with di\u000b",
      "erent initializa-\n",
      "tions have the same training error but di\u000b",
      "erent generalization performance.4\n",
      "In summary, the takehome message here is that the choice of optimizer\n",
      "does not only a\u000b",
      "ect minimizing the training loss, but also imposes implicit\n",
      "regularization and a\u000b",
      "ects the generalization of the model. Even if your cur-\n",
      "rent optimizer already converges to a small training error perfectly, you may\n",
      "still need to tune your optimizer for a better generalization, .\n",
      "4The setting is the same as in Woodworth et al. [2020], HaoChen et al. [2020]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "139\n",
      "One may wonder which components of the optimizers bias towards what\n",
      "type of global minima and what type of global minima may generalize bet-\n",
      "ter. These are open questions that researchers are actively investigating.\n",
      "Empirical and theoretical research have o\u000b",
      "ered some clues and heuristics.\n",
      "In many (but de\f",
      "nitely far from all) situations, among those setting where\n",
      "optimization can succeed in minimizing the training loss, the use of larger\n",
      "initial learning rate, smaller initialization, smaller batch size, and momen-\n",
      "tum appears to help with biasing towards more generalizable solutions. A\n",
      "conjecture (that can be proven in certain simpli\f",
      "ed case) is that stochas-\n",
      "atter globale optimization process help the optimizer to \f",
      "nd \n",
      "atnima (global minima where the curvature of the loss is small), and \n",
      "global minima tend to give more Lipschitz models and better generalization.\n",
      "Characterizing the implicit regularization e\u000b",
      "ect formally is still a challenging\n",
      "open research question.\n",
      "9.3 Model selection via cross validation\n",
      "Suppose we are trying select among several di\u000b",
      "erent models for a learning\n",
      "problem. For instance, we might be using a polynomial regression model\n",
      "h\u0012(x) =g(\u00120+\u00121x+\u00122x2+\u0001\u0001\u0001+\u0012kxk), and wish to decide if kshould be\n",
      "0, 1, . . . , or 10. How can we automatically select a model that represents\n",
      "a good tradeo\u000b",
      " between the twin evils of bias and variance5? Alternatively,\n",
      "suppose we want to automatically choose the bandwidth parameter \u001c",
      "for\n",
      "locally weighted regression, or the parameter Cfor our`1-regularized SVM.\n",
      "How can we do that?\n",
      "For the sake of concreteness, in these notes we assume we have some\n",
      "\f",
      "nite set of models M=fM1;:::;Mdgthat we're trying to select among.\n",
      "For instance, in our \f",
      "rst example above, the model Miwould be an i-th\n",
      "degree polynomial regression model. (The generalization to in\f",
      "nite Mis\n",
      "not hard.6) Alternatively, if we are trying to decide between using an SVM,\n",
      "a neural network or logistic regression, then Mmay contain these models.\n",
      "5Given that we said in the previous set of notes that bias and variance are two very\n",
      "di\u000b",
      "erent beasts, some readers may be wondering if we should be calling them \\twin\" evils\n",
      "here. Perhaps it'd be better to think of them as non-identical twins. The phrase \\the\n",
      "fraternal twin evils of bias and variance\" doesn't have the same ring to it, though.\n",
      "6If we are trying to choose from an in\f",
      "nite set of models, say corresponding to the\n",
      "possible values of the bandwidth \u001c",
      "2R+, we may discretize \u001c",
      "and consider only a \f",
      "nite\n",
      "number of possible values for it. More generally, most of the algorithms described here\n",
      "can all be viewed as performing optimization search in the space of models, and we can\n",
      "perform this search over in\f",
      "nite model classes as well.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "140\n",
      "Cross validation. Lets suppose we are, as usual, given a training set S.\n",
      "Given what we know about empirical risk minimization, here's what might\n",
      "initially seem like a algorithm, resulting from using empirical risk minimiza-\n",
      "tion for model selection:\n",
      "1. Train each model MionS, to get some hypothesis hi.\n",
      "2. Pick the hypotheses with the smallest training error.\n",
      "This algorithm does notwork. Consider choosing the degree of a poly-\n",
      "nomial. The higher the degree of the polynomial, the better it will \f",
      "t the\n",
      "training set S, and thus the lower the training error. Hence, this method will\n",
      "always select a high-variance, high-degree polynomial model, which we saw\n",
      "previously is often poor choice.\n",
      "Here's an algorithm that works better. In hold-out cross validation\n",
      "(also called simple cross validation ), we do the following:\n",
      "1. Randomly split SintoStrain(say, 70% of the data) and Scv(the remain-\n",
      "ing 30%). Here, Scvis called the hold-out cross validation set.\n",
      "2. Train each model MionStrainonly, to get some hypothesis hi.\n",
      "3. Select and output the hypothesis hithat had the smallest error ^ \"Scv(hi)\n",
      "on the hold out cross validation set. (Here ^ \"Scv(h) denotes the average\n",
      "error ofhon the set of examples in Scv.) The error on the hold out\n",
      "validation set is also referred to as the validation error.\n",
      "By testing/validating on a set of examples Scvthat the models were not\n",
      "trained on, we obtain a better estimate of each hypothesis hi's true general-\n",
      "ization/test error. Thus, this approach is essentially picking the model with\n",
      "the smallest estimated generalization/test error. The size of the validation\n",
      "set depends on the total number of available examples. Usually, somewhere\n",
      "between 1=4\u00001=3 of the data is used in the hold out cross validation set, and\n",
      "30% is a typical choice. However, when the total dataset is huge, validation\n",
      "set can be a smaller fraction of the total examples as long as the absolute\n",
      "number of validation examples is decent. For example, for the ImageNet\n",
      "dataset that has about 1M training images, the validation set is sometimes\n",
      "set to be 50K images, which is only about 5% of the total examples.\n",
      "Optionally, step 3 in the algorithm may also be replaced with selecting\n",
      "the modelMiaccording to arg min i^\"Scv(hi), and then retraining Mion the\n",
      "entire training set S. (This is often a good idea, with one exception being\n",
      "learning algorithms that are be very sensitive to perturbations of the initial\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "141\n",
      "conditions and/or data. For these methods, Midoing well on Straindoes not\n",
      "necessarily mean it will also do well on Scv, and it might be better to forgo\n",
      "this retraining step.)\n",
      "The disadvantage of using hold out cross validation is that it \\wastes\"\n",
      "about 30% of the data. Even if we were to take the optional step of retraining\n",
      "the model on the entire training set, it's still as if we're trying to \f",
      "nd a good\n",
      "model for a learning problem in which we had 0 :7ntraining examples, rather\n",
      "thanntraining examples, since we're testing models that were trained on\n",
      "only 0:7nexamples each time. While this is \f",
      "ne if data is abundant and/or\n",
      "cheap, in learning problems in which data is scarce (consider a problem with\n",
      "n= 20, say), we'd like to do something better.\n",
      "Here is a method, called k-fold cross validation , that holds out less\n",
      "data each time:\n",
      "1. Randomly split Sintokdisjoint subsets of m=k training examples each.\n",
      "Lets call these subsets S1;:::;Sk.\n",
      "2. For each model Mi, we evaluate it as follows:\n",
      "Forj= 1;:::;k\n",
      "Train the model MionS1[\u0001\u0001\u0001[Sj\u00001[Sj+1[\u0001\u0001\u0001Sk(i.e., train\n",
      "on all the data except Sj) to get some hypothesis hij.\n",
      "Test the hypothesis hijonSj, to get ^\"Sj(hij).\n",
      "The estimated generalization error of model Miis then calculated\n",
      "as the average of the ^ \"Sj(hij)'s (averaged over j).\n",
      "3. Pick the model Miwith the lowest estimated generalization error, and\n",
      "retrain that model on the entire training set S. The resulting hypothesis\n",
      "is then output as our \f",
      "nal answer.\n",
      "A typical choice for the number of folds to use here would be k= 10.\n",
      "While the fraction of data held out each time is now 1 =k|much smaller\n",
      "than before|this procedure may also be more computationally expensive\n",
      "than hold-out cross validation, since we now need train to each model k\n",
      "times.\n",
      "Whilek= 10 is a commonly used choice, in problems in which data is\n",
      "really scarce, sometimes we will use the extreme choice of k=min order\n",
      "to leave out as little data as possible each time. In this setting, we would\n",
      "repeatedly train on all but one of the training examples in S, and test on that\n",
      "held-out example. The resulting m=kerrors are then averaged together to\n",
      "obtain our estimate of the generalization error of a model. This method has\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "142\n",
      "its own name; since we're holding out one training example at a time, this\n",
      "method is called leave-one-out cross validation.\n",
      "Finally, even though we have described the di\u000b",
      "erent versions of cross vali-\n",
      "dation as methods for selecting a model, they can also be used more simply to\n",
      "evaluate a single model or algorithm. For example, if you have implemented\n",
      "some learning algorithm and want to estimate how well it performs for your\n",
      "application (or if you have invented a novel learning algorithm and want to\n",
      "report in a technical paper how well it performs on various test sets), cross\n",
      "validation would give a reasonable way of doing so.\n",
      "9.4 Bayesian statistics and regularization\n",
      "In this section, we will talk about one more tool in our arsenal for our battle\n",
      "against over\f",
      "tting.\n",
      "At the beginning of the quarter, we talked about parameter \f",
      "tting using\n",
      "maximum likelihood estimation (MLE), and chose our parameters according\n",
      "to\n",
      "\u0012MLE= arg max\n",
      "\u0012nY\n",
      "i=1p(y(i)jx(i);\u0012):\n",
      "Throughout our subsequent discussions, we viewed \u0012as an unknown param-\n",
      "eter of the world. This view of the \u0012as being constant-valued but unknown\n",
      "is taken in frequentist statistics. In the frequentist this view of the world, \u0012\n",
      "is not random|it just happens to be unknown|and it's our job to come up\n",
      "with statistical procedures (such as maximum likelihood) to try to estimate\n",
      "this parameter.\n",
      "An alternative way to approach our parameter estimation problems is to\n",
      "take the Bayesian view of the world, and think of \u0012as being a random\n",
      "variable whose value is unknown. In this approach, we would specify a\n",
      "prior distribution p(\u0012) on\u0012that expresses our \\prior beliefs\" about the\n",
      "parameters. Given a training set S=f(x(i);y(i))gn\n",
      "i=1, when we are asked to\n",
      "make a prediction on a new value of x, we can then compute the posterior\n",
      "distribution on the parameters\n",
      "p(\u0012jS) =p(Sj\u0012)p(\u0012)\n",
      "p(S)\n",
      "=\u0000Qn\n",
      "i=1p(y(i)jx(i);\u0012)\u0001\n",
      "p(\u0012)R\n",
      "\u0012(Qn\n",
      "i=1p(y(i)jx(i);\u0012)p(\u0012))d\u0012(9.3)\n",
      "In the equation above, p(y(i)jx(i);\u0012) comes from whatever model you're using\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "143\n",
      "for your learning problem. For example, if you are using Bayesian logistic re-\n",
      "gression, then you might choose p(y(i)jx(i);\u0012) =h\u0012(x(i))y(i)(1\u0000h\u0012(x(i)))(1\u0000y(i)),\n",
      "whereh\u0012(x(i)) = 1=(1 + exp(\u0000\u0012Tx(i))).7\n",
      "When we are given a new test example xand asked to make it prediction\n",
      "on it, we can compute our posterior distribution on the class label using the\n",
      "posterior distribution on \u0012:\n",
      "p(yjx;S) =Z\n",
      "\u0012p(yjx;\u0012)p(\u0012jS)d\u0012 (9.4)\n",
      "In the equation above, p(\u0012jS) comes from Equation (9.3). Thus, for example,\n",
      "if the goal is to the predict the expected value of ygivenx, then we would\n",
      "output8\n",
      "E[yjx;S] =Z\n",
      "yyp(yjx;S)dy\n",
      "The procedure that we've outlined here can be thought of as doing \\fully\n",
      "Bayesian\" prediction, where our prediction is computed by taking an average\n",
      "with respect to the posterior p(\u0012jS) over\u0012. Unfortunately, in general it is\n",
      "computationally very di\u000ecult to compute this posterior distribution. This is\n",
      "because it requires taking integrals over the (usually high-dimensional) \u0012as\n",
      "in Equation (9.3), and this typically cannot be done in closed-form.\n",
      "Thus, in practice we will instead approximate the posterior distribution\n",
      "for\u0012. One common approximation is to replace our posterior distribution for\n",
      "\u0012(as in Equation 9.4) with a single point estimate. The MAP (maximum\n",
      "a posteriori) estimate for \u0012is given by\n",
      "\u0012MAP= arg max\n",
      "\u0012nY\n",
      "i=1p(y(i)jx(i);\u0012)p(\u0012): (9.5)\n",
      "Note that this is the same formulas as for the MLE (maximum likelihood)\n",
      "estimate for \u0012, except for the prior p(\u0012) term at the end.\n",
      "In practical applications, a common choice for the prior p(\u0012) is to assume\n",
      "that\u0012\u0018N(0;\u001c",
      "2I). Using this choice of prior, the \f",
      "tted parameters \u0012MAPwill\n",
      "have smaller norm than that selected by maximum likelihood. In practice,\n",
      "this causes the Bayesian MAP estimate to be less susceptible to over\f",
      "tting\n",
      "than the ML estimate of the parameters. For example, Bayesian logistic\n",
      "regression turns out to be an e\u000b",
      "ective algorithm for text classi\f",
      "cation, even\n",
      "though in text classi\f",
      "cation we usually have d\u001d",
      "n.\n",
      "7Since we are now viewing \u0012as a random variable, it is okay to condition on it value,\n",
      "and write \\ p(yjx;\u0012)\" instead of \\ p(yjx;\u0012).\"\n",
      "8The integral below would be replaced by a summation if yis discrete-valued.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Part IV\n",
      "Unsupervised learning\n",
      "144\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 10\n",
      "Clustering and the k-means\n",
      "algorithm\n",
      "In the clustering problem, we are given a training set fx(1);:::;x(n)g, and\n",
      "want to group the data into a few cohesive \\clusters.\" Here, x(i)2Rd\n",
      "as usual; but no labels y(i)are given. So, this is an unsupervised learning\n",
      "problem.\n",
      "Thek-means clustering algorithm is as follows:\n",
      "1. Initialize cluster centroids \u00161;\u00162;:::;\u0016k2Rdrandomly.\n",
      "2. Repeat until convergence: f\n",
      "For everyi, set\n",
      "c(i):= arg min\n",
      "jjjx(i)\u0000\u0016jjj2:\n",
      "For eachj, set\n",
      "\u0016j:=Pn\n",
      "i=11fc(i)=jgx(i)\n",
      "Pn\n",
      "i=11fc(i)=jg:\n",
      "g\n",
      "In the algorithm above, k(a parameter of the algorithm) is the number\n",
      "of clusters we want to \f",
      "nd; and the cluster centroids \u0016jrepresent our current\n",
      "guesses for the positions of the centers of the clusters. To initialize the cluster\n",
      "centroids (in step 1 of the algorithm above), we could choose ktraining\n",
      "examples randomly, and set the cluster centroids to be equal to the values of\n",
      "thesekexamples. (Other initialization methods are also possible.)\n",
      "The inner-loop of the algorithm repeatedly carries out two steps: (i)\n",
      "\\Assigning\" each training example x(i)to the closest cluster centroid \u0016j, and\n",
      "145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "146\n",
      "Figure 10.1: K-means algorithm. Training examples are shown as dots, and\n",
      "cluster centroids are shown as crosses. (a) Original dataset. (b) Random ini-\n",
      "tial cluster centroids (in this instance, not chosen to be equal to two training\n",
      "examples). (c-f) Illustration of running two iterations of k-means. In each\n",
      "iteration, we assign each training example to the closest cluster centroid\n",
      "(shown by \\painting\" the training examples the same color as the cluster\n",
      "centroid to which is assigned); then we move each cluster centroid to the\n",
      "mean of the points assigned to it. (Best viewed in color.) Images courtesy\n",
      "Michael Jordan.\n",
      "(ii) Moving each cluster centroid \u0016jto the mean of the points assigned to it.\n",
      "Figure 10.1 shows an illustration of running k-means.\n",
      "Is thek-means algorithm guaranteed to converge? Yes it is, in a certain\n",
      "sense. In particular, let us de\f",
      "ne the distortion function to be:\n",
      "J(c;\u0016) =nX\n",
      "i=1jjx(i)\u0000\u0016c(i)jj2\n",
      "Thus,Jmeasures the sum of squared distances between each training exam-\n",
      "plex(i)and the cluster centroid \u0016c(i)to which it has been assigned. It can\n",
      "be shown that k-means is exactly coordinate descent on J. Speci\f",
      "cally, the\n",
      "inner-loop of k-means repeatedly minimizes Jwith respect to cwhile holding\n",
      "\u0016\f",
      "xed, and then minimizes Jwith respect to \u0016while holding c\f",
      "xed. Thus,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "147\n",
      "Jmust monotonically decrease, and the value of Jmust converge. (Usu-\n",
      "ally, this implies that cand\u0016will converge too. In theory, it is possible for\n",
      "k-means to oscillate between a few di\u000b",
      "erent clusterings|i.e., a few di\u000b",
      "erent\n",
      "values forcand/or\u0016|that have exactly the same value of J, but this almost\n",
      "never happens in practice.)\n",
      "The distortion function Jis a non-convex function, and so coordinate\n",
      "descent on Jis not guaranteed to converge to the global minimum. In other\n",
      "words,k-means can be susceptible to local optima. Very often k-means will\n",
      "work \f",
      "ne and come up with very good clusterings despite this. But if you\n",
      "are worried about getting stuck in bad local minima, one common thing to\n",
      "do is runk-means many times (using di\u000b",
      "erent random initial values for the\n",
      "cluster centroids \u0016j). Then, out of all the di\u000b",
      "erent clusterings found, pick\n",
      "the one that gives the lowest distortion J(c;\u0016).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 11\n",
      "EM algorithms\n",
      "In this set of notes, we discuss the EM (Expectation-Maximization) algorithm\n",
      "for density estimation.\n",
      "11.1 EM for mixture of Gaussians\n",
      "Suppose that we are given a training set fx(1);:::;x(n)gas usual. Since we\n",
      "are in the unsupervised learning setting, these points do not come with any\n",
      "labels.\n",
      "We wish to model the data by specifying a joint distribution p(x(i);z(i)) =\n",
      "p(x(i)jz(i))p(z(i)). Here,z(i)\u0018Multinomial( \u001e",
      ") (where\u001e",
      "j\u00150,Pk\n",
      "j=1\u001e",
      "j= 1,\n",
      "and the parameter \u001e",
      "jgivesp(z(i)=j)), andx(i)jz(i)=j\u0018N (\u0016j;\u0006j). We\n",
      "letkdenote the number of values that the z(i)'s can take on. Thus, our\n",
      "model posits that each x(i)was generated by randomly choosing z(i)from\n",
      "f1;:::;kg, and then x(i)was drawn from one of kGaussians depending on\n",
      "z(i). This is called the mixture of Gaussians model. Also, note that the\n",
      "z(i)'s are latent random variables, meaning that they're hidden/unobserved.\n",
      "This is what will make our estimation problem di\u000ecult.\n",
      "The parameters of our model are thus \u001e",
      ",\u0016and \u0006. To estimate them, we\n",
      "can write down the likelihood of our data:\n",
      "`(\u001e",
      ";\u0016; \u0006) =nX\n",
      "i=1logp(x(i);\u001e",
      ";\u0016; \u0006)\n",
      "=nX\n",
      "i=1logkX\n",
      "z(i)=1p(x(i)jz(i);\u0016;\u0006)p(z(i);\u001e",
      "):\n",
      "However, if we set to zero the derivatives of this formula with respect to\n",
      "148\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "149\n",
      "the parameters and try to solve, we'll \f",
      "nd that it is not possible to \f",
      "nd the\n",
      "maximum likelihood estimates of the parameters in closed form. (Try this\n",
      "yourself at home.)\n",
      "The random variables z(i)indicate which of the kGaussians each x(i)\n",
      "had come from. Note that if we knew what the z(i)'s were, the maximum\n",
      "likelihood problem would have been easy. Speci\f",
      "cally, we could then write\n",
      "down the likelihood as\n",
      "`(\u001e",
      ";\u0016; \u0006) =nX\n",
      "i=1logp(x(i)jz(i);\u0016;\u0006) + logp(z(i);\u001e",
      "):\n",
      "Maximizing this with respect to \u001e",
      ",\u0016and \u0006 gives the parameters:\n",
      "\u001e",
      "j=1\n",
      "nnX\n",
      "i=11fz(i)=jg;\n",
      "\u0016j=Pn\n",
      "i=11fz(i)=jgx(i)\n",
      "Pn\n",
      "i=11fz(i)=jg;\n",
      "\u0006j=Pn\n",
      "i=11fz(i)=jg(x(i)\u0000\u0016j)(x(i)\u0000\u0016j)T\n",
      "Pn\n",
      "i=11fz(i)=jg:\n",
      "Indeed, we see that if the z(i)'s were known, then maximum likelihood\n",
      "estimation becomes nearly identical to what we had when estimating the\n",
      "parameters of the Gaussian discriminant analysis model, except that here\n",
      "thez(i)'s playing the role of the class labels.1\n",
      "However, in our density estimation problem, the z(i)'s are notknown.\n",
      "What can we do?\n",
      "The EM algorithm is an iterative algorithm that has two main steps.\n",
      "Applied to our problem, in the E-step, it tries to \\guess\" the values of the\n",
      "z(i)'s. In the M-step, it updates the parameters of our model based on our\n",
      "guesses. Since in the M-step we are pretending that the guesses in the \f",
      "rst\n",
      "part were correct, the maximization becomes easy. Here's the algorithm:\n",
      "Repeat until convergence: f\n",
      "(E-step) For each i;j, set\n",
      "w(i)\n",
      "j:=p(z(i)=jjx(i);\u001e",
      ";\u0016; \u0006)\n",
      "1There are other minor di\u000b",
      "erences in the formulas here from what we'd obtained in\n",
      "PS1 with Gaussian discriminant analysis, \f",
      "rst because we've generalized the z(i)'s to be\n",
      "multinomial rather than Bernoulli, and second because here we are using a di\u000b",
      "erent \u0006 j\n",
      "for each Gaussian.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "150\n",
      "(M-step) Update the parameters:\n",
      "\u001e",
      "j:=1\n",
      "nnX\n",
      "i=1w(i)\n",
      "j;\n",
      "\u0016j:=Pn\n",
      "i=1w(i)\n",
      "jx(i)\n",
      "Pn\n",
      "i=1w(i)\n",
      "j;\n",
      "\u0006j:=Pn\n",
      "i=1w(i)\n",
      "j(x(i)\u0000\u0016j)(x(i)\u0000\u0016j)T\n",
      "Pn\n",
      "i=1w(i)\n",
      "j\n",
      "g\n",
      "In the E-step, we calculate the posterior probability of our parameters\n",
      "thez(i)'s, given the x(i)and using the current setting of our parameters. I.e.,\n",
      "using Bayes rule, we obtain:\n",
      "p(z(i)=jjx(i);\u001e",
      ";\u0016; \u0006) =p(x(i)jz(i)=j;\u0016;\u0006)p(z(i)=j;\u001e",
      ")Pk\n",
      "l=1p(x(i)jz(i)=l;\u0016;\u0006)p(z(i)=l;\u001e",
      ")\n",
      "Here,p(x(i)jz(i)=j;\u0016;\u0006) is given by evaluating the density of a Gaussian\n",
      "with mean \u0016jand covariance \u0006 jatx(i);p(z(i)=j;\u001e",
      ") is given by \u001e",
      "j, and so\n",
      "on. The values w(i)\n",
      "jcalculated in the E-step represent our \\soft\" guesses2for\n",
      "the values of z(i).\n",
      "Also, you should contrast the updates in the M-step with the formulas we\n",
      "had when the z(i)'s were known exactly. They are identical, except that in-\n",
      "stead of the indicator functions \\1 fz(i)=jg\" indicating from which Gaussian\n",
      "each datapoint had come, we now instead have the w(i)\n",
      "j's.\n",
      "The EM-algorithm is also reminiscent of the K-means clustering algo-\n",
      "rithm, except that instead of the \\hard\" cluster assignments c(i), we instead\n",
      "have the \\soft\" assignments w(i)\n",
      "j. Similar to K-means, it is also susceptible\n",
      "to local optima, so reinitializing at several di\u000b",
      "erent initial parameters may\n",
      "be a good idea.\n",
      "It's clear that the EM algorithm has a very natural interpretation of\n",
      "repeatedly trying to guess the unknown z(i)'s; but how did it come about,\n",
      "and can we make any guarantees about it, such as regarding its convergence?\n",
      "In the next set of notes, we will describe a more general view of EM, one\n",
      "2The term \\soft\" refers to our guesses being probabilities and taking values in [0 ;1]; in\n",
      "contrast, a \\hard\" guess is one that represents a single best guess (such as taking values\n",
      "inf0;1gorf1;:::;kg).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "151\n",
      "that will allow us to easily apply it to other estimation problems in which\n",
      "there are also latent variables, and which will allow us to give a convergence\n",
      "guarantee.\n",
      "11.2 Jensen's inequality\n",
      "We begin our discussion with a very useful result called Jensen's inequality\n",
      "Letfbe a function whose domain is the set of real numbers. Recall that\n",
      "fis a convex function if f00(x)\u00150 (for allx2R). In the case of ftaking\n",
      "vector-valued inputs, this is generalized to the condition that its hessian H\n",
      "is positive semi-de\f",
      "nite ( H\u00150). Iff00(x)>0 for allx, then we say fis\n",
      "strictly convex (in the vector-valued case, the corresponding statement is\n",
      "thatHmust be positive de\f",
      "nite, written H > 0). Jensen's inequality can\n",
      "then be stated as follows:\n",
      "Theorem. Letfbe a convex function, and let Xbe a random variable.\n",
      "Then:\n",
      "E[f(X)]\u0015f(EX):\n",
      "Moreover, if fis strictly convex, then E[ f(X)] =f(EX) holds true if and\n",
      "only ifX= E[X] with probability 1 (i.e., if Xis a constant).\n",
      "Recall our convention of occasionally dropping the parentheses when writ-\n",
      "ing expectations, so in the theorem above, f(EX) =f(E[X]).\n",
      "For an interpretation of the theorem, consider the \f",
      "gure below.\n",
      "a E[X] bf(a)\n",
      "f(b)\n",
      "f(EX)E[f(X)]f\n",
      "Here,fis a convex function shown by the solid line. Also, Xis a random\n",
      "variable that has a 0.5 chance of taking the value a, and a 0.5 chance of\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "152\n",
      "taking the value b(indicated on the x-axis). Thus, the expected value of X\n",
      "is given by the midpoint between aandb.\n",
      "We also see the values f(a),f(b) andf(E[X]) indicated on the y-axis.\n",
      "Moreover, the value E[ f(X)] is now the midpoint on the y-axis between f(a)\n",
      "andf(b). From our example, we see that because fis convex, it must be the\n",
      "case that E[ f(X)]\u0015f(EX).\n",
      "Incidentally, quite a lot of people have trouble remembering which way\n",
      "the inequality goes, and remembering a picture like this is a good way to\n",
      "quickly \f",
      "gure out the answer.\n",
      "Remark. Recall that fis [strictly] concave if and only if \u0000fis [strictly]\n",
      "convex (i.e., f00(x)\u00140 orH\u00140). Jensen's inequality also holds for concave\n",
      "functionsf, but with the direction of all the inequalities reversed (E[ f(X)]\u0014\n",
      "f(EX), etc.).\n",
      "11.3 General EM algorithms\n",
      "Suppose we have an estimation problem in which we have a training set\n",
      "fx(1);:::;x(n)gconsisting of nindependent examples. We have a latent vari-\n",
      "able model p(x;z;\u0012) withzbeing the latent variable (which for simplicity is\n",
      "assumed to take \f",
      "nite number of values). The density for xcan be obtained\n",
      "by marginalized over the latent variable z:\n",
      "p(x;\u0012) =X\n",
      "zp(x;z;\u0012) (11.1)\n",
      "We wish to \f",
      "t the parameters \u0012by maximizing the log-likelihood of the\n",
      "data, de\f",
      "ned by\n",
      "`(\u0012) =nX\n",
      "i=1logp(x(i);\u0012) (11.2)\n",
      "We can rewrite the objective in terms of the joint density p(x;z;\u0012) by\n",
      "`(\u0012) =nX\n",
      "i=1logp(x(i);\u0012) (11.3)\n",
      "=nX\n",
      "i=1logX\n",
      "z(i)p(x(i);z(i);\u0012): (11.4)\n",
      "But, explicitly \f",
      "nding the maximum likelihood estimates of the parameters\n",
      "\u0012may be hard since it will result in di\u000ecult non-convex optimization prob-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "153\n",
      "lems.3Here, thez(i)'s are the latent random variables; and it is often the case\n",
      "that if the z(i)'s were observed, then maximum likelihood estimation would\n",
      "be easy.\n",
      "In such a setting, the EM algorithm gives an e\u000ecient method for max-\n",
      "imum likelihood estimation. Maximizing `(\u0012) explicitly might be di\u000ecult,\n",
      "and our strategy will be to instead repeatedly construct a lower-bound on `\n",
      "(E-step), and then optimize that lower-bound (M-step).4\n",
      "It turns out that the summationPn\n",
      "i=1is not essential here, and towards a\n",
      "simpler exposition of the EM algorithm, we will \f",
      "rst consider optimizing the\n",
      "the likelihood log p(x) fora single example x. After we derive the algorithm\n",
      "for optimizing log p(x), we will convert it to an algorithm that works for n\n",
      "examples by adding back the sum to each of the relevant equations. Thus,\n",
      "now we aim to optimize log p(x;\u0012) which can be rewritten as\n",
      "logp(x;\u0012) = logX\n",
      "zp(x;z;\u0012) (11.5)\n",
      "LetQbe a distribution over the possible values of z. That is,P\n",
      "zQ(z) = 1,\n",
      "Q(z)\u00150).\n",
      "Consider the following:5\n",
      "logp(x;\u0012) = logX\n",
      "zp(x;z;\u0012)\n",
      "= logX\n",
      "zQ(z)p(x;z;\u0012)\n",
      "Q(z)(11.6)\n",
      "\u0015X\n",
      "zQ(z) logp(x;z;\u0012)\n",
      "Q(z)(11.7)\n",
      "The last step of this derivation used Jensen's inequality. Speci\f",
      "cally,\n",
      "f(x) = logxis a concave function, since f00(x) =\u00001=x2<0 over its domain\n",
      "3It's mostly an empirical observation that the optimization problem is di\u000ecult to op-\n",
      "timize.\n",
      "4Empirically, the E-step and M-step can often be computed more e\u000eciently than op-\n",
      "timizing the function `(\u0001) directly. However, it doesn't necessarily mean that alternating\n",
      "the two steps can always converge to the global optimum of `(\u0001). Even for mixture of\n",
      "Gaussians, the EM algorithm can either converge to a global optimum or get stuck, de-\n",
      "pending on the properties of the training data. Empirically, for real-world data, often EM\n",
      "can converge to a solution with relatively high likelihood (if not the optimum), and the\n",
      "theory behind it is still largely not understood.\n",
      "5Ifzwere continuous, then Qwould be a density, and the summations over zin our\n",
      "discussion are replaced with integrals over z.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "154\n",
      "x2R+. Also, the term\n",
      "X\n",
      "zQ(z)\u0014p(x;z;\u0012)\n",
      "Q(z)\u0015\n",
      "in the summation is just an expectation of the quantity [ p(x;z;\u0012)=Q(z)] with\n",
      "respect tozdrawn according to the distribution given by Q.6By Jensen's\n",
      "inequality, we have\n",
      "f\u0012\n",
      "Ez\u0018Q\u0014p(x;z;\u0012)\n",
      "Q(z)\u0015\u0013\n",
      "\u0015Ez\u0018Q\u0014\n",
      "f\u0012p(x;z;\u0012)\n",
      "Q(z)\u0013\u0015\n",
      ";\n",
      "where the \\ z\u0018Q\" subscripts above indicate that the expectations are with\n",
      "respect tozdrawn from Q. This allowed us to go from Equation (11.6) to\n",
      "Equation (11.7).\n",
      "Now, for any distribution Q, the formula (11.7) gives a lower-bound on\n",
      "logp(x;\u0012). There are many possible choices for the Q's. Which should we\n",
      "choose? Well, if we have some current guess \u0012of the parameters, it seems\n",
      "natural to try to make the lower-bound tight at that value of \u0012. I.e., we will\n",
      "make the inequality above hold with equality at our particular value of \u0012.\n",
      "To make the bound tight for a particular value of \u0012, we need for the step\n",
      "involving Jensen's inequality in our derivation above to hold with equality.\n",
      "For this to be true, we know it is su\u000ecient that the expectation be taken\n",
      "over a \\constant\"-valued random variable. I.e., we require that\n",
      "p(x;z;\u0012)\n",
      "Q(z)=c\n",
      "for some constant cthat does not depend on z. This is easily accomplished\n",
      "by choosing\n",
      "Q(z)/p(x;z;\u0012):\n",
      "Actually, since we knowP\n",
      "zQ(z) = 1 (because it is a distribution), this\n",
      "further tells us that\n",
      "Q(z) =p(x;z;\u0012)P\n",
      "zp(x;z;\u0012)\n",
      "=p(x;z;\u0012)\n",
      "p(x;\u0012)\n",
      "=p(zjx;\u0012) (11.8)\n",
      "6We note that the notionp(x;z;\u0012)\n",
      "Q(z)only makes sense if Q(z)6= 0 whenever p(x;z;\u0012)6= 0.\n",
      "Here we implicitly assume that we only consider those Qwith such a property.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "155\n",
      "Thus, we simply set the Q's to be the posterior distribution of the z's given\n",
      "xand the setting of the parameters \u0012.\n",
      "Indeed, we can directly verify that when Q(z) =p(zjx;\u0012), then equa-\n",
      "tion (11.7) is an equality because\n",
      "X\n",
      "zQ(z) logp(x;z;\u0012)\n",
      "Q(z)=X\n",
      "zp(zjx;\u0012) logp(x;z;\u0012)\n",
      "p(zjx;\u0012)\n",
      "=X\n",
      "zp(zjx;\u0012) logp(zjx;\u0012)p(x;\u0012)\n",
      "p(zjx;\u0012)\n",
      "=X\n",
      "zp(zjx;\u0012) logp(x;\u0012)\n",
      "= logp(x;\u0012)X\n",
      "zp(zjx;\u0012)\n",
      "= logp(x;\u0012) (becauseP\n",
      "zp(zjx;\u0012) = 1)\n",
      "For convenience, we call the expression in Equation (11.7) the evidence\n",
      "lower bound (ELBO) and we denote it by\n",
      "ELBO(x;Q;\u0012) =X\n",
      "zQ(z) logp(x;z;\u0012)\n",
      "Q(z)(11.9)\n",
      "With this equation, we can re-write equation (11.7) as\n",
      "8Q;\u0012;x; logp(x;\u0012)\u0015ELBO(x;Q;\u0012) (11.10)\n",
      "Intuitively, the EM algorithm alternatively updates Qand\u0012by a) set-\n",
      "tingQ(z) =p(zjx;\u0012) following Equation (11.8) so that ELBO( x;Q;\u0012) =\n",
      "logp(x;\u0012) forxand the current \u0012, and b) maximizing ELBO( x;Q;\u0012) w.r.t\u0012\n",
      "while \f",
      "xing the choice of Q.\n",
      "Recall that all the discussion above was under the assumption that we\n",
      "aim to optimize the log-likelihood log p(x;\u0012) for a single example x. It turns\n",
      "out that with multiple training examples, the basic idea is the same and we\n",
      "only needs to take a sum over examples at relevant places. Next, we will\n",
      "build the evidence lower bound for multiple training examples and make the\n",
      "EM algorithm formal.\n",
      "Recall we have a training set fx(1);:::;x(n)g. Note that the optimal choice\n",
      "ofQisp(zjx;\u0012), and it depends on the particular example x. Therefore here\n",
      "we will introduce ndistributions Q1;:::;Qn, one for each example x(i). For\n",
      "each example x(i), we can build the evidence lower bound\n",
      "logp(x(i);\u0012)\u0015ELBO(x(i);Qi;\u0012) =X\n",
      "z(i)Qi(z(i)) logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "156\n",
      "Taking sum over all the examples, we obtain a lower bound for the log-\n",
      "likelihood\n",
      "`(\u0012)\u0015X\n",
      "iELBO(x(i);Qi;\u0012) (11.11)\n",
      "=X\n",
      "iX\n",
      "z(i)Qi(z(i)) logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\n",
      "Foranyset of distributions Q1;:::;Qn, the formula (11.11) gives a lower-\n",
      "bound on`(\u0012), and analogous to the argument around equation (11.8), the\n",
      "Qithat attains equality satis\f",
      "es\n",
      "Qi(z(i)) =p(z(i)jx(i);\u0012)\n",
      "Thus, we simply set the Qi's to be the posterior distribution of the z(i)'s\n",
      "givenx(i)with the current setting of the parameters \u0012.\n",
      "Now, for this choice of the Qi's, Equation (11.11) gives a lower-bound on\n",
      "the loglikelihood `that we're trying to maximize. This is the E-step. In the\n",
      "M-step of the algorithm, we then maximize our formula in Equation (11.11)\n",
      "with respect to the parameters to obtain a new setting of the \u0012's. Repeatedly\n",
      "carrying out these two steps gives us the EM algorithm, which is as follows:\n",
      "Repeat until convergence f\n",
      "(E-step) For each i, set\n",
      "Qi(z(i)) :=p(z(i)jx(i);\u0012):\n",
      "(M-step) Set\n",
      "\u0012:= arg max\n",
      "\u0012nX\n",
      "i=1ELBO(x(i);Qi;\u0012)\n",
      "= arg max\n",
      "\u0012X\n",
      "iX\n",
      "z(i)Qi(z(i)) logp(x(i);z(i);\u0012)\n",
      "Qi(z(i)): (11.12)\n",
      "g\n",
      "How do we know if this algorithm will converge? Well, suppose \u0012(t)and\n",
      "\u0012(t+1)are the parameters from two successive iterations of EM. We will now\n",
      "prove that `(\u0012(t))\u0014`(\u0012(t+1)), which shows EM always monotonically im-\n",
      "proves the log-likelihood. The key to showing this result lies in our choice of\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "157\n",
      "theQi's. Speci\f",
      "cally, on the iteration of EM in which the parameters had\n",
      "started out as \u0012(t), we would have chosen Q(t)\n",
      "i(z(i)) :=p(z(i)jx(i);\u0012(t)). We\n",
      "saw earlier that this choice ensures that Jensen's inequality, as applied to get\n",
      "Equation (11.11), holds with equality, and hence\n",
      "`(\u0012(t)) =nX\n",
      "i=1ELBO(x(i);Q(t)\n",
      "i;\u0012(t)) (11.13)\n",
      "The parameters \u0012(t+1)are then obtained by maximizing the right hand side\n",
      "of the equation above. Thus,\n",
      "`(\u0012(t+1))\u0015nX\n",
      "i=1ELBO(x(i);Q(t)\n",
      "i;\u0012(t+1))\n",
      "(because ineqaulity (11.11) holds for all Qand\u0012)\n",
      "\u0015nX\n",
      "i=1ELBO(x(i);Q(t)\n",
      "i;\u0012(t)) (see reason below)\n",
      "=`(\u0012(t)) (by equation (11.13))\n",
      "where the last inequality follows from that \u0012(t+1)is chosen explicitly to be\n",
      "arg max\n",
      "\u0012nX\n",
      "i=1ELBO(x(i);Q(t)\n",
      "i;\u0012)\n",
      "Hence, EM causes the likelihood to converge monotonically. In our de-\n",
      "scription of the EM algorithm, we said we'd run it until convergence. Given\n",
      "the result that we just showed, one reasonable convergence test would be\n",
      "to check if the increase in `(\u0012) between successive iterations is smaller than\n",
      "some tolerance parameter, and to declare convergence if EM is improving\n",
      "`(\u0012) too slowly.\n",
      "Remark. If we de\f",
      "ne (by overloading ELBO( \u0001))\n",
      "ELBO(Q;\u0012) =nX\n",
      "i=1ELBO(x(i);Qi;\u0012) =X\n",
      "iX\n",
      "z(i)Qi(z(i)) logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\n",
      "(11.14)\n",
      "then we know `(\u0012)\u0015ELBO(Q;\u0012) from our previous derivation. The EM\n",
      "can also be viewed an alternating maximization algorithm on ELBO( Q;\u0012),\n",
      "in which the E-step maximizes it with respect to Q(check this yourself), and\n",
      "the M-step maximizes it with respect to \u0012.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "158\n",
      "11.3.1 Other interpretation of ELBO\n",
      "Let ELBO( x;Q;\u0012) =P\n",
      "zQ(z) logp(x;z;\u0012)\n",
      "Q(z)be de\f",
      "ned as in equation (11.9).\n",
      "There are several other forms of ELBO. First, we can rewrite\n",
      "ELBO(x;Q;\u0012) = Ez\u0018Q[logp(x;z;\u0012)]\u0000Ez\u0018Q[logQ(z)]\n",
      "= Ez\u0018Q[logp(xjz;\u0012)]\u0000DKL(Qkpz) (11.15)\n",
      "where we use pzto denote the marginal distribution of z(under the distri-\n",
      "butionp(x;z;\u0012)), andDKL() denotes the KL divergence\n",
      "DKL(Qkpz) =X\n",
      "zQ(z) logQ(z)\n",
      "p(z)(11.16)\n",
      "In many cases, the marginal distribution of zdoes not depend on the param-\n",
      "eter\u0012. In this case, we can see that maximizing ELBO over \u0012is equivalent\n",
      "to maximizing the \f",
      "rst term in (11.15). This corresponds to maximizing the\n",
      "conditional likelihood of xconditioned on z, which is often a simpler question\n",
      "than the original question.\n",
      "Another form of ELBO( \u0001) is (please verify yourself)\n",
      "ELBO(x;Q;\u0012) = logp(x)\u0000DKL(Qkpzjx) (11.17)\n",
      "wherepzjxis the conditional distribution of zgivenxunder the parameter\n",
      "\u0012. This forms shows that the maximizer of ELBO( Q;\u0012) overQis obtained\n",
      "whenQ=pzjx, which was shown in equation (11.8) before.\n",
      "11.4 Mixture of Gaussians revisited\n",
      "Armed with our general de\f",
      "nition of the EM algorithm, let's go back to our\n",
      "old example of \f",
      "tting the parameters \u001e",
      ",\u0016and \u0006 in a mixture of Gaussians.\n",
      "For the sake of brevity, we carry out the derivations for the M-step updates\n",
      "only for\u001e",
      "and\u0016j, and leave the updates for \u0006 jas an exercise for the reader.\n",
      "The E-step is easy. Following our algorithm derivation above, we simply\n",
      "calculate\n",
      "w(i)\n",
      "j=Qi(z(i)=j) =P(z(i)=jjx(i);\u001e",
      ";\u0016; \u0006):\n",
      "Here, \\Qi(z(i)=j)\" denotes the probability of z(i)taking the value junder\n",
      "the distribution Qi.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "159\n",
      "Next, in the M-step, we need to maximize, with respect to our parameters\n",
      "\u001e",
      ";\u0016; \u0006, the quantity\n",
      "nX\n",
      "i=1X\n",
      "z(i)Qi(z(i)) logp(x(i);z(i);\u001e",
      ";\u0016; \u0006)\n",
      "Qi(z(i))\n",
      "=nX\n",
      "i=1kX\n",
      "j=1Qi(z(i)=j) logp(x(i)jz(i)=j;\u0016;\u0006)p(z(i)=j;\u001e",
      ")\n",
      "Qi(z(i)=j)\n",
      "=nX\n",
      "i=1kX\n",
      "j=1w(i)\n",
      "jlog1\n",
      "(2\u0019)d=2j\u0006jj1=2exp\u0000\n",
      "\u00001\n",
      "2(x(i)\u0000\u0016j)T\u0006\u00001\n",
      "j(x(i)\u0000\u0016j)\u0001\n",
      "\u0001\u001e",
      "j\n",
      "w(i)\n",
      "j\n",
      "Let's maximize this with respect to \u0016l. If we take the derivative with respect\n",
      "to\u0016l, we \f",
      "nd\n",
      "r\u0016lnX\n",
      "i=1kX\n",
      "j=1w(i)\n",
      "jlog1\n",
      "(2\u0019)d=2j\u0006jj1=2exp\u0000\n",
      "\u00001\n",
      "2(x(i)\u0000\u0016j)T\u0006\u00001\n",
      "j(x(i)\u0000\u0016j)\u0001\n",
      "\u0001\u001e",
      "j\n",
      "w(i)\n",
      "j\n",
      "=\u0000r\u0016lnX\n",
      "i=1kX\n",
      "j=1w(i)\n",
      "j1\n",
      "2(x(i)\u0000\u0016j)T\u0006\u00001\n",
      "j(x(i)\u0000\u0016j)\n",
      "=1\n",
      "2nX\n",
      "i=1w(i)\n",
      "lr\u0016l2\u0016T\n",
      "l\u0006\u00001\n",
      "lx(i)\u0000\u0016T\n",
      "l\u0006\u00001\n",
      "l\u0016l\n",
      "=nX\n",
      "i=1w(i)\n",
      "l\u0000\n",
      "\u0006\u00001\n",
      "lx(i)\u0000\u0006\u00001\n",
      "l\u0016l\u0001\n",
      "Setting this to zero and solving for \u0016ltherefore yields the update rule\n",
      "\u0016l:=Pn\n",
      "i=1w(i)\n",
      "lx(i)\n",
      "Pn\n",
      "i=1w(i)\n",
      "l;\n",
      "which was what we had in the previous set of notes.\n",
      "Let's do one more example, and derive the M-step update for the param-\n",
      "eters\u001e",
      "j. Grouping together only the terms that depend on \u001e",
      "j, we \f",
      "nd that\n",
      "we need to maximize\n",
      "nX\n",
      "i=1kX\n",
      "j=1w(i)\n",
      "jlog\u001e",
      "j:\n",
      "However, there is an additional constraint that the \u001e",
      "j's sum to 1, since they\n",
      "represent the probabilities \u001e",
      "j=p(z(i)=j;\u001e",
      "). To deal with the constraint\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "160\n",
      "thatPk\n",
      "j=1\u001e",
      "j= 1, we construct the Lagrangian\n",
      "L(\u001e",
      ") =nX\n",
      "i=1kX\n",
      "j=1w(i)\n",
      "jlog\u001e",
      "j+\f",
      "(kX\n",
      "j=1\u001e",
      "j\u00001);\n",
      "where\f",
      "is the Lagrange multiplier.7Taking derivatives, we \f",
      "nd\n",
      "@\n",
      "@\u001e",
      "jL(\u001e",
      ") =nX\n",
      "i=1w(i)\n",
      "j\n",
      "\u001e",
      "j+\f",
      "\n",
      "Setting this to zero and solving, we get\n",
      "\u001e",
      "j=Pn\n",
      "i=1w(i)\n",
      "j\n",
      "\u0000\f",
      "\n",
      "I.e.,\u001e",
      "j/Pn\n",
      "i=1w(i)\n",
      "j. Using the constraint thatP\n",
      "j\u001e",
      "j= 1, we easily \f",
      "nd\n",
      "that\u0000\f",
      "=Pn\n",
      "i=1Pk\n",
      "j=1w(i)\n",
      "j=Pn\n",
      "i=11 =n. (This used the fact that w(i)\n",
      "j=\n",
      "Qi(z(i)=j), and since probabilities sum to 1,P\n",
      "jw(i)\n",
      "j= 1.) We therefore\n",
      "have our M-step updates for the parameters \u001e",
      "j:\n",
      "\u001e",
      "j:=1\n",
      "nnX\n",
      "i=1w(i)\n",
      "j:\n",
      "The derivation for the M-step updates to \u0006 jare also entirely straightfor-\n",
      "ward.\n",
      "11.5 Variational inference and variational\n",
      "auto-encoder (optional reading)\n",
      "Loosely speaking, variational auto-encoder Kingma and Welling [2013] gen-\n",
      "erally refers to a family of algorithms that extend the EM algorithms to more\n",
      "complex models parameterized by neural networks. It extends the technique\n",
      "of variational inference with the additional \\re-parametrization trick\" which\n",
      "will be introduced below. Variational auto-encoder may not give the best\n",
      "performance for many datasets, but it contains several central ideas about\n",
      "how to extend EM algorithms to high-dimensional continuous latent variables\n",
      "7We don't need to worry about the constraint that \u001e",
      "j\u00150, because as we'll shortly see,\n",
      "the solution we'll \f",
      "nd from this derivation will automatically satisfy that anyway.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "161\n",
      "with non-linear models. Understanding it will likely give you the language\n",
      "and backgrounds to understand various recent papers related to it.\n",
      "As a running example, we will consider the following parameterization of\n",
      "p(x;z;\u0012) by a neural network. Let \u0012be the collection of the weights of a\n",
      "neural network g(z;\u0012) that maps z2RktoRd. Let\n",
      "z\u0018N(0;Ik\u0002k) (11.18)\n",
      "xjz\u0018N(g(z;\u0012);\u001b2Id\u0002d) (11.19)\n",
      "HereIk\u0002kdenotes identity matrix of dimension kbyk, and\u001bis a scalar that\n",
      "we assume to be known for simplicity.\n",
      "For the Gaussian mixture models in Section 11.4, the optimal choice of\n",
      "Q(z) =p(zjx;\u0012) for each \f",
      "xed \u0012, that is the posterior distribution of z,\n",
      "can be analytically computed. In many more complex models such as the\n",
      "model (11.19), it's intractable to compute the exact the posterior distribution\n",
      "p(zjx;\u0012).\n",
      "Recall that from equation (11.10), ELBO is always a lower bound for any\n",
      "choice ofQ, and therefore, we can also aim for \f",
      "nding an approximation of\n",
      "the true posterior distribution. Often, one has to use some particular form\n",
      "to approximate the true posterior distribution. Let Qbe a family of Q's that\n",
      "we are considering, and we will aim to \f",
      "nd a Qwithin the family of Qthat is\n",
      "closest to the true posterior distribution. To formalize, recall the de\f",
      "nition of\n",
      "the ELBO lower bound as a function of Qand\u0012de\f",
      "ned in equation (11.14)\n",
      "ELBO(Q;\u0012) =nX\n",
      "i=1ELBO(x(i);Qi;\u0012) =X\n",
      "iX\n",
      "z(i)Qi(z(i)) logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\n",
      "Recall that EM can be viewed as alternating maximization of\n",
      "ELBO(Q;\u0012). Here instead, we optimize the EBLO over Q2Q\n",
      "max\n",
      "Q2Qmax\n",
      "\u0012ELBO(Q;\u0012) (11.20)\n",
      "Now the next question is what form of Q(or what structural assumptions\n",
      "to make about Q) allows us to e\u000eciently maximize the objective above. When\n",
      "the latent variable zare high-dimensional discrete variables, one popular as-\n",
      "sumption is the mean \f",
      "eld assumption , which assumes that Qi(z) gives a\n",
      "distribution with independent coordinates, or in other words, Qican be de-\n",
      "composed into Qi(z) =Q1\n",
      "i(z1)\u0001\u0001\u0001Qk\n",
      "i(zk). There are tremendous applications\n",
      "of mean \f",
      "eld assumptions to learning generative models with discrete latent\n",
      "variables, and we refer to Blei et al. [2017] for a survey of these models and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "162\n",
      "their impact to a wide range of applications including computational biology,\n",
      "computational neuroscience, social sciences. We will not get into the details\n",
      "about the discrete latent variable cases, and our main focus is to deal with\n",
      "continuous latent variables, which requires not only mean \f",
      "eld assumptions,\n",
      "but additional techniques.\n",
      "Whenz2Rkis a continuous latent variable, there are several decisions to\n",
      "make towards successfully optimizing (11.20). First we need to give a succinct\n",
      "representation of the distribution Qibecause it is over an in\f",
      "nite number of\n",
      "points. A natural choice is to assume Qiis a Gaussian distribution with some\n",
      "mean and variance. We would also like to have more succinct representation\n",
      "of the means of Qiof all the examples. Note that Qi(z(i)) is supposed to\n",
      "approximate p(z(i)jx(i);\u0012). It would make sense let all the means of the Qi's\n",
      "be some function of x(i). Concretely, let q(\u0001;\u001e",
      ");v(\u0001;\u001e",
      ") be two functions that\n",
      "map from dimension dtok, which are parameterized by \u001e",
      "and , we assume\n",
      "that\n",
      "Qi=N(q(x(i);\u001e",
      ");diag(v(x(i); ))2) (11.21)\n",
      "Here diag(w) means the k\u0002kmatrix with the entries of w2Rkon the\n",
      "diagonal. In other words, the distribution Qiis assumed to be a Gaussian\n",
      "distribution with independent coordinates, and the mean and standard de-\n",
      "viations are governed by qandv. Often in variational auto-encoder, qandv\n",
      "are chosen to be neural networks.8In recent deep learning literature, often\n",
      "q;vare called encoder (in the sense of encoding the data into latent code),\n",
      "whereasg(z;\u0012) if often referred to as the decoder.\n",
      "We remark that Qiof such form in many cases are very far from a good ap-\n",
      "proximation of the true posterior distribution. However, some approximation\n",
      "is necessary for feasible optimization. In fact, the form of Qineeds to satisfy\n",
      "other requirements (which happened to be satis\f",
      "ed by the form (11.21))\n",
      "Before optimizing the ELBO, let's \f",
      "rst verify whether we can e\u000eciently\n",
      "evaluate the value of the ELBO for \f",
      "xed Qof the form (11.21) and \u0012. We\n",
      "rewrite the ELBO as a function of \u001e",
      "; ;\u0012 by\n",
      "ELBO(\u001e",
      "; ;\u0012 ) =nX\n",
      "i=1Ez(i)\u0018Qi\u0014\n",
      "logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\u0015\n",
      "; (11.22)\n",
      "whereQi=N(q(x(i);\u001e",
      ");diag(v(x(i); ))2)\n",
      "Note that to evaluate Qi(z(i)) inside the expectation, we should be able to\n",
      "compute the density of Qi. To estimate the expectation Ez(i)\u0018Qi, we\n",
      "8qandvcan also share parameters. We sweep this level of details under the rug in this\n",
      "note.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "163\n",
      "should be able to sample from distribution Qiso that we can build an\n",
      "empirical estimator with samples. It happens that for Gaussian distribution\n",
      "Qi=N(q(x(i);\u001e",
      ");diag(v(x(i); ))2), we are able to be both e\u000eciently.\n",
      "Now let's optimize the ELBO. It turns out that we can run gradient ascent\n",
      "over\u001e",
      "; ;\u0012 instead of alternating maximization. There is no strong need to\n",
      "compute the maximum over each variable at a much greater cost. (For Gaus-\n",
      "sian mixture model in Section 11.4, computing the maximum is analytically\n",
      "feasible and relatively cheap, and therefore we did alternating maximization.)\n",
      "Mathematically, let \u0011be the learning rate, the gradient ascent step is\n",
      "\u0012:=\u0012+\u0011r\u0012ELBO(\u001e",
      "; ;\u0012 )\n",
      "\u001e",
      ":=\u001e",
      "+\u0011r\u001e",
      "ELBO(\u001e",
      "; ;\u0012 )\n",
      " := +\u0011r ELBO(\u001e",
      "; ;\u0012 )\n",
      "Computing the gradient over \u0012is simple because\n",
      "r\u0012ELBO(\u001e",
      "; ;\u0012 ) =r\u0012nX\n",
      "i=1Ez(i)\u0018Qi\u0014\n",
      "logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\u0015\n",
      "=r\u0012nX\n",
      "i=1Ez(i)\u0018Qi\u0002\n",
      "logp(x(i);z(i);\u0012)\u0003\n",
      "=nX\n",
      "i=1Ez(i)\u0018Qi\u0002\n",
      "r\u0012logp(x(i);z(i);\u0012)\u0003\n",
      "; (11.23)\n",
      "But computing the gradient over \u001e",
      "and is tricky because the sam-\n",
      "pling distribution Qidepends on \u001e",
      "and . (Abstractly speaking, the is-\n",
      "sue we face can be simpli\f",
      "ed as the problem of computing the gradi-\n",
      "ent Ez\u0018Q\u001e",
      "[f(\u001e",
      ")] with respect to variable \u001e",
      ". We know that in general,\n",
      "rEz\u0018Q\u001e",
      "[f(\u001e",
      ")]6= Ez\u0018Q\u001e",
      "[rf(\u001e",
      ")] because the dependency of Q\u001e",
      "on\u001e",
      "has to be\n",
      "taken into account as well. )\n",
      "The idea that comes to rescue is the so-called re-parameterization\n",
      "trick : we rewrite z(i)\u0018Qi=N(q(x(i);\u001e",
      ");diag(v(x(i); ))2) in an equivalent\n",
      "way:\n",
      "z(i)=q(x(i);\u001e",
      ") +v(x(i); )\f",
      "\u0018(i)where\u0018(i)\u0018N(0;Ik\u0002k) (11.24)\n",
      "Herex\f",
      "ydenotes the entry-wise product of two vectors of the same\n",
      "dimension. Here we used the fact that x\u0018N(\u0016;\u001b2) is equivalent to that\n",
      "x=\u0016+\u0018\u001bwith\u0018\u0018N(0;1). We mostly just used this fact in every dimension\n",
      "simultaneously for the random variable z(i)\u0018Qi.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "164\n",
      "With this re-parameterization, we have that\n",
      "Ez(i)\u0018Qi\u0014\n",
      "logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\u0015\n",
      "(11.25)\n",
      "= E\u0018(i)\u0018N(0;1)\u0014\n",
      "logp(x(i);q(x(i);\u001e",
      ") +v(x(i); )\f",
      "\u0018(i);\u0012)\n",
      "Qi(q(x(i);\u001e",
      ") +v(x(i); )\f",
      "\u0018(i))\u0015\n",
      "It follows that\n",
      "r\u001e",
      "Ez(i)\u0018Qi\u0014\n",
      "logp(x(i);z(i);\u0012)\n",
      "Qi(z(i))\u0015\n",
      "=r\u001e",
      "E\u0018(i)\u0018N(0;1)\u0014\n",
      "logp(x(i);q(x(i);\u001e",
      ") +v(x(i); )\f",
      "\u0018(i);\u0012)\n",
      "Qi(q(x(i);\u001e",
      ") +v(x(i); )\f",
      "\u0018(i))\u0015\n",
      "= E\u0018(i)\u0018N(0;1)\u0014\n",
      "r\u001e",
      "logp(x(i);q(x(i);\u001e",
      ") +v(x(i); )\f",
      "\u0018(i);\u0012)\n",
      "Qi(q(x(i);\u001e",
      ") +v(x(i); )\f",
      "\u0018(i))\u0015\n",
      "We can now sample multiple copies of \u0018(i)'s to estimate the the expecta-\n",
      "tion in the RHS of the equation above.9We can estimate the gradient with\n",
      "respect to similarly, and with these, we can implement the gradient ascent\n",
      "algorithm to optimize the ELBO over \u001e",
      "; ;\u0012:\n",
      "There are not many high-dimensional distributions with analytically com-\n",
      "putable density function are known to be re-parameterizable. We refer to\n",
      "Kingma and Welling [2013] for a few other choices that can replace Gaussian\n",
      "distribution.\n",
      "9Empirically people sometimes just use one sample to estimate it for maximum com-\n",
      "putational e\u000eciency.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 12\n",
      "Principal components analysis\n",
      "In this set of notes, we will develop a method, Principal Components Analysis\n",
      "(PCA), that tries to identify the subspace in which the data approximately\n",
      "lies. PCA is computationally e\u000ecient: it will require only an eigenvector\n",
      "calculation (easily done with the eigfunction in Matlab).\n",
      "Suppose we are given a dataset fx(i);i= 1;:::;ngof attributes of ndif-\n",
      "ferent types of automobiles, such as their maximum speed, turn radius, and\n",
      "so on. Let x(i)2Rdfor eachi(d\u001c",
      "n). But unknown to us, two di\u000b",
      "erent\n",
      "attributes|some xiandxj|respectively give a car's maximum speed mea-\n",
      "sured in miles per hour, and the maximum speed measured in kilometers per\n",
      "hour. These two attributes are therefore almost linearly dependent, up to\n",
      "only small di\u000b",
      "erences introduced by rounding o\u000b",
      " to the nearest mph or kph.\n",
      "Thus, the data really lies approximately on an n\u00001 dimensional subspace.\n",
      "How can we automatically detect, and perhaps remove, this redundancy?\n",
      "For a less contrived example, consider a dataset resulting from a survey of\n",
      "pilots for radio-controlled helicopters, where x(i)\n",
      "1is a measure of the piloting\n",
      "skill of pilot i, andx(i)\n",
      "ying. Because much he/she enjoys \n",
      "y, only the most committed students,\n",
      "ying, become good pilots. So, the two attributes\n",
      "x1andx2are strongly correlated. Indeed, we might posit that that the\n",
      "data actually likes along some diagonal axis (the u1direction) capturing the\n",
      "intrinsic piloting \\karma\" of a person, with only a small amount of noise\n",
      "lying o\u000b",
      " this axis. (See \f",
      "gure.) How can we automatically compute this u1\n",
      "direction?\n",
      "165\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "166\n",
      "x1x2(enjoyment)\n",
      "(skill)1\n",
      "uu\n",
      "2\n",
      "We will shortly develop the PCA algorithm. But prior to running PCA\n",
      "per se, typically we \f",
      "rst preprocess the data by normalizing each feature\n",
      "to have mean 0 and variance 1. We do this by subtracting the mean and\n",
      "dividing by the empirical standard deviation:\n",
      "x(i)\n",
      "j x(i)\n",
      "j\u0000\u0016j\n",
      "\u001bj\n",
      "where\u0016j=1\n",
      "nPn\n",
      "i=1x(i)\n",
      "jand\u001b2\n",
      "j=1\n",
      "nPn\n",
      "i=1(x(i)\n",
      "j\u0000\u0016j)2are the mean variance of\n",
      "featurej, respectively.\n",
      "Subtracting \u0016jzeros out the mean and may be omitted for data known\n",
      "to have zero mean (for instance, time series corresponding to speech or other\n",
      "acoustic signals). Dividing by the standard deviation \u001bjrescales each coor-\n",
      "dinate to have unit variance, which ensures that di\u000b",
      "erent attributes are all\n",
      "treated on the same \\scale.\" For instance, if x1was cars' maximum speed in\n",
      "mph (taking values in the high tens or low hundreds) and x2were the num-\n",
      "ber of seats (taking values around 2-4), then this renormalization rescales\n",
      "the di\u000b",
      "erent attributes to make them more comparable. This rescaling may\n",
      "be omitted if we had a priori knowledge that the di\u000b",
      "erent attributes are all\n",
      "on the same scale. One example of this is if each data point represented a\n",
      "grayscale image, and each x(i)\n",
      "jtook a value inf0;1;:::; 255gcorresponding\n",
      "to the intensity value of pixel jin imagei.\n",
      "Now, having normalized our data, how do we compute the \\major axis\n",
      "of variation\" u|that is, the direction on which the data approximately lies?\n",
      "One way is to pose this problem as \f",
      "nding the unit vector uso that when\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "167\n",
      "the data is projected onto the direction corresponding to u, the variance of\n",
      "the projected data is maximized. Intuitively, the data starts o\u000b",
      " with some\n",
      "amount of variance/information in it. We would like to choose a direction u\n",
      "so that if we were to approximate the data as lying in the direction/subspace\n",
      "corresponding to u, as much as possible of this variance is still retained.\n",
      "Consider the following dataset, on which we have already carried out the\n",
      "normalization steps:\n",
      "Now, suppose we pick uto correspond the the direction shown in the\n",
      "\f",
      "gure below. The circles denote the projections of the original data onto this\n",
      "line.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "168\n",
      "/0/0/1/1/0/0/0/0/1/1/1/1\n",
      "/0 /1\n",
      "/0/0/0/0/1/1/1/1\n",
      "/0 /1 /0/0/0/0/1/1/1/1 /0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1\n",
      "/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n",
      "/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1\n",
      "We see that the projected data still has a fairly large variance, and the\n",
      "points tend to be far from zero. In contrast, suppose had instead picked the\n",
      "following direction:\n",
      "/0/0 /1/1\n",
      "/0/0/0/0/1/1/1/1 /0 /1\n",
      "/0/0/0/0/1/1/1/1/0/0/1/1\n",
      "/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n",
      "/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n",
      "/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n",
      "/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0/0\n",
      "/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1/1\n",
      "Here, the projections have a signi\f",
      "cantly smaller variance, and are much\n",
      "closer to the origin.\n",
      "We would like to automatically select the direction ucorresponding to\n",
      "the \f",
      "rst of the two \f",
      "gures shown above. To formalize this, note that given a\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "169\n",
      "unit vector uand a point x, the length of the projection of xontouis given\n",
      "byxTu. I.e., ifx(i)is a point in our dataset (one of the crosses in the plot),\n",
      "then its projection onto u(the corresponding circle in the \f",
      "gure) is distance\n",
      "xTufrom the origin. Hence, to maximize the variance of the projections, we\n",
      "would like to choose a unit-length uso as to maximize:\n",
      "1\n",
      "nnX\n",
      "i=1(x(i)Tu)2=1\n",
      "nnX\n",
      "i=1uTx(i)x(i)Tu\n",
      "=uT \n",
      "1\n",
      "nnX\n",
      "i=1x(i)x(i)T!\n",
      "u:\n",
      "We easily recognize that the maximizing this subject to kuk2= 1 gives the\n",
      "principal eigenvector of \u0006 =1\n",
      "nPn\n",
      "i=1x(i)x(i)T, which is just the empirical\n",
      "covariance matrix of the data (assuming it has zero mean).1\n",
      "To summarize, we have found that if we wish to \f",
      "nd a 1-dimensional\n",
      "subspace with with to approximate the data, we should choose uto be the\n",
      "principal eigenvector of \u0006. More generally, if we wish to project our data\n",
      "into ak-dimensional subspace ( k<d ), we should choose u1;:::;ukto be the\n",
      "topkeigenvectors of \u0006. The ui's now form a new, orthogonal basis for the\n",
      "data.2\n",
      "Then, to represent x(i)in this basis, we need only compute the corre-\n",
      "sponding vector\n",
      "y(i)=2\n",
      "6664uT\n",
      "1x(i)\n",
      "uT\n",
      "2x(i)\n",
      "...\n",
      "uT\n",
      "kx(i)3\n",
      "77752Rk:\n",
      "Thus, whereas x(i)2Rd, the vector y(i)now gives a lower, k-dimensional,\n",
      "approximation/representation for x(i). PCA is therefore also referred to as\n",
      "adimensionality reduction algorithm. The vectors u1;:::;ukare called\n",
      "the \f",
      "rstkprincipal components of the data.\n",
      "Remark. Although we have shown it formally only for the case of k= 1,\n",
      "using well-known properties of eigenvectors it is straightforward to show that\n",
      "1If you haven't seen this before, try using the method of Lagrange multipliers to max-\n",
      "imizeuT\u0006usubject to that uTu= 1. You should be able to show that \u0006 u=\u0015u, for some\n",
      "\u0015, which implies uis an eigenvector of \u0006, with eigenvalue \u0015.\n",
      "2Because \u0006 is symmetric, the ui's will (or always can be chosen to be) orthogonal to\n",
      "each other.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "170\n",
      "of all possible orthogonal bases u1;:::;uk, the one that we have chosen max-\n",
      "imizesP\n",
      "iky(i)k2\n",
      "2. Thus, our choice of a basis preserves as much variability\n",
      "as possible in the original data.\n",
      "PCA can also be derived by picking the basis that minimizes the ap-\n",
      "proximation error arising from projecting the data onto the k-dimensional\n",
      "subspace spanned by them. (See more in homework.)\n",
      "PCA has many applications; we will close our discussion with a few exam-\n",
      "ples. First, compression|representing x(i)'s with lower dimension y(i)'s|is\n",
      "an obvious application. If we reduce high dimensional data to k= 2 or 3 di-\n",
      "mensions, then we can also plot the y(i)'s to visualize the data. For instance,\n",
      "if we were to reduce our automobiles data to 2 dimensions, then we can plot\n",
      "it (one point in our plot would correspond to one car type, say) to see what\n",
      "cars are similar to each other and what groups of cars may cluster together.\n",
      "Another standard application is to preprocess a dataset to reduce its\n",
      "dimension before running a supervised learning learning algorithm with the\n",
      "x(i)'s as inputs. Apart from computational bene\f",
      "ts, reducing the data's\n",
      "dimension can also reduce the complexity of the hypothesis class considered\n",
      "and help avoid over\f",
      "tting (e.g., linear classi\f",
      "ers over lower dimensional input\n",
      "spaces will have smaller VC dimension).\n",
      "Lastly, as in our RC pilot example, we can also view PCA as a noise\n",
      "reduction algorithm. In our example it, estimates the intrinsic \\piloting\n",
      "karma\" from the noisy measures of piloting skill and enjoyment. In class, we\n",
      "also saw the application of this idea to face images, resulting in eigenfaces\n",
      "method. Here, each point x(i)2R100\u0002100was a 10000 dimensional vector,\n",
      "with each coordinate corresponding to a pixel intensity value in a 100x100\n",
      "image of a face. Using PCA, we represent each image x(i)with a much lower-\n",
      "dimensional y(i). In doing so, we hope that the principal components we\n",
      "found retain the interesting, systematic variations between faces that capture\n",
      "what a person really looks like, but not the \\noise\" in the images introduced\n",
      "by minor lighting variations, slightly di\u000b",
      "erent imaging conditions, and so on.\n",
      "We then measure distances between faces iandjby working in the reduced\n",
      "dimension, and computing ky(i)\u0000y(j)k2. This resulted in a surprisingly good\n",
      "face-matching and retrieval algorithm.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 13\n",
      "Independent components\n",
      "analysis\n",
      "Our next topic is Independent Components Analysis (ICA). Similar to PCA,\n",
      "this will \f",
      "nd a new basis in which to represent our data. However, the goal\n",
      "is very di\u000b",
      "erent.\n",
      "As a motivating example, consider the \\cocktail party problem.\" Here, d\n",
      "speakers are speaking simultaneously at a party, and any microphone placed\n",
      "in the room records only an overlapping combination of the dspeakers' voices.\n",
      "But lets say we have ddi\u000b",
      "erent microphones placed in the room, and because\n",
      "each microphone is a di\u000b",
      "erent distance from each of the speakers, it records a\n",
      "di\u000b",
      "erent combination of the speakers' voices. Using these microphone record-\n",
      "ings, can we separate out the original dspeakers' speech signals?\n",
      "To formalize this problem, we imagine that there is some data s2Rd\n",
      "that is generated via dindependent sources. What we observe is\n",
      "x=As;\n",
      "whereAis an unknown square matrix called the mixing matrix . Repeated\n",
      "observations gives us a dataset fx(i);i= 1;:::;ng, and our goal is to recover\n",
      "the sources s(i)that had generated our data ( x(i)=As(i)).\n",
      "In our cocktail party problem, s(i)is and-dimensional vector, and s(i)\n",
      "jis\n",
      "the sound that speaker jwas uttering at time i. Also,x(i)in and-dimensional\n",
      "vector, and x(i)\n",
      "jis the acoustic reading recorded by microphone jat timei.\n",
      "LetW=A\u00001be the unmixing matrix. Our goal is to \f",
      "nd W, so\n",
      "that given our microphone recordings x(i), we can recover the sources by\n",
      "computing s(i)=Wx(i). For notational convenience, we also let wT\n",
      "idenote\n",
      "171\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "172\n",
      "thei-th row ofW, so that\n",
      "W=2\n",
      "64|wT\n",
      "1|\n",
      "...\n",
      "|wT\n",
      "d|3\n",
      "75:\n",
      "Thus,wi2Rd, and thej-th source can be recovered as s(i)\n",
      "j=wT\n",
      "jx(i).\n",
      "13.1 ICA ambiguities\n",
      "To what degree can W=A\u00001be recovered? If we have no prior knowledge\n",
      "about the sources and the mixing matrix, it is easy to see that there are some\n",
      "inherent ambiguities in Athat are impossible to recover, given only the x(i)'s.\n",
      "Speci\f",
      "cally, let Pbe anyd-by-dpermutation matrix. This means that\n",
      "each row and each column of Phas exactly one \\1.\" Here are some examples\n",
      "of permutation matrices:\n",
      "P=2\n",
      "40 1 0\n",
      "1 0 0\n",
      "0 0 13\n",
      "5;P=\u00140 1\n",
      "1 0\u0015\n",
      ";P=\u00141 0\n",
      "0 1\u0015\n",
      ":\n",
      "Ifzis a vector, then Pzis another vector that contains a permuted version\n",
      "ofz's coordinates. Given only the x(i)'s, there will be no way to distinguish\n",
      "betweenWandPW. Speci\f",
      "cally, the permutation of the original sources is\n",
      "ambiguous, which should be no surprise. Fortunately, this does not matter\n",
      "for most applications.\n",
      "Further, there is no way to recover the correct scaling of the wi's. For in-\n",
      "stance, ifAwere replaced with 2 A, and every s(i)were replaced with (0 :5)s(i),\n",
      "then our observed x(i)= 2A\u0001(0:5)s(i)would still be the same. More broadly,\n",
      "if a single column of Awere scaled by a factor of \u000b",
      ", and the corresponding\n",
      "source were scaled by a factor of 1 =\u000b",
      ", then there is again no way to determine\n",
      "that this had happened given only the x(i)'s. Thus, we cannot recover the\n",
      "\\correct\" scaling of the sources. However, for the applications that we are\n",
      "concerned with|including the cocktail party problem|this ambiguity also\n",
      "does not matter. Speci\f",
      "cally, scaling a speaker's speech signal s(i)\n",
      "jby some\n",
      "positive factor \u000b",
      "a\u000b",
      "ects only the volume of that speaker's speech. Also, sign\n",
      "changes do not matter, and s(i)\n",
      "jand\u0000s(i)\n",
      "jsound identical when played on a\n",
      "speaker. Thus, if the wifound by an algorithm is scaled by any non-zero real\n",
      "number, the corresponding recovered source si=wT\n",
      "ixwill be scaled by the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "173\n",
      "same factor; but this usually does not matter. (These comments also apply\n",
      "to ICA for the brain/MEG data that we talked about in class.)\n",
      "Are these the only sources of ambiguity in ICA? It turns out that they\n",
      "are, so long as the sources siarenon-Gaussian . To see what the di\u000eculty is\n",
      "with Gaussian data, consider an example in which n= 2, ands\u0018N (0;I).\n",
      "Here,Iis the 2x2 identity matrix. Note that the contours of the density of\n",
      "the standard normal distribution N(0;I) are circles centered on the origin,\n",
      "and the density is rotationally symmetric.\n",
      "Now, suppose we observe some x=As, whereAis our mixing matrix.\n",
      "Then, the distribution of xwill be Gaussian, x\u0018N(0;AAT), since\n",
      "Es\u0018N(0;I)[x] = E[As] =AE[s] = 0\n",
      "Cov[x] = Es\u0018N(0;I)[xxT] = E[AssTAT] =AE[ssT]AT=A\u0001Cov[s]\u0001AT=AAT\n",
      "ection)tRbe an arbitrary orthogonal (less formally, a rotation/re\n",
      "matrix, so that RRT=RTR=I, and letA0=AR. Then if the data had\n",
      "been mixed according to A0instead ofA, we would have instead observed\n",
      "x0=A0s. The distribution of x0is also Gaussian, x0\u0018N (0;AAT), since\n",
      "Es\u0018N(0;I)[x0(x0)T] = E[A0ssT(A0)T] = E[ARssT(AR)T] =ARRTAT=AAT.\n",
      "Hence, whether the mixing matrix is AorA0, we would observe data from\n",
      "aN(0;AAT) distribution. Thus, there is no way to tell if the sources were\n",
      "mixed using AandA0. There is an arbitrary rotational component in the\n",
      "mixing matrix that cannot be determined from the data, and we cannot\n",
      "recover the original sources.\n",
      "Our argument above was based on the fact that the multivariate standard\n",
      "normal distribution is rotationally symmetric. Despite the bleak picture that\n",
      "this paints for ICA on Gaussian data, it turns out that, so long as the data is\n",
      "notGaussian, it is possible, given enough data, to recover the dindependent\n",
      "sources.\n",
      "13.2 Densities and linear transformations\n",
      "yefore moving on to derive the ICA algorithm proper, we \f",
      "rst digress brie\n",
      "to talk about the e\u000b",
      "ect of linear transformations on densities.\n",
      "Suppose a random variable sis drawn according to some density ps(s).\n",
      "For simplicity, assume for now that s2Ris a real number. Now, let the\n",
      "random variable xbe de\f",
      "ned according to x=As(here,x2R;A2R). Let\n",
      "pxbe the density of x. What ispx?\n",
      "LetW=A\u00001. To calculate the \\probability\" of a particular value of x,\n",
      "it is tempting to compute s=Wx, then then evaluate psat that point, and\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "174\n",
      "conclude that \\ px(x) =ps(Wx).\" However, this is incorrect . For example,\n",
      "lets\u0018Uniform[0;1], sops(s) = 1f0\u0014s\u00141g. Now, let A= 2, sox= 2s.\n",
      "Clearly,xis distributed uniformly in the interval [0 ;2]. Thus, its density is\n",
      "given bypx(x) = (0:5)1f0\u0014x\u00142g. This does not equal ps(Wx), where\n",
      "W= 0:5 =A\u00001. Instead, the correct formula is px(x) =ps(Wx)jWj.\n",
      "More generally, if sis a vector-valued distribution with density ps, and\n",
      "x=Asfor a square, invertible matrix A, then the density of xis given by\n",
      "px(x) =ps(Wx)\u0001jWj;\n",
      "whereW=A\u00001.\n",
      "Remark. If you're seen the result that Amaps [0;1]dto a set of volume jAj,\n",
      "then here's another way to remember the formula for pxgiven above, that also\n",
      "generalizes our previous 1-dimensional example. Speci\f",
      "cally, let A2Rd\u0002dbe\n",
      "given, and let W=A\u00001as usual. Also let C1= [0;1]dbe thed-dimensional\n",
      "hypercube, and de\f",
      "ne C2=fAs:s2C1g\u0012Rdto be the image of C1\n",
      "under the mapping given by A. Then it is a standard result in linear algebra\n",
      "(and, indeed, one of the ways of de\f",
      "ning determinants) that the volume of\n",
      "C2is given byjAj. Now, suppose sis uniformly distributed in [0 ;1]d, so its\n",
      "density isps(s) = 1fs2C1g. Then clearly xwill be uniformly distributed\n",
      "inC2. Its density is therefore found to be px(x) = 1fx2C2g=vol(C2) (since\n",
      "it must integrate over C2to 1). But using the fact that the determinant\n",
      "of the inverse of a matrix is just the inverse of the determinant, we have\n",
      "1=vol(C2) = 1=jAj=jA\u00001j=jWj. Thus,px(x) = 1fx2C2gjWj= 1fWx2\n",
      "C1gjWj=ps(Wx)jWj.\n",
      "13.3 ICA algorithm\n",
      "We are now ready to derive an ICA algorithm. We describe an algorithm\n",
      "by Bell and Sejnowski, and we give an interpretation of their algorithm as a\n",
      "method for maximum likelihood estimation. (This is di\u000b",
      "erent from their orig-\n",
      "inal interpretation involving a complicated idea called the infomax principal\n",
      "which is no longer necessary given the modern understanding of ICA.)\n",
      "We suppose that the distribution of each source sjis given by a density\n",
      "ps, and that the joint distribution of the sources sis given by\n",
      "p(s) =dY\n",
      "j=1ps(sj):\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "175\n",
      "Note that by modeling the joint distribution as a product of marginals, we\n",
      "capture the assumption that the sources are independent. Using our formulas\n",
      "from the previous section, this implies the following density on x=As=\n",
      "W\u00001s:\n",
      "p(x) =dY\n",
      "j=1ps(wT\n",
      "jx)\u0001jWj:\n",
      "All that remains is to specify a density for the individual sources ps.\n",
      "Recall that, given a real-valued random variable z, its cumulative distri-\n",
      "bution function (cdf) Fis de\f",
      "ned by F(z0) =P(z\u0014z0) =Rz0\n",
      "\u00001pz(z)dzand\n",
      "the density is the derivative of the cdf: pz(z) =F0(z).\n",
      "Thus, to specify a density for the si's, all we need to do is to specify some\n",
      "cdf for it. A cdf has to be a monotonic function that increases from zero\n",
      "to one. Following our previous discussion, we cannot choose the Gaussian\n",
      "cdf, as ICA doesn't work on Gaussian data. What we'll choose instead as\n",
      "a reasonable \\default\" cdf that slowly increases from 0 to 1, is the sigmoid\n",
      "functiong(s) = 1=(1 +e\u0000s). Hence,ps(s) =g0(s).1\n",
      "The square matrix Wis the parameter in our model. Given a training\n",
      "setfx(i);i= 1;:::;ng, the log likelihood is given by\n",
      "`(W) =nX\n",
      "i=1 dX\n",
      "j=1logg0(wT\n",
      "jx(i)) + logjWj!\n",
      ":\n",
      "We would like to maximize this in terms W. By taking derivatives and using\n",
      "the fact (from the \f",
      "rst set of notes) that rWjWj=jWj(W\u00001)T, we easily\n",
      "derive a stochastic gradient ascent learning rule. For a training example x(i),\n",
      "the update rule is:\n",
      "W:=W+\u000b",
      "0\n",
      "BBB@2\n",
      "66641\u00002g(wT\n",
      "1x(i))\n",
      "1\u00002g(wT\n",
      "2x(i))\n",
      "...\n",
      "1\u00002g(wT\n",
      "dx(i))3\n",
      "7775x(i)T+ (WT)\u000011\n",
      "CCCA;\n",
      "1If you have prior knowledge that the sources' densities take a certain form, then it\n",
      "is a good idea to substitute that in here. But in the absence of such knowledge, the\n",
      "sigmoid function can be thought of as a reasonable default that seems to work well for\n",
      "many problems. Also, the presentation here assumes that either the data x(i)has been\n",
      "preprocessed to have zero mean, or that it can naturally be expected to have zero mean\n",
      "(such as acoustic signals). This is necessary because our assumption that ps(s) =g0(s)\n",
      "implies E[s] = 0 (the derivative of the logistic function is a symmetric function, and\n",
      "hence gives a density corresponding to a random variable with zero mean), which implies\n",
      "E[x] = E[As] = 0.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "176\n",
      "where\u000b",
      "is the learning rate.\n",
      "After the algorithm converges, we then compute s(i)=Wx(i)to recover\n",
      "the original sources.\n",
      "Remark. When writing down the likelihood of the data, we implicitly as-\n",
      "sumed that the x(i)'s were independent of each other (for di\u000b",
      "erent values\n",
      "ofi; note this issue is di\u000b",
      "erent from whether the di\u000b",
      "erent coordinates of\n",
      "x(i)are independent), so that the likelihood of the training set was given\n",
      "byQ\n",
      "ip(x(i);W). This assumption is clearly incorrect for speech data and\n",
      "other time series where the x(i)'s are dependent, but it can be shown that\n",
      "having correlated training examples will not hurt the performance of the al-\n",
      "gorithm if we have su\u000ecient data. However, for problems where successive\n",
      "training examples are correlated, when implementing stochastic gradient as-\n",
      "cent, it sometimes helps accelerate convergence if we visit training examples\n",
      "in a randomly permuted order. (I.e., run stochastic gradient ascent on a\n",
      "randomly shu\u000fed copy of the training set.)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 14\n",
      "Self-supervised learning and\n",
      "foundation models\n",
      "Despite its huge success, supervised learning with neural networks typically\n",
      "relies on the availability of a labeled dataset of decent size, which is some-\n",
      "times costly to collect. Recently, AI and machine learning are undergoing a\n",
      "paradigm shift with the rise of models (e.g., BERT [Devlin et al., 2019] and\n",
      "GPT-3 [Brown et al., 2020]) that are pre-trained on broad data at scale and\n",
      "are adaptable to a wide range of downstream tasks. These models, called\n",
      "foundation models by Bommasani et al. [2021], oftentimes leverage massive\n",
      "unlabeled data so that much fewer labeled data in the downstream tasks are\n",
      "needed. Moreover, though foundation models are based on standard deep\n",
      "learning and transfer learning, their scale results in new emergent capabil-\n",
      "ities. These models are typically (pre-)trained by self-supervised learning\n",
      "methods where the supervisions/labels come from parts of the inputs.\n",
      "This chapter will introduce the paradigm of foundation models and basic\n",
      "related concepts.\n",
      "14.1 Pretraining and adaptation\n",
      "The foundation models paradigm consists of two phases: pretraining (or sim-\n",
      "ply training) and adaptation. We \f",
      "rst pretrain a large model on a massive\n",
      "unlabeled dataset (e.g., billions of unlabeled images).1Then, we adapt the\n",
      "pretrained model to a downstream task (e.g., detecting cancer from scan im-\n",
      "ages). These downstream tasks are often prediction tasks with limited or\n",
      "1Sometimes, pretraining can involve large-scale labeled datasets as well (e.g., the Ima-\n",
      "geNet dataset).\n",
      "177\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "178\n",
      "even no labeled data. The intuition is that the pretrained models learn good\n",
      "representations that capture intrinsic semantic structure/ information about\n",
      "the data, and the adaptation phase customizes the model to a particular\n",
      "downstream task by, e.g., retrieving the information speci\f",
      "c to it. For ex-\n",
      "ample, a model pretrained on massive unlabeled image data may learn good\n",
      "general visual representations/features, and we adapt the representations to\n",
      "solve biomedical imagining tasks.\n",
      "We formalize the two phases below.\n",
      "Pretraining. Suppose we have an unlabeled pretraining dataset\n",
      "fx(1);x(2)\u0001\u0001\u0001;x(n)gthat consists of nexamples in Rd. Let\u001e",
      "\u0012be a model that\n",
      "is parameterized by \u0012and maps the input xto somem-dimensional represen-\n",
      "tation\u001e",
      "\u0012(x). (People also call \u001e",
      "\u0012(x)2Rmthe embedding or features of the\n",
      "examplex.) We pretrain the model \u0012with a pretraining loss, which is often\n",
      "an average of loss functions on all the examples: Lpre(\u0012) =1\n",
      "nPn\n",
      "i=1`pre(\u0012;x(i)).\n",
      "Here`preis a so-called self-supervised loss on a single datapoint x(i), because\n",
      "as shown later, e.g., in Section 14.3, the \\supervision\" comes from the data\n",
      "pointx(i)itself. It is also possible that the pretraining loss is not a sum\n",
      "of losses on individual examples. We will discuss two pretraining losses in\n",
      "Section 14.2 and Section 14.3.\n",
      "We use some optimizers (mostly likely SGD or ADAM [Kingma and Ba,\n",
      "2014]) to minimize Lpre(\u0012). We denote the obtained pretrained model by ^\u0012.\n",
      "Adaptation. For a downstream task, we usually have a labeled dataset\n",
      "f(x(1)\n",
      "task;y(1)\n",
      "task);\u0001\u0001\u0001;(x(ntask)\n",
      "task;y(ntask)\n",
      "task )gwithntaskexamples. The setting when\n",
      "ntask= 0 is called zero-shot learning|the downstream task doesn't have any\n",
      "labeled examples. When ntaskis relatively small (say, between 1 and 50), the\n",
      "setting is called few-shot learning. It's also pretty common to have a larger\n",
      "ntaskon the order of ranging from hundreds to tens of thousands.\n",
      "An adaptation algorithm generally takes in a downstream dataset and the\n",
      "pretrained model ^\u0012, and outputs a variant of ^\u0012that solves the downstream\n",
      "task. We will discuss below two popular and general adaptation methods,\n",
      "linear probe and \f",
      "netuning. In addition, two other methods speci\f",
      "c to lan-\n",
      "guage problems are introduced in 14.3.1.\n",
      "The linear probe approach uses a linear head on top of the representation\n",
      "to predict the downstream labels. Mathematically, the adapted model out-\n",
      "putsw>\u001e",
      "^\u0012(x), wherew2Rmis a parameter to be learned, and ^\u0012is exactly\n",
      "the pretrained model (\f",
      "xed). We can use SGD (or other optimizers) to train\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "179\n",
      "won the downstream task loss to predict the task label\n",
      "min\n",
      "w2Rm1\n",
      "ntaskntaskX\n",
      "i=1`task(y(i)\n",
      "task;w>\u001e",
      "^\u0012(x(i)\n",
      "task)) (14.1)\n",
      "E.g., if the downstream task is a regression problem, we will have\n",
      "`task(ytask;w>\u001e",
      "^\u0012(xtask)) = (ytask\u0000w>\u001e",
      "^\u0012(xtask))2.\n",
      "The \f",
      "netuning algorithm uses a similar structure for the downstream\n",
      "prediction model, but also further \f",
      "netunes the pretrained model (instead\n",
      "of keeping it \f",
      "xed). Concretely, the prediction model is w>\u001e",
      "\u0012(x) with pa-\n",
      "rameterswand\u0012:We optimize both wand\u0012to \f",
      "t the downstream data,\n",
      "but initialize \u0012with the pretrained model ^\u0012. The linear head wis usually\n",
      "initialized randomly.\n",
      "minimize\n",
      "w;\u00121\n",
      "ntaskntaskX\n",
      "i=1`task(y(i)\n",
      "task;w>\u001e",
      "\u0012(x(i)\n",
      "task)) (14.2)\n",
      "with initialization w random vector (14.3)\n",
      "\u0012 ^\u0012 (14.4)\n",
      "Various other adaptation methods exists and are sometimes specialized\n",
      "to the particular pretraining methods. We will discuss one of them in Sec-\n",
      "tion 14.3.1.\n",
      "14.2 Pretraining methods in computer vision\n",
      "This section introduces two concrete pretraining methods for computer vi-\n",
      "sion: supervised pretraining and contrastive learning.\n",
      "Supervised pretraining. Here, the pretraining dataset is a large-scale\n",
      "labeled dataset (e.g., ImageNet), and the pretrained models are simply a\n",
      "neural network trained with vanilla supervised learning (with the last layer\n",
      "being removed). Concretely, suppose we write the learned neural network as\n",
      "U\u001e",
      "^\u0012(x), whereUis the last (fully-connected) layer parameters, ^\u0012corresponds\n",
      "to the parameters of all the other layers, and \u001e",
      "^\u0012(x) are the penultimate\n",
      "activations layer (which serves as the representation). We simply discard U\n",
      "and use\u001e",
      "^\u0012(x) as the pretrained model.\n",
      "Contrastive learning. Contrastive learning is a self-supervised pretraining\n",
      "method that uses only unlabeled data. The main intuition is that a good\n",
      "representation function \u001e",
      "\u0012(\u0001) should map semantically similar images to sim-\n",
      "ilar representations, and that random pair of images should generally have\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "180\n",
      "distinct representations. E.g., we may want to map images of two huskies to\n",
      "similar representations, but a husky and an elephant should have di\u000b",
      "erent\n",
      "representations. One de\f",
      "nition of similarity is that images from the same\n",
      "class are similar. Using this de\f",
      "nition will result in the so-called supervised\n",
      "contrastive algorithms that work well when labeled pretraining datasets are\n",
      "available.\n",
      "Without labeled data, we can use data augmentation to generate a pair\n",
      "of \\similar\" augmented images given an original image x. Data augmenta-\n",
      "ipping, and/or color that we apply random cropping, \n",
      "transformation on the original image xto generate a variant. We can take\n",
      "two random augmentations, denoted by ^ xand ~x, of the same original image\n",
      "x, and call them a positive pair. We observe that positive pairs of images\n",
      "are often semantically related because they are augmentations of the same\n",
      "image. We will design a loss function for \u0012such that the representations of\n",
      "a positive pair, \u001e",
      "\u0012(^x);\u001e",
      "\u0012(~x), as close to each other as possible.\n",
      "On the other hand, we can also take another random image zfrom the\n",
      "pretraining dataset and generate an augmentation ^ zfromz. Note that (^ x;^z)\n",
      "are from di\u000b",
      "erent images; therefore, with a good chance, they are not seman-\n",
      "tically related. We call (^ x;^z) a negative or random pair.2We will design a\n",
      "loss to push the representation of random pairs, \u001e",
      "\u0012(^x);\u001e",
      "\u0012(^z), far away from\n",
      "each other.\n",
      "There are many recent algorithms based on the contrastive learning prin-\n",
      "ciple, and here we introduce SIMCLR [Chen et al., 2020] as an concrete\n",
      "example. The loss function is de\f",
      "ned on a batch of examples ( x1;\u0001\u0001\u0001;x(B))\n",
      "with batch size B. The algorithm computes two random augmentations for\n",
      "each example x(i)in the batch, denoted by ^ x(i)and ~x(i):As a result, we\n",
      "have the augmented batch of 2 Bexamples: ^x1;\u0001\u0001\u0001;^x(B), ~x1;\u0001\u0001\u0001;~x(B). The\n",
      "SIMCLR loss is de\f",
      "ned as3\n",
      "Lpre(\u0012) =\u0000BX\n",
      "i=1logexp\u0000\n",
      "\u001e",
      "\u0012(^x(i))>\u001e",
      "\u0012(~x(i))\u0001\n",
      "exp (\u001e",
      "\u0012(^x(i))>\u001e",
      "\u0012(~x(i))) +P\n",
      "j6=iexp (\u001e",
      "\u0012(^x(i))>\u001e",
      "\u0012(~x(j))):\n",
      "The intuition is as follows. The loss is increasing in \u001e",
      "\u0012(^x(i))>\u001e",
      "\u0012(~x(j)), and\n",
      "thus minimizing the loss encourages \u001e",
      "\u0012(^x(i))>\u001e",
      "\u0012(~x(j)) to be small, making\n",
      "\u001e",
      "\u0012(^x(i)) far away from \u001e",
      "\u0012(~x(j)). On the other hand, the loss is decreasing in\n",
      "2Random pair may be a more accurate term because it's still possible (though not\n",
      "likely) that xandzare semantically related, so are ^ xand ^z. But in the literature, the\n",
      "term negative pair seems to be also common.\n",
      "3This is a variant and simpli\f",
      "cation of the original loss that does not change the essence\n",
      "(but may change the e\u000eciency slightly).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "181\n",
      "\u001e",
      "\u0012(^x(i))>\u001e",
      "\u0012(~x(i)), and thus minimizing the loss encourages \u001e",
      "\u0012(^x(i))>\u001e",
      "\u0012(~x(i))\n",
      "to be large, resulting in \u001e",
      "\u0012(^x(i)) and\u001e",
      "\u0012(~x(i)) to be close.4\n",
      "14.3 Pretrained large language models\n",
      "Natural language processing is another area where pretraining models are\n",
      "particularly successful. In language problems, an examples typically cor-\n",
      "responds to a document or generally a sequence/trunk of words,5denoted\n",
      "byx= (x1;\u0001\u0001\u0001;xT) whereTis the length of the document/sequence,\n",
      "xi2f1;\u0001\u0001\u0001;Vgare words in the document, and Vis the vocabulary size.6\n",
      "A language model is a probabilistic model representing the probability of\n",
      "a document, denoted by p(x1;\u0001\u0001\u0001;xT):This probability distribution is very\n",
      "complex because its support size is VT| exponential in the length of the\n",
      "document. Instead of modeling the distribution of a document itself, we can\n",
      "apply the chain rule of conditional probability to decompose it as follows:\n",
      "p(x1;\u0001\u0001\u0001;xT) =p(x1)p(x2jx1)\u0001\u0001\u0001p(xTjx1;\u0001\u0001\u0001;xT\u00001): (14.5)\n",
      "Now the support of each of the conditional probability p(xtjx1;\u0001\u0001\u0001;xt\u00001) is\n",
      "V.\n",
      "We will model the conditional probability p(xtjx1;\u0001\u0001\u0001;xt\u00001) with some\n",
      "parameterized form. To this end, we \f",
      "rst turn the discrete words into word\n",
      "embeddings.\n",
      "Letei2Rdbe the embedding of the word i2f1;2;\u0001\u0001\u0001;Vg:We call\n",
      "[e1;\u0001\u0001\u0001;eV]2Rd\u0002Vthe embedding matrix. The most commonly used model\n",
      "is transformer [Vaswani et al., 2017]. We will introduce the basic concepts\n",
      "regarding the inputs and outputs of a transformer, but treat the interme-\n",
      "diate computation in transformer as a blackbox. We refer the students to\n",
      "more advanced courses or the original paper for more details. The high-level\n",
      "pipeline is visualized in Figure 14.1. Given a document ( x1;\u0001\u0001\u0001;xT), we \f",
      "rst\n",
      "compute the corresponding word embeddings ( ex1;\u0001\u0001\u0001;exT). Then, the word\n",
      "embeddings is passed to a transformer model, which takes in a sequence of\n",
      "4To see this, you can verify that the function \u0000logp\n",
      "p+qis decreasing in p, and increasing\n",
      "inqwhenp;q> 0:\n",
      "5In the practical implementations, typically all the data are concatenated into a single\n",
      "sequence in some order, and each example typically corresponds a sub-sequence of consec-\n",
      "utive words which may corresponds to a subset of a document or may span across multiple\n",
      "documents.\n",
      "6Technically, words may be decomposed into tokens which could be words or sub-words\n",
      "(combinations of letters), but this note omits this technicality. In fact most commons words\n",
      "are a single token themselves.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "182\n",
      "vectors and outputs a sequence of vectors ( c2;\u0001\u0001\u0001;cT+1):In particular, here\n",
      "we use the autoregressive version of the transformers which makes sure that\n",
      "ctonly depends on x1;\u0001\u0001\u0001;xt\u00001.7Thect's are often called the representations\n",
      "or the contextualized embeddings, and we write ct=\u001e",
      "\u0012(x1;:::;xt\u00001) where\n",
      "\u001e",
      "\u0012denotes the mapping from the input to the representations.\n",
      "Figure 14.1: The input-output interface of a transformer.\n",
      "To learn the parameters \u0012in the transformer, we use ctto predict\n",
      "the conditional probability p(xtjx1;\u0001\u0001\u0001;xt\u00001).8We parameterize p(xtj\n",
      "x1;\u0001\u0001\u0001;xt\u00001) by\n",
      "2\n",
      "6664p(xt= 1jx1\u0001\u0001\u0001;xt\u00001)\n",
      "p(xt= 2jx1\u0001\u0001\u0001;xt\u00001)\n",
      "...\n",
      "p(xt=Vjx1\u0001\u0001\u0001;xt\u00001)3\n",
      "7775= softmax( Wtct)2RV(14.6)\n",
      "= softmax( Wt\u001e",
      "\u0012(x1;\u0001\u0001\u0001;xt\u00001)); (14.7)\n",
      "whereW2RV\u0002dis a weight matrix that maps the contextualized embedding\n",
      "ctto the logits. In other words, Wtis an additional linear layer for the\n",
      "prediction of the conditional probability. Recall that softmax( \u0001) :RV7!RV\n",
      "maps the logits to the probabilities:\n",
      "softmax(u) =2\n",
      "664exp(u1)PV\n",
      "i=1exp(ui)\n",
      "...\n",
      "exp(uV)PV\n",
      "i=1exp(ui)3\n",
      "775(14.8)\n",
      "7This property no longer holds in masked language models [Devlin et al., 2019] where\n",
      "the losses are also di\u000b",
      "erent.\n",
      "8Heret\u00152 and we omit the loss for predicting p(x1) for simplicity (which also doesn't\n",
      "a\u000b",
      "ect the performance much). To formally model p(x1), an option is to prepend a special\n",
      "tokenx0=?to the sequence, and then ask the language model to predict p(x1jx0=?).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "183\n",
      "We train all the parameters \u0012in the transformers as well as the\n",
      "parameters W= (W1;:::;WT) by the cross entropy loss. Let pt=\n",
      "softmax(Wt\u001e",
      "\u0012(x1;\u0001\u0001\u0001;xt\u00001))2RVbe the predicted conditional probability\n",
      "at position t. LetWbe the concatenation of W1;\u0001\u0001\u0001;WT. The loss function\n",
      "is often called language modeling loss and de\f",
      "ned as\n",
      "L(W;\u0012) =TX\n",
      "t=2(cross entropy loss at position t)\n",
      "=TX\n",
      "t=2\u0000logpt;xt; (14.9)\n",
      "wherept;jdenotes the j-th entry of the probability vector pt.\n",
      "14.3.1 Zero-shot learning and in-context learning\n",
      "For language models, there are many ways to adapt a pretrained model to\n",
      "downstream tasks. In this notes, we discuss three of them: \f",
      "netuning, zero-\n",
      "shot learning, and in-context learning.\n",
      "Finetuning is not very common for the autoregressive language models that\n",
      "we introduced in Section 14.3 but much more common for other variants\n",
      "such as masked language models which has similar input-output interfaces\n",
      "but are pretrained di\u000b",
      "erently [Devlin et al., 2019]. The \f",
      "netuning method is\n",
      "the same as introduced generally in Section 14.1|the only question is how\n",
      "we de\f",
      "ne the prediction task with an additional linear head. One option\n",
      "is to treat cT+1=\u001e",
      "\u0012(x1;\u0001\u0001\u0001;xT) as the representation and use w>cT+1=\n",
      "w>\u001e",
      "\u0012(x1;\u0001\u0001\u0001;xT) to predict task label. As described in Section 14.1, we\n",
      "initialize\u0012to the pretrained model ^\u0012and then optimize both wand\u0012.\n",
      "Zero-shot adaptation or zero-shot learning is the setting where there is no\n",
      "input-output pairs from the downstream tasks. For language problems tasks,\n",
      "typically the task is formatted as a question or a cloze test form via natural\n",
      "language. For example, we can format an example as a question:\n",
      "xtask= (xtask;1;\u0001\u0001\u0001;xtask;T) = \\Is the speed of light a universal constant?\"\n",
      "Then, we compute the most likely next word predicted by the lan-\n",
      "guage model given this question, that is, computing argmaxxT+1p(xT+1j\n",
      "xtask;1;\u0001\u0001\u0001;xtask;T). In this case, if the most likely next word xT+1is \\No\",\n",
      "then we solve the task. (The speed of light is only a constant in vacuum).\n",
      "We note that there are many ways to decode the answer from the language\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "184\n",
      "models, e.g., instead of computing the argmax, we may use the language\n",
      "model to generate a few words word. It is an active research question to \f",
      "nd\n",
      "the best way to utilize the language models.\n",
      "In-context learning is mostly used for few-shot settings where we have a\n",
      "few labeled examples ( x(1)\n",
      "task;y(1)\n",
      "task);\u0001\u0001\u0001;(x(ntask)\n",
      "task;y(ntask)\n",
      "task ). Given a test example\n",
      "xtest, we construct a document ( x1;\u0001\u0001\u0001;xT), which is more commonly called\n",
      "a \\prompt\" in this context, by concatenating the labeled examples and the\n",
      "text example in some format. For example, we may construct the prompt as\n",
      "follows\n",
      "x1;\u0001\u0001\u0001;xT= \\Q: 2\u00183 = ? x(1)\n",
      "task\n",
      "A: 5 y(1)\n",
      "task\n",
      "Q: 6\u00187 = ? x(2)\n",
      "task\n",
      "A: 13 y(2)\n",
      "task\n",
      "\u0001\u0001\u0001\n",
      "Q: 15\u00182 = ?\" xtest\n",
      "Then, we let the pretrained model generate the most likely xT+1;xT+2;\u0001\u0001\u0001:\n",
      "In this case, if the model can \\learn\" that the symbol \u0018means addition from\n",
      "the few examples, we will obtain the following which suggests the answer is\n",
      "17.\n",
      "xT+1;xT+2;\u0001\u0001\u0001=\\A: 17\":\n",
      "The area of foundation models is very new and quickly growing. The notes\n",
      "here only attempt to introduce these models on a conceptual level with a\n",
      "signi\f",
      "cant amount of simpli\f",
      "cation. We refer the readers to other materials,\n",
      "e.g., Bommasani et al. [2021], for more details.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Part V\n",
      "Reinforcement Learning and\n",
      "Control\n",
      "185\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 15\n",
      "Reinforcement learning\n",
      "We now begin our study of reinforcement learning and adaptive control.\n",
      "In supervised learning, we saw algorithms that tried to make their outputs\n",
      "mimic the labels ygiven in the training set. In that setting, the labels gave\n",
      "an unambiguous \\right answer\" for each of the inputs x. In contrast, for\n",
      "many sequential decision making and control problems, it is very di\u000ecult to\n",
      "provide this type of explicit supervision to a learning algorithm. For example,\n",
      "if we have just built a four-legged robot and are trying to program it to walk,\n",
      "then initially we have no idea what the \\correct\" actions to take are to make\n",
      "it walk, and so do not know how to provide explicit supervision for a learning\n",
      "algorithm to try to mimic.\n",
      "In the reinforcement learning framework, we will instead provide our al-\n",
      "gorithms only a reward function, which indicates to the learning agent when\n",
      "it is doing well, and when it is doing poorly. In the four-legged walking ex-\n",
      "ample, the reward function might give the robot positive rewards for moving\n",
      "forwards, and negative rewards for either moving backwards or falling over.\n",
      "It will then be the learning algorithm's job to \f",
      "gure out how to choose actions\n",
      "over time so as to obtain large rewards.\n",
      "Reinforcement learning has been successful in applications as diverse as\n",
      "ight, robot legged locomotion, cell-phone network\n",
      "routing, marketing strategy selection, factory control, and e\u000ecient web-page\n",
      "indexing. Our study of reinforcement learning will begin with a de\f",
      "nition of\n",
      "theMarkov decision processes (MDP) , which provides the formalism in\n",
      "which RL problems are usually posed.\n",
      "186\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "187\n",
      "15.1 Markov decision processes\n",
      ";R ), where:ision process is a tuple ( S;A;fPsag;\n",
      "ight, Sset of states . (For example, in autonomous helicopter \n",
      "might be the set of all possible positions and orientations of the heli-\n",
      "copter.)\n",
      "•Ais a set of actions . (For example, the set of all possible directions in\n",
      "which you can push the helicopter's control sticks.)\n",
      "•Psaare the state transition probabilities. For each state s2Sand\n",
      "actiona2A,Psais a distribution over the state space. We'll say more\n",
      "y, Psagives the distribution over what states\n",
      "we will transition to if we take action ain states.\n",
      "2[0;1) is called the discount factor .\n",
      "•R:S\u0002A7!Ris the reward function . (Rewards are sometimes also\n",
      "written as a function of a state Sonly, in which case we would have\n",
      "R:S7!R).\n",
      "The dynamics of an MDP proceeds as follows: We start in some state s0,\n",
      "and get to choose some action a02Ato take in the MDP. As a result of our\n",
      "choice, the state of the MDP randomly transitions to some successor state\n",
      "s1, drawn according to s1\u0018Ps0a0. Then, we get to pick another action a1.\n",
      "As a result of this action, the state transitions again, now to some s2\u0018Ps1a1.\n",
      "We then pick a2, and so on. . . . Pictorially, we can represent this process as\n",
      "follows:\n",
      "s0a0\u0000!s1a1\u0000!s2a2\u0000!s3a3\u0000!:::\n",
      "Upon visiting the sequence of states s0;s1;:::with actions a0;a1;:::, our\n",
      "total payo\u000b",
      " is given by\n",
      "2R(s2;a2) +\u0001\u0001\u0001:\n",
      "Or, when we are writing rewards as a function of the states only, this becomes\n",
      "2R(s2) +\u0001\u0001\u0001:\n",
      "For most of our development, we will use the simpler state-rewards R(s),\n",
      "though the generalization to state-action rewards R(s;a) o\u000b",
      "ers no special\n",
      "di\u000eculties.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "188\n",
      "Our goal in reinforcement learning is to choose actions over time so as to\n",
      "maximize the expected value of the total payo\u000b",
      ":\n",
      "E\u0002\n",
      "2R(s2) +\u0001\u0001\u0001\u0003\n",
      "t. Thus, tohe reward at timestep tisdiscounted by a factor of \n",
      "make this expectation large, we would like to accrue positive rewards as soon\n",
      "as possible (and postpone negative rewards as long as possible). In economic\n",
      "also has a natural R(\u0001) is the amount of money made, \n",
      "interpretation in terms of the interest rate (where a dollar today is worth\n",
      "more than a dollar tomorrow).\n",
      "Apolicy is any function \u0019:S7!Amapping from the states to the\n",
      "actions. We say that we are executing some policy \u0019if, whenever we are\n",
      "in states, we take action a=\u0019(s). We also de\f",
      "ne the value function for\n",
      "a policy\u0019according to\n",
      "V\u0019(s) = E\u0002\n",
      "2R(s2) +\u0001\u0001\u0001\f",
      "\f",
      "s0=s;\u0019]:\n",
      "V\u0019(s) is simply the expected sum of discounted rewards upon starting in\n",
      "states, and taking actions according to \u0019.1\n",
      "Given a \f",
      "xed policy \u0019, its value function V\u0019satis\f",
      "es the Bellman equa-\n",
      "tions :\n",
      "X\u0019(s) =R(s) +\n",
      "s02SPs\u0019(s)(s0)V\u0019(s0):\n",
      "This says that the expected sum of discounted rewards V\u0019(s) for starting\n",
      "insconsists of two terms: First, the immediate reward R(s) that we get\n",
      "right away simply for starting in state s, and second, the expected sum of\n",
      "future discounted rewards. Examining the second term in more detail, we\n",
      "see that the summation term above can be rewritten E s0\u0018Ps\u0019(s)[V\u0019(s0)]. This\n",
      "is the expected sum of discounted rewards for starting in state s0, wheres0\n",
      "is distributed according Ps\u0019(s), which is the distribution over where we will\n",
      "end up after taking the \f",
      "rst action \u0019(s) in the MDP from state s. Thus, the\n",
      "second term above gives the expected sum of discounted rewards obtained\n",
      "after the \f",
      "rst step in the MDP.\n",
      "Bellman's equations can be used to e\u000eciently solve for V\u0019. Speci\f",
      "cally,\n",
      "in a \f",
      "nite-state MDP ( jSj<1), we can write down one such equation for\n",
      "V\u0019(s) for every state s. This gives us a set of jSjlinear equations in jSj\n",
      "variables (the unknown V\u0019(s)'s, one for each state), which can be e\u000eciently\n",
      "solved for the V\u0019(s)'s.\n",
      "1This notation in which we condition on \u0019isn't technically correct because \u0019isn't a\n",
      "random variable, but this is quite standard in the literature.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "189\n",
      "We also de\f",
      "ne the optimal value function according to\n",
      "V\u0003(s) = max\n",
      "\u0019V\u0019(s): (15.1)\n",
      "In other words, this is the best possible expected sum of discounted rewards\n",
      "that can be attained using any policy. There is also a version of Bellman's\n",
      "equations for the optimal value function:\n",
      "V\u0003(s) =R(s) + max\n",
      "X2A\n",
      "s02SPsa(s0)V\u0003(s0): (15.2)\n",
      "The \f",
      "rst term above is the immediate reward as before. The second term\n",
      "is the maximum over all actions aof the expected future sum of discounted\n",
      "rewards we'll get upon after action a. You should make sure you understand\n",
      "this equation and see why it makes sense.\n",
      "We also de\f",
      "ne a policy \u0019\u0003:S7!Aas follows:\n",
      "\u0019\u0003(s) = arg max\n",
      "a2AX\n",
      "s02SPsa(s0)V\u0003(s0): (15.3)\n",
      "Note that\u0019\u0003(s) gives the action athat attains the maximum in the \\max\"\n",
      "in Equation (15.2).\n",
      "It is a fact that for every state sand every policy \u0019, we have\n",
      "V\u0003(s) =V\u0019\u0003(s)\u0015V\u0019(s):\n",
      "The \f",
      "rst equality says that the V\u0019\u0003, the value function for \u0019\u0003, is equal to the\n",
      "optimal value function V\u0003for every state s. Further, the inequality above\n",
      "says that\u0019\u0003's value is at least a large as the value of any other other policy.\n",
      "In other words, \u0019\u0003as de\f",
      "ned in Equation (15.3) is the optimal policy.\n",
      "Note that\u0019\u0003has the interesting property that it is the optimal policy\n",
      "forallstatess. Speci\f",
      "cally, it is not the case that if we were starting in\n",
      "some state sthen there'd be some optimal policy for that state, and if we\n",
      "were starting in some other state s0then there'd be some other policy that's\n",
      "optimal policy for s0. The same policy \u0019\u0003attains the maximum in Equa-\n",
      "tion (15.1) for allstatess. This means that we can use the same policy \u0019\u0003\n",
      "no matter what the initial state of our MDP is.\n",
      "15.2 Value iteration and policy iteration\n",
      "We now describe two e\u000ecient algorithms for solving \f",
      "nite-state MDPs. For\n",
      "now, we will consider only MDPs with \f",
      "nite state and action spaces ( jSj<\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "190\n",
      "1;jAj<1). In this section, we will also assume that we know the state\n",
      "transition probabilities fPsagand the reward function R.\n",
      "The \f",
      "rst algorithm, value iteration , is as follows:\n",
      "Algorithm 4 Value Iteration\n",
      "1:For each state s, initializeV(s) := 0.\n",
      "2:foruntil convergence do\n",
      "3: For every state, update\n",
      "V(s) :=R(s) + max\n",
      "X2A\n",
      "s0Psa(s0)V(s0): (15.4)\n",
      "This algorithm can be thought of as repeatedly trying to update the\n",
      "estimated value function using Bellman Equations (15.2).\n",
      "There are two possible ways of performing the updates in the inner loop of\n",
      "the algorithm. In the \f",
      "rst, we can \f",
      "rst compute the new values for V(s) for\n",
      "every state s, and then overwrite all the old values with the new values. This\n",
      "is called a synchronous update. In this case, the algorithm can be viewed as\n",
      "implementing a \\Bellman backup operator\" that takes a current estimate of\n",
      "the value function, and maps it to a new estimate. (See homework problem\n",
      "for details.) Alternatively, we can also perform asynchronous updates.\n",
      "Here, we would loop over the states (in some order), updating the values one\n",
      "at a time.\n",
      "Under either synchronous or asynchronous updates, it can be shown that\n",
      "value iteration will cause Vto converge to V\u0003. Having found V\u0003, we can\n",
      "then use Equation (15.3) to \f",
      "nd the optimal policy.\n",
      "Apart from value iteration, there is a second standard algorithm for \f",
      "nd-\n",
      "ing an optimal policy for an MDP. The policy iteration algorithm proceeds\n",
      "as follows:\n",
      "Thus, the inner-loop repeatedly computes the value function for the cur-\n",
      "rent policy, and then updates the policy using the current value function.\n",
      "(The policy \u0019found in step (b) is also called the policy that is greedy with\n",
      "respect to V.) Note that step (a) can be done via solving Bellman's equa-\n",
      "tions as described earlier, which in the case of a \f",
      "xed policy, is just a set of\n",
      "jSjlinear equations in jSjvariables.\n",
      "After at most a \f",
      "nite number of iterations of this algorithm, Vwill con-\n",
      "verge toV\u0003, and\u0019will converge to \u0019\u0003.2\n",
      "2Note that value iteration cannot reach the exact V\u0003in a \f",
      "nite number of iterations,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "191\n",
      "Algorithm 5 Policy Iteration\n",
      "1:Initialize\u0019randomly.\n",
      "2:foruntil convergence do\n",
      "3: LetV:=V\u0019. .typically by linear system solver\n",
      "4: For each state s, let\n",
      "\u0019(s) := arg max\n",
      "a2AX\n",
      "s0Psa(s0)V(s0):\n",
      "Both value iteration and policy iteration are standard algorithms for solv-\n",
      "ing MDPs, and there isn't currently universal agreement over which algo-\n",
      "rithm is better. For small MDPs, policy iteration is often very fats and\n",
      "converges with very few iterations. However, for MDPs with large state\n",
      "spaces, solving for V\u0019explicitly would involve solving a large system of lin-\n",
      "ear equations, and could be di\u000ecult (and note that one has to solve the\n",
      "linear system multiple times in policy iteration). In these problems, value\n",
      "iteration may be preferred. For this reason, in practice value iteration seems\n",
      "to be used more often than policy iteration. For some more discussions on\n",
      "the comparison and connection of value iteration and policy iteration, please\n",
      "see Section 15.5.\n",
      "15.3 Learning a model for an MDP\n",
      "So far, we have discussed MDPs and algorithms for MDPs assuming that the\n",
      "state transition probabilities and rewards are known. In many realistic prob-\n",
      "lems, we are not given state transition probabilities and rewards explicitly,\n",
      "are known.)stead estimate them from data. (Usually, S;A and\n",
      "For example, suppose that, for the inverted pendulum problem (see prob-\n",
      "whereas policy iteration with an exact linear system solver, can. This is because when\n",
      "the actions space and policy space are discrete and \f",
      "nite, and once the policy reaches the\n",
      "optimal policy in policy iteration, then it will not change at all. On the other hand, even\n",
      "though value iteration will converge to the V\u0003, but there is always some non-zero error in\n",
      "the learned value function.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "192\n",
      "lem set 4), we had a number of trials in the MDP, that proceeded as follows:\n",
      "s(1)\n",
      "0a(1)\n",
      "0\u0000!s(1)\n",
      "1a(1)\n",
      "1\u0000!s(1)\n",
      "2a(1)\n",
      "2\u0000!s(1)\n",
      "3a(1)\n",
      "3\u0000!:::\n",
      "s(2)\n",
      "0a(2)\n",
      "0\u0000!s(2)\n",
      "1a(2)\n",
      "1\u0000!s(2)\n",
      "2a(2)\n",
      "2\u0000!s(2)\n",
      "3a(2)\n",
      "3\u0000!:::\n",
      ":::\n",
      "Here,s(j)\n",
      "iis the state we were at time iof trialj, anda(j)\n",
      "iis the cor-\n",
      "responding action that was taken from that state. In practice, each of the\n",
      "trials above might be run until the MDP terminates (such as if the pole falls\n",
      "over in the inverted pendulum problem), or it might be run for some large\n",
      "but \f",
      "nite number of timesteps.\n",
      "Given this \\experience\" in the MDP consisting of a number of trials,\n",
      "we can then easily derive the maximum likelihood estimates for the state\n",
      "transition probabilities:\n",
      "Psa(s0) =#times took we action ain statesand got to s0\n",
      "#times we took action a in state s(15.5)\n",
      "Or, if the ratio above is \\0/0\"|corresponding to the case of never having\n",
      "taken action ain statesbefore|the we might simply estimate Psa(s0) to be\n",
      "1=jSj. (I.e., estimate Psato be the uniform distribution over all states.)\n",
      "Note that, if we gain more experience (observe more trials) in the MDP,\n",
      "there is an e\u000ecient way to update our estimated state transition probabilities\n",
      "using the new experience. Speci\f",
      "cally, if we keep around the counts for both\n",
      "the numerator and denominator terms of (15.5), then as we observe more\n",
      "trials, we can simply keep accumulating those counts. Computing the ratio\n",
      "of these counts then given our estimate of Psa.\n",
      "Using a similar procedure, if Ris unknown, we can also pick our estimate\n",
      "of the expected immediate reward R(s) in statesto be the average reward\n",
      "observed in state s.\n",
      "Having learned a model for the MDP, we can then use either value it-\n",
      "eration or policy iteration to solve the MDP using the estimated transition\n",
      "probabilities and rewards. For example, putting together model learning and\n",
      "value iteration, here is one possible algorithm for learning in an MDP with\n",
      "unknown state transition probabilities:\n",
      "1. Initialize \u0019randomly.\n",
      "2. Repeatf\n",
      "(a) Execute \u0019in the MDP for some number of trials.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "193\n",
      "(b) Using the accumulated experience in the MDP, update our esti-\n",
      "mates forPsa(andR, if applicable).\n",
      "(c) Apply value iteration with the estimated state transition probabil-\n",
      "ities and rewards to get a new estimated value function V.\n",
      "(d) Update \u0019to be the greedy policy with respect to V.\n",
      "g\n",
      "We note that, for this particular algorithm, there is one simple optimiza-\n",
      "tion that can make it run much more quickly. Speci\f",
      "cally, in the inner loop\n",
      "of the algorithm where we apply value iteration, if instead of initializing value\n",
      "iteration with V= 0, we initialize it with the solution found during the pre-\n",
      "vious iteration of our algorithm, then that will provide value iteration with\n",
      "a much better initial starting point and make it converge more quickly.\n",
      "15.4 Continuous state MDPs\n",
      "So far, we've focused our attention on MDPs with a \f",
      "nite number of states.\n",
      "We now discuss algorithms for MDPs that may have an in\f",
      "nite number of\n",
      "states. For example, for a car, we might represent the state as ( x;y;\u0012; _x;_y;_\u0012),\n",
      "comprising its position ( x;y); orientation \u0012; velocity in the xandydirections\n",
      "_xand _y; and angular velocity _\u0012. Hence,S=R6is an in\f",
      "nite set of states,\n",
      "because there is an in\f",
      "nite number of possible positions and orientations\n",
      "for the car.3Similarly, the inverted pendulum you saw in PS4 has states\n",
      "ying in 3d\u0012), where\u0012is the angle of the pole. And, a helicopter \n",
      "space has states of the form ( x;y;z;\u001e",
      ";\u0012; ; _x;_y;_z;_\u001e",
      ";_\u0012;_ ), where here the roll\n",
      "\u001e",
      ", pitch\u0012, and yaw angles specify the 3d orientation of the helicopter.\n",
      "In this section, we will consider settings where the state space is S=Rd,\n",
      "and describe ways for solving such MDPs.\n",
      "15.4.1 Discretization\n",
      "Perhaps the simplest way to solve a continuous-state MDP is to discretize\n",
      "the state space, and then to use an algorithm like value iteration or policy\n",
      "iteration, as described previously.\n",
      "For example, if we have 2d states ( s1;s2), we can use a grid to discretize\n",
      "the state space:\n",
      "3Technically, \u0012is an orientation and so the range of \u0012is better written \u00122[\u0000\u0019;\u0019) than\n",
      "\u00122R; but for our purposes, this distinction is not important.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "194\n",
      "[t]\n",
      "Here, each grid cell represents a separate discrete state \u0016 s. We can\n",
      "then approximate the continuous-state MDP via a discrete-state one\n",
      ";R ), where \u0016Sis the set of discrete states, fP\u0016sagare our state\n",
      "transition probabilities over the discrete states, and so on. We can then use\n",
      "value iteration or policy iteration to solve for the V\u0003(\u0016s) and\u0019\u0003(\u0016s) in the\n",
      ";R ). When our actual system is in some\n",
      "continuous-valued state s2Sand we need to pick an action to execute, we\n",
      "compute the corresponding discretized state \u0016 s, and execute action \u0019\u0003(\u0016s).\n",
      "This discretization approach can work well for many problems. However,\n",
      "there are two downsides. First, it uses a fairly naive representation for V\u0003\n",
      "(and\u0019\u0003). Speci\f",
      "cally, it assumes that the value function is takes a constant\n",
      "value over each of the discretization intervals (i.e., that the value function is\n",
      "piecewise constant in each of the gridcells).\n",
      "To better understand the limitations of such a representation, consider a\n",
      "supervised learning problem of \f",
      "tting a function to this dataset:\n",
      "[t]\n",
      "1 2 3 4 5 6 7 81.522.533.544.555.5\n",
      "xy\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "195\n",
      "Clearly, linear regression would do \f",
      "ne on this problem. However, if we\n",
      "instead discretize the x-axis, and then use a representation that is piecewise\n",
      "constant in each of the discretization intervals, then our \f",
      "t to the data would\n",
      "look like this:\n",
      "[t]\n",
      "1 2 3 4 5 6 7 81.522.533.544.555.5\n",
      "xy\n",
      "This piecewise constant representation just isn't a good representation for\n",
      "many smooth functions. It results in little smoothing over the inputs, and no\n",
      "generalization over the di\u000b",
      "erent grid cells. Using this sort of representation,\n",
      "we would also need a very \f",
      "ne discretization (very small grid cells) to get a\n",
      "good approximation.\n",
      "A second downside of this representation is called the curse of dimen-\n",
      "sionality . SupposeS=Rd, and we discretize each of the ddimensions of the\n",
      "state intokvalues. Then the total number of discrete states we have is kd.\n",
      "This grows exponentially quickly in the dimension of the state space d, and\n",
      "thus does not scale well to large problems. For example, with a 10d state, if\n",
      "we discretize each state variable into 100 values, we would have 10010= 1020\n",
      "discrete states, which is far too many to represent even on a modern desktop\n",
      "computer.\n",
      "As a rule of thumb, discretization usually works extremely well for 1d\n",
      "and 2d problems (and has the advantage of being simple and quick to im-\n",
      "plement). Perhaps with a little bit of cleverness and some care in choosing\n",
      "the discretization method, it often works well for problems with up to 4d\n",
      "states. If you're extremely clever, and somewhat lucky, you may even get it\n",
      "to work for some 6d problems. But it very rarely works for problems any\n",
      "higher dimensional than that.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "196\n",
      "15.4.2 Value function approximation\n",
      "We now describe an alternative method for \f",
      "nding policies in continuous-\n",
      "state MDPs, in which we approximate V\u0003directly, without resorting to dis-\n",
      "cretization. This approach, called value function approximation, has been\n",
      "successfully applied to many RL problems.\n",
      "Using a model or simulator\n",
      "To develop a value function approximation algorithm, we will assume that\n",
      "we have a model , orsimulator , for the MDP. Informally, a simulator is\n",
      "a black-box that takes as input any (continuous-valued) state stand action\n",
      "at, and outputs a next-state st+1sampled according to the state transition\n",
      "probabilities Pstat:\n",
      "[t]\n",
      "There are several ways that one can get such a model. One is to use\n",
      "physics simulation. For example, the simulator for the inverted pendulum\n",
      "in PS4 was obtained by using the laws of physics to calculate what position\n",
      "and orientation the cart/pole will be in at time t+ 1, given the current state\n",
      "at timetand the action ataken, assuming that we know all the parameters\n",
      "of the system such as the length of the pole, the mass of the pole, and so\n",
      "on. Alternatively, one can also use an o\u000b",
      "-the-shelf physics simulation software\n",
      "package which takes as input a complete physical description of a mechanical\n",
      "system, the current state stand actionat, and computes the state st+1of the\n",
      "system a small fraction of a second into the future.4\n",
      "An alternative way to get a model is to learn one from data collected in\n",
      "the MDP. For example, suppose we execute ntrials in which we repeatedly\n",
      "take actions in an MDP, each trial for Ttimesteps. This can be done picking\n",
      "actions at random, executing some speci\f",
      "c policy, or via some other way of\n",
      "4Open Dynamics Engine (http://www.ode.com) is one example of a free/open-source\n",
      "physics simulator that can be used to simulate systems like the inverted pendulum, and\n",
      "that has been a reasonably popular choice among RL researchers.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "197\n",
      "choosing actions. We would then observe nstate sequences like the following:\n",
      "s(1)\n",
      "0a(1)\n",
      "0\u0000!s(1)\n",
      "1a(1)\n",
      "1\u0000!s(1)\n",
      "2a(1)\n",
      "2\u0000!\u0001\u0001\u0001a(1)\n",
      "T\u00001\u0000!s(1)\n",
      "T\n",
      "s(2)\n",
      "0a(2)\n",
      "0\u0000!s(2)\n",
      "1a(2)\n",
      "1\u0000!s(2)\n",
      "2a(2)\n",
      "2\u0000!\u0001\u0001\u0001a(2)\n",
      "T\u00001\u0000!s(2)\n",
      "T\n",
      "\u0001\u0001\u0001\n",
      "s(n)\n",
      "0a(n)\n",
      "0\u0000!s(n)\n",
      "1a(n)\n",
      "1\u0000!s(n)\n",
      "2a(n)\n",
      "2\u0000!\u0001\u0001\u0001a(n)\n",
      "T\u00001\u0000!s(n)\n",
      "T\n",
      "We can then apply a learning algorithm to predict st+1as a function of st\n",
      "andat.\n",
      "For example, one may choose to learn a linear model of the form\n",
      "st+1=Ast+Bat; (15.6)\n",
      "using an algorithm similar to linear regression. Here, the parameters of the\n",
      "model are the matrices AandB, and we can estimate them using the data\n",
      "collected from our ntrials, by picking\n",
      "arg min\n",
      "A;BnX\n",
      "i=1T\u00001X\n",
      "s(i)\n",
      "t+1\u0000\u0010\n",
      "As(i)\n",
      "t+Ba(i)\n",
      "2\u0011\n",
      "2:\n",
      "We could also potentially use other loss functions for learning the model.\n",
      "For example, it has been found in recent work Luo et al. [2018] that using\n",
      "k\u0001k 2norm (without the square) may be helpful in certain cases.\n",
      "Having learned AandB, one option is to build a deterministic model,\n",
      "in which given an input standat, the output st+1is exactly determined.\n",
      "Speci\f",
      "cally, we always compute st+1according to Equation (15.6). Alter-\n",
      "natively, we may also build a stochastic model, in which st+1is a random\n",
      "function of the inputs, by modeling it as\n",
      "st+1=Ast+Bat+\u000ft;\n",
      "where here \u000ftis a noise term, usually modeled as \u000ft\u0018N(0;\u0006). (The covari-\n",
      "ance matrix \u0006 can also be estimated from data in a straightforward way.)\n",
      "Here, we've written the next-state st+1as a linear function of the current\n",
      "state and action; but of course, non-linear functions are also possible. Specif-\n",
      "ically, one can learn a model st+1=A\u001e",
      "s(st) +B\u001e",
      "a(at), where\u001e",
      "sand\u001e",
      "aare\n",
      "some non-linear feature mappings of the states and actions. Alternatively,\n",
      "one can also use non-linear learning algorithms, such as locally weighted lin-\n",
      "ear regression, to learn to estimate st+1as a function of standat. These\n",
      "approaches can also be used to build either deterministic or stochastic sim-\n",
      "ulators of an MDP.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "198\n",
      "Fitted value iteration\n",
      "We now describe the \f",
      "tted value iteration algorithm for approximating\n",
      "the value function of a continuous state MDP. In the sequel, we will assume\n",
      "that the problem has a continuous state space S=Rd, but that the action\n",
      "spaceAis small and discrete.5\n",
      "Recall that in value iteration, we would like to perform the update\n",
      "max) :=R(s) +\n",
      "aZ\n",
      "s0Psa(s0)V(s0)ds0(15.7)\n",
      "maxs) +\n",
      "aEs0\u0018Psa[V(s0)] (15.8)\n",
      "(In Section 15.2, we had written the value iteration update with a summation\n",
      "maxaP:=R(s) +\n",
      "s0Psa(s0)V(s0) rather than an integral over states;\n",
      "ects that we are now working in continuous states rather\n",
      "than discrete states.)\n",
      "The main idea of \f",
      "tted value iteration is that we are going to approxi-\n",
      "mately carry out this step, over a \f",
      "nite sample of states s(1);:::;s(n). Specif-\n",
      "ically, we will use a supervised learning algorithm|linear regression in our\n",
      "description below|to approximate the value function as a linear or non-linear\n",
      "function of the states:\n",
      "V(s) =\u0012T\u001e",
      "(s):\n",
      "Here,\u001e",
      "is some appropriate feature mapping of the states.\n",
      "For each state sin our \f",
      "nite sample of nstates, \f",
      "tted value iteration\n",
      "will \f",
      "rst compute a quantity y(i), which will be our approximation to R(s) +\n",
      "maxaEs0\u0018Psa[V(s0)] (the right hand side of Equation 15.8). Then, it will\n",
      "apply a supervised learning algorithm to try to get V(s) close toR(s) +\n",
      "maxaEs0\u0018Psa[V(s0)] (or, in other words, to try to get V(s) close toy(i)).\n",
      "In detail, the algorithm is as follows:\n",
      "1. Randomly sample nstatess(1);s(2);:::s(n)2S.\n",
      "2. Initialize \u0012:= 0.\n",
      "3. Repeatf\n",
      "Fori= 1;:::;nf\n",
      "5In practice, most MDPs have much smaller action spaces than state spaces. E.g., a car\n",
      "has a 6d state space, and a 2d action space (steering and velocity controls); the inverted\n",
      "pendulum has a 4d state space, and a 1d action space; a helicopter has a 12d state space,\n",
      "and a 4d action space. So, discretizing this set of actions is usually less of a problem than\n",
      "discretizing the state space would have been.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "199\n",
      "For each action a2Af\n",
      "Samples0\n",
      "1;:::;s0\n",
      "k\u0018Ps(i)a(using a model of the MDP).\n",
      "Setq(a) =1\n",
      "kPk\n",
      "V(s0(s(i)) +\n",
      "j)\n",
      "==Hence,q(a) is an estimate of R(s(i)) +\n",
      "Es0\u0018Ps(i)a[V(s0)].\n",
      "g\n",
      "Sety(i)= maxaq(a).\n",
      "==Hence,y(i)is an estimate of R(s(i)) +\n",
      "maxaEs0\u0018Ps(i)a[V(s0)].\n",
      "g\n",
      "==In the original value iteration algorithm (over discrete states)\n",
      "==we updated the value function according to V(s(i)) :=y(i).\n",
      "==In this algorithm, we want V(s(i))\u0019y(i), which we'll achieve\n",
      "==using supervised learning (linear regression).\n",
      "Set\u0012:= arg min \u00121\n",
      "2Pn\n",
      "i=1\u0000\n",
      "\u0012T\u001e",
      "(s(i))\u0000y(i)\u00012\n",
      "g\n",
      "Above, we had written out \f",
      "tted value iteration using linear regression\n",
      "as the algorithm to try to make V(s(i)) close toy(i). That step of the algo-\n",
      "rithm is completely analogous to a standard supervised learning (regression)\n",
      "problem in which we have a training set ( x(1);y(1));(x(2);y(2));:::; (x(n);y(n)),\n",
      "and want to learn a function mapping from xtoy; the only di\u000b",
      "erence is that\n",
      "heresplays the role of x. Even though our description above used linear re-\n",
      "gression, clearly other regression algorithms (such as locally weighted linear\n",
      "regression) can also be used.\n",
      "Unlike value iteration over a discrete set of states, \f",
      "tted value iteration\n",
      "cannot be proved to always to converge. However, in practice, it often does\n",
      "converge (or approximately converge), and works well for many problems.\n",
      "Note also that if we are using a deterministic simulator/model of the MDP,\n",
      "then \f",
      "tted value iteration can be simpli\f",
      "ed by setting k= 1 in the algorithm.\n",
      "This is because the expectation in Equation (15.8) becomes an expectation\n",
      "over a deterministic distribution, and so a single example is su\u000ecient to\n",
      "exactly compute that expectation. Otherwise, in the algorithm above, we\n",
      "had to draw ksamples, and average to try to approximate that expectation\n",
      "(see the de\f",
      "nition of q(a), in the algorithm pseudo-code).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "200\n",
      "Finally, \f",
      "tted value iteration outputs V, which is an approximation to\n",
      "V\u0003. This implicitly de\f",
      "nes our policy. Speci\f",
      "cally, when our system is in\n",
      "some state s, and we need to choose an action, we would like to choose the\n",
      "action\n",
      "arg max\n",
      "aEs0\u0018Psa[V(s0)] (15.9)\n",
      "The process for computing/approximating this is similar to the inner-loop of\n",
      "\f",
      "tted value iteration, where for each action, we sample s0\n",
      "1;:::;s0\n",
      "k\u0018Psato\n",
      "approximate the expectation. (And again, if the simulator is deterministic,\n",
      "we can setk= 1.)\n",
      "In practice, there are often other ways to approximate this step as well.\n",
      "For example, one very common case is if the simulator is of the form st+1=\n",
      "f(st;at) +\u000ft, wherefis some deterministic function of the states (such as\n",
      "f(st;at) =Ast+Bat), and\u000fis zero-mean Gaussian noise. In this case, we\n",
      "can pick the action given by\n",
      "arg max\n",
      "aV(f(s;a)):\n",
      "In other words, here we are just setting \u000ft= 0 (i.e., ignoring the noise in\n",
      "the simulator), and setting k= 1. Equivalent, this can be derived from\n",
      "Equation (15.9) using the approximation\n",
      "Es0[V(s0)]\u0019V(Es0[s0]) (15.10)\n",
      "=V(f(s;a)); (15.11)\n",
      "where here the expectation is over the random s0\u0018Psa. So long as the noise\n",
      "terms\u000ftare small, this will usually be a reasonable approximation.\n",
      "However, for problems that don't lend themselves to such approximations,\n",
      "having to sample kjAjstates using the model, in order to approximate the\n",
      "expectation above, can be computationally expensive.\n",
      "15.5 Connections between Policy and Value\n",
      "Iteration (Optional)\n",
      "In the policy iteration, line 3 of Algorithm 5, we typically use linear system\n",
      "solver to compute V\u0019. Alternatively, one can also the iterative Bellman\n",
      "updates, similarly to the value iteration, to evaluate V\u0019, as in the Procedure\n",
      "VE(\u0001) in Line 1 of Algorithm 6 below. Here if we take option 1 in Line 2 of\n",
      "the Procedure VE, then the di\u000b",
      "erence between the Procedure VE from the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "201\n",
      "Algorithm 6 Variant of Policy Iteration\n",
      "1:procedure VE(\u0019,k) .To evaluate V\u0019\n",
      "2: Option 1: initialize V(s) := 0; Option 2: Initialize from the current\n",
      "Vin the main algorithm.\n",
      "3: fori= 0 tok\u00001do\n",
      "4: For every state s, update\n",
      "X(s) :=R(s) +\n",
      "s0Ps\u0019(s)(s0)V(s0): (15.12)\n",
      "returnV\n",
      "5:\n",
      "Require: hyperparameter k.\n",
      "6:Initialize\u0019randomly.\n",
      "7:foruntil convergence do\n",
      "8: LetV= VE(\u0019;k).\n",
      "9: For each state s, let\n",
      "\u0019(s) := arg max\n",
      "a2AX\n",
      "s0Psa(s0)V(s0): (15.13)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "202\n",
      "value iteration (Algorithm 4) is that on line 4, the procedure is using the\n",
      "action from \u0019instead of the greedy action.\n",
      "Using the Procedure VE, we can build Algorithm 6, which is a variant\n",
      "of policy iteration that serves an intermediate algorithm that connects pol-\n",
      "icy iteration and value iteration. Here we are going to use option 2 in VE\n",
      "to maximize the re-use of knowledge learned before. One can verify indeed\n",
      "that if we take k= 1 and use option 2 in Line 2 in Algorithm 6, then Algo-\n",
      "rithm 6 is semantically equivalent to value iteration (Algorithm 4). In other\n",
      "words, both Algorithm 6 and value iteration interleave the updates in (15.13)\n",
      "and (15.12). Algorithm 6 alternate between ksteps of update (15.12) and\n",
      "one step of (15.13), whereas value iteration alternates between 1 steps of up-\n",
      "date (15.12) and one step of (15.13). Therefore generally Algorithm 6 should\n",
      "not be faster than value iteration, because assuming that update (15.12)\n",
      "and (15.13) are equally useful and time-consuming, then the optimal balance\n",
      "of the update frequencies could be just k= 1 ork\u00191.\n",
      "On the other hand, if ksteps of update (15.12) can be done much faster\n",
      "thanktimes a single step of (15.12), then taking additional steps of equa-\n",
      "tion (15.12) in group might be useful. This is what policy iteration is lever-\n",
      "aging | the linear system solver can give us the result of Procedure VE with\n",
      "ip1much faster than using the Procedure VE for a large k. On the \n",
      "side, when such a speeding-up e\u000b",
      "ect no longer exists, e.g.,, when the state\n",
      "space is large and linear system solver is also not fast, then value iteration is\n",
      "more preferable.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 16\n",
      "LQR, DDP and LQG\n",
      "16.1 Finite-horizon MDPs\n",
      "In Chapter 15, we de\f",
      "ned Markov Decision Processes (MDPs) and covered\n",
      "Value Iteration / Policy Iteration in a simpli\f",
      "ed setting. More speci\f",
      "cally we\n",
      "introduced the optimal Bellman equation that de\f",
      "nes the optimal value\n",
      "functionV\u0019\u0003of the optimal policy \u0019\u0003.\n",
      "V\u0019\u0003(s) =R(s) + max\n",
      "X2A\n",
      "s02SPsa(s0)V\u0019\u0003(s0)\n",
      "Recall that from the optimal value function, we were able to recover the\n",
      "optimal policy \u0019\u0003with\n",
      "\u0019\u0003(s) = argmaxa2AX\n",
      "s02SPsa(s0)V\u0003(s0)\n",
      "In this chapter, we'll place ourselves in a more general setting:\n",
      "1. We want to write equations that make sense for both the discrete and\n",
      "the continuous case. We'll therefore write\n",
      "Es0\u0018Psa\u0002\n",
      "V\u0019\u0003(s0)\u0003\n",
      "instead ofX\n",
      "s02SPsa(s0)V\u0019\u0003(s0)\n",
      "meaning that we take the expectation of the value function at the next\n",
      "state. In the \f",
      "nite case, we can rewrite the expectation as a sum over\n",
      "203\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "204\n",
      "states. In the continuous case, we can rewrite the expectation as an\n",
      "integral. The notation s0\u0018Psameans that the state s0is sampled from\n",
      "the distribution Psa.\n",
      "2. We'll assume that the rewards depend on both states and actions . In\n",
      "other words, R:S\u0002A! R. This implies that the previous mechanism\n",
      "for computing the optimal action is changed into\n",
      "Es0\u0018Psa\u0002argmaxa2AR(s;a) +\n",
      "V\u0019\u0003(s0)\u0003\n",
      "3. Instead of considering an in\f",
      "nite horizon MDP, we'll assume that we\n",
      "have a \f",
      "nite horizon MDP that will be de\f",
      "ned as a tuple\n",
      "(S;A;Psa;T;R )\n",
      "withT > 0 the time horizon (for instance T= 100). In this setting,\n",
      "our de\f",
      "nition of payo\u000b",
      " is going to be (slightly) di\u000b",
      "erent:\n",
      "R(s0;a0) +R(s1;a1) +\u0001\u0001\u0001+R(sT;aT)\n",
      "instead of (in\f",
      "nite horizon case)\n",
      "2R(s2;a2) +:::\n",
      "1X\n",
      "t=0R(st;at)\n",
      "?Remember that the intro-ount factor \n",
      "was (partly) justi\f",
      "ed by the necessity of making sure that\n",
      "the in\f",
      "nite sum would be \f",
      "nite and well-de\f",
      "ned. If the rewards are\n",
      "bounded by a constant \u0016R, the payo\u000b",
      " is indeed bounded by\n",
      "j1X\n",
      "tj\u0014\u0016R1X)\n",
      "t=0\n",
      "and we recognize a geometric sum! Here, as the payo\u000b",
      " is a \f",
      "nite sum,\n",
      "is not necessary anymore.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "205\n",
      "In this new setting, things behave quite di\u000b",
      "erently. First, the optimal\n",
      "policy\u0019\u0003might be non-stationary, meaning that it changes over time .\n",
      "In other words, now we have\n",
      "\u0019(t):S!A\n",
      "where the superscript ( t) denotes the policy at time step t. The dynam-\n",
      "ics of the \f",
      "nite horizon MDP following policy \u0019(t)proceeds as follows:\n",
      "we start in some state s0, take some action a0:=\u0019(0)(s0) according to\n",
      "our policy at time step 0. The MDP transitions to a successor s1, drawn\n",
      "according to Ps0a0. Then, we get to pick another action a1:=\u0019(1)(s1)\n",
      "following our new policy at time step 1 and so on...\n",
      "Why does the optimal policy happen to be non-stationary in the \f",
      "nite-\n",
      "horizon setting? Intuitively, as we have a \f",
      "nite numbers of actions to\n",
      "take, we might want to adopt di\u000b",
      "erent strategies depending on where\n",
      "we are in the environment and how much time we have left. Imagine\n",
      "a grid with 2 goals with rewards +1 and +10. At the beginning, we\n",
      "might want to take actions to aim for the +10 goal. But if after some\n",
      "steps, dynamics somehow pushed us closer to the +1 goal and we don't\n",
      "have enough steps left to be able to reach the +10 goal, then a better\n",
      "strategy would be to aim for the +1 goal...\n",
      "4. This observation allows us to use time dependent dynamics\n",
      "st+1\u0018P(t)\n",
      "st;at\n",
      "meaning that the transition's distribution P(t)\n",
      "st;atchanges over time. The\n",
      "same thing can be said about R(t). Note that this setting is a better\n",
      "model for real life. In a car, the gas tank empties, tra\u000ec changes,\n",
      "etc. Combining the previous remarks, we'll use the following general\n",
      "formulation for our \f",
      "nite horizon MDP\n",
      "\u0000\n",
      "S;A;P(t)\n",
      "sa;T;R(t)\u0001\n",
      "Remark : notice that the above formulation would be equivalent to\n",
      "adding the time into the state.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "206\n",
      "The value function at time tfor a policy \u0019is then de\f",
      "ned in the same\n",
      "way as before, as an expectation over trajectories generated following\n",
      "policy\u0019starting in state s.\n",
      "Vt(s) =E\u0002\n",
      "R(t)(st;at) +\u0001\u0001\u0001+R(T)(sT;aT)jst=s;\u0019\u0003\n",
      "Now, the question is\n",
      "In this \f",
      "nite-horizon setting, how do we \f",
      "nd the optimal value function\n",
      "V\u0003\n",
      "t(s) = max\n",
      "\u0019V\u0019\n",
      "t(s)\n",
      "It turns out that Bellman's equation for Value Iteration is made for Dy-\n",
      "namic Programming . This may come as no surprise as Bellman is one of\n",
      "the fathers of dynamic programming and the Bellman equation is strongly\n",
      "related to the \f",
      "eld. To understand how we can simplify the problem by\n",
      "adopting an iteration-based approach, we make the following observations:\n",
      "1. Notice that at the end of the game (for time step T), the optimal value\n",
      "is obvious\n",
      "8s2S:V\u0003\n",
      "T(s) := max\n",
      "a2AR(T)(s;a) (16.1)\n",
      "2. For another time step 0 \u0014t < T , if we suppose that we know the\n",
      "optimal value function for the next time step V\u0003\n",
      "t+1, then we have\n",
      "8t<T;s2S:V\u0003\n",
      "t(s) := max\n",
      "a2Ah\n",
      "R(t)(s;a) +Es0\u0018P(t)\n",
      "sa\u0002\n",
      "V\u0003\n",
      "t+1(s0)\u0003i\n",
      "(16.2)\n",
      "With these observations in mind, we can come up with a clever algorithm\n",
      "to solve for the optimal value function:\n",
      "1. compute V\u0003\n",
      "Tusing equation (16.1).\n",
      "2. fort=T\u00001;:::; 0:\n",
      "computeV\u0003\n",
      "tusingV\u0003\n",
      "t+1using equation (16.2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "207\n",
      "Side note We can interpret standard value iteration as a special case\n",
      "of this general case, but without keeping track of time. It turns out that\n",
      "Tn the standard setting, if we run value iteration for T steps, we get a \n",
      "approximation of the optimal value iteration (geometric convergence). See\n",
      "problem set 4 for a proof of the following result:\n",
      "Theorem LetBdenote the Bellman update and jjf(x)jj1:= supxjf(x)j.\n",
      "IfVtdenotes the value function at the t-th step, then\n",
      "jjVt+1\u0000V\u0003jj1=jjB(Vt)\u0000V\u0003jj1\n",
      "jjVt\u0000V\u0003jj1\n",
      "tjjV1\u0000V\u0003jj1\n",
      "-contracting operator.llman operator Bis a\n",
      "16.2 Linear Quadratic Regulation (LQR)\n",
      "In this section, we'll cover a special case of the \f",
      "nite-horizon setting described\n",
      "in Section 16.1, for which the exact solution is (easily) tractable. This\n",
      "model is widely used in robotics, and a common technique in many problems\n",
      "is to reduce the formulation to this framework.\n",
      "First, let's describe the model's assumptions. We place ourselves in the\n",
      "continuous setting, with\n",
      "S=Rd;A=Rd\n",
      "and we'll assume linear transitions (with noise)\n",
      "st+1=Atst+Btat+wt\n",
      "whereAt2Rd\u0002d;Bt2Rd\u0002dare matrices and wt\u0018N (0;\u0006t) is some\n",
      "gaussian noise (with zero mean). As we'll show in the following paragraphs,\n",
      "it turns out that the noise, as long as it has zero mean, does not impact the\n",
      "optimal policy!\n",
      "We'll also assume quadratic rewards\n",
      "R(t)(st;at) =\u0000s>\n",
      "tUtst\u0000a>\n",
      "tWtat\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "208\n",
      "whereUt2Rd\u0002n;Wt2Rd\u0002dare positive de\f",
      "nite matrices (meaning that\n",
      "the reward is always negative ).\n",
      "Remark Note that the quadratic formulation of the reward is equivalent\n",
      "to saying that we want our state to be close to the origin (where the reward\n",
      "is higher). For example, if Ut=Id(the identity matrix) and Wt=Id, then\n",
      "Rt=\u0000jjstjj2\u0000jjatjj2, meaning that we want to take smooth actions (small\n",
      "norm ofat) to go back to the origin (small norm of st). This could model a\n",
      "car trying to stay in the middle of lane without making impulsive moves...\n",
      "Now that we have de\f",
      "ned the assumptions of our LQR model, let's cover\n",
      "the 2 steps of the LQR algorithm\n",
      "step 1 suppose that we don't know the matrices A;B; \u0006. To esti-\n",
      "mate them, we can follow the ideas outlined in the Value Ap-\n",
      "proximation section of the RL notes. First, collect transitions\n",
      "from an arbitrary policy. Then, use linear regression to \f",
      "nd\n",
      "argminA;BPn\n",
      "i=1PT\u00001\n",
      "s(i)\n",
      "t+1\u0000\u0010\n",
      "As(i)\n",
      "t+Ba(i)\n",
      "2\u0011\n",
      ". Finally, use a tech-\n",
      "nique seen in Gaussian Discriminant Analysis to learn \u0006.\n",
      "step 2 assuming that the parameters of our model are known (given or esti-\n",
      "mated with step 1), we can derive the optimal policy using dynamic\n",
      "programming.\n",
      "In other words, given\n",
      "(\n",
      "st+1 =Atst+Btat+wtAt;Bt;Ut;Wt;\u0006tknown\n",
      "R(t)(st;at) =\u0000s>\n",
      "tUtst\u0000a>\n",
      "tWtat\n",
      "we want to compute V\u0003\n",
      "t. If we go back to section 16.1, we can apply\n",
      "dynamic programming, which yields\n",
      "1.Initialization step\n",
      "For the last time step T,\n",
      "V\u0003\n",
      "T(sT) = max\n",
      "aT2ART(sT;aT)\n",
      "= max\n",
      "aT2A\u0000s>\n",
      "TUTsT\u0000a>\n",
      "TWtaT\n",
      "=\u0000s>\n",
      "TUtsT (maximized for aT= 0)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "209\n",
      "2.Recurrence step\n",
      "Lett<T . Suppose we know V\u0003\n",
      "t+1.\n",
      "Fact 1: It can be shown that if V\u0003\n",
      "t+1is a quadratic function in st, thenV\u0003\n",
      "t\n",
      "is also a quadratic function. In other words, there exists some matrix\n",
      "and some scalar \t such that\n",
      "ifV\u0003\n",
      "t+1(st+1) =s>\n",
      "t+t+1st+1+ \tt+1\n",
      "thenV\u0003\n",
      "t(st) =s>\n",
      "tst+ \tt\n",
      "For time step t=T, we had t=\u0000UTand \tT= 0.\n",
      "Fact 2: We can show that the optimal policy is just a linear function of\n",
      "the state.\n",
      "KnowingV\u0003\n",
      "t+1is equivalent to knowing t+1and \tt+1, so we just need\n",
      "to explain how we compute tand \ttfromt+1and \tt+1and the other\n",
      "parameters of the problem.\n",
      "V\u0003\n",
      "t(st) =s>\n",
      "tst+ \tt\n",
      "= max\n",
      "ath\n",
      "R(t)(st;at) +Est+1\u0018P(t)\n",
      "st;at[V\u0003\n",
      "t+1(st+1)]i\n",
      "= max\n",
      "at\u0002\n",
      "\u0000s>\n",
      "tUtst\u0000a>\n",
      "tVtat+Est+1\u0018N(Atst+Btat;\u0006t)[s>\n",
      "t+t+1st+1+ \tt+1]\u0003\n",
      "where the second line is just the de\f",
      "nition of the optimal value function\n",
      "and the third line is obtained by plugging in the dynamics of our model\n",
      "along with the quadratic assumption. Notice that the last expression is\n",
      "a quadratic function in atand can thus be (easily) optimized1. We get\n",
      "the optimal action a\u0003\n",
      "t\n",
      "a\u0003\n",
      "t=\u0002\n",
      "(B>\n",
      "t+1Bt\u0000Vt)\u00001Bt+1At\u0003\n",
      "\u0001st\n",
      "=Lt\u0001st\n",
      "where\n",
      "Lt:=\u0002\n",
      "(B>\n",
      "t+1Bt\u0000Wt)\u00001Bt+1At\u0003\n",
      "1Use the identity E\u0002\n",
      "w>\n",
      "t+1wt\u0003\n",
      "= Tr(\u0006t+1) withwt\u0018N (0;\u0006t)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "210\n",
      "which is an impressive result: our optimal policy is linear inst. Given\n",
      "a\u0003\n",
      "twe can solve for tand \tt. We \f",
      "nally get the Discrete Ricatti\n",
      "equations\n",
      "\bt=A>\n",
      "t\u0010\n",
      "\bt+1t+1Bt\u0000\n",
      "B>\n",
      "t+1Bt\u0000Wt\u0001\u00001Bt+1\u0011\n",
      "At\u0000Ut\n",
      "\tt=\u0000tr (\u0006t+1) + \tt+1\n",
      "Fact 3: we notice that tdepends on neither \t nor the noise \u0006 t! AsLt\n",
      "is a function of At;Btandt+1, it implies that the optimal policy also\n",
      "does not depend on the noise ! (But \t tdoes depend on \u0006 t, which\n",
      "implies that V\u0003\n",
      "tdepends on \u0006 t.)\n",
      "Then, to summarize, the LQR algorithm works as follows\n",
      "1. (if necessary) estimate parameters At;Bt;\u0006t\n",
      "2. initialize T:=\u0000UTand \tT:= 0.\n",
      "3. iterate from t=T\u00001:::0 to update tand \ttusingt+1and \tt+1\n",
      "using the discrete Ricatti equations. If there exists a policy that drives\n",
      "the state towards zero, then convergence is guaranteed!\n",
      "Using Fact 3 , we can be even more clever and make our algorithm run\n",
      "(slightly) faster! As the optimal policy does not depend on \t t, and the\n",
      "update of tonly depends on t, it is su\u000ecient to update onlyt!\n",
      "16.3 From non-linear dynamics to LQR\n",
      "It turns out that a lot of problems can be reduced to LQR, even if dynamics\n",
      "are non-linear. While LQR is a nice formulation because we are able to come\n",
      "up with a nice exact solution, it is far from being general. Let's take for\n",
      "instance the case of the inverted pendulum. The transitions between states\n",
      "look like\n",
      "0\n",
      "BB@xt+1\n",
      "_xt+1\n",
      "\u0012t+1\n",
      "_\u0012t+11\n",
      "CCA=F0\n",
      "BB@0\n",
      "BB@xt\n",
      "_xt\n",
      "\u0012t\n",
      "_\u0012t1\n",
      "CCA;at1\n",
      "CCA\n",
      "where the function Fdepends on the cos of the angle etc. Now, the\n",
      "question we may ask is\n",
      "Can we linearize this system?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "211\n",
      "16.3.1 Linearization of dynamics\n",
      "Let's suppose that at time t, the system spends most of its time in some state\n",
      "\u0016stand the actions we perform are around \u0016 at. For the inverted pendulum, if\n",
      "we reached some kind of optimal, this is true: our actions are small and we\n",
      "don't deviate much from the vertical.\n",
      "We are going to use Taylor expansion to linearize the dynamics. In the\n",
      "simple case where the state is one-dimensional and the transition function F\n",
      "does not depend on the action, we would write something like\n",
      "st+1=F(st)\u0019F(\u0016st) +F0(\u0016st)\u0001(st\u0000\u0016st)\n",
      "In the more general setting, the formula looks the same, with gradients\n",
      "instead of simple derivatives\n",
      "st+1\u0019F(\u0016st;\u0016at) +rsF(\u0016st;\u0016at)\u0001(st\u0000\u0016st) +raF(\u0016st;\u0016at)\u0001(at\u0000\u0016at) (16.3)\n",
      "and now,st+1is linear instandat, because we can rewrite equation (16.3)\n",
      "as\n",
      "st+1\u0019Ast+Bst+\u0014\n",
      "where\u0014is some constant and A;B are matrices. Now, this writing looks\n",
      "awfully similar to the assumptions made for LQR. We just have to get rid\n",
      "of the constant term \u0014! It turns out that the constant term can be absorbed\n",
      "intostby arti\f",
      "cially increasing the dimension by one. This is the same trick\n",
      "that we used at the beginning of the class for linear regression...\n",
      "16.3.2 Di\u000b",
      "erential Dynamic Programming (DDP)\n",
      "The previous method works well for cases where the goal is to stay around\n",
      "some state s\u0003(think about the inverted pendulum, or a car having to stay\n",
      "in the middle of a lane). However, in some cases, the goal can be more\n",
      "complicated.\n",
      "We'll cover a method that applies when our system has to follow some\n",
      "trajectory (think about a rocket). This method is going to discretize the\n",
      "trajectory into discrete time steps, and create intermediary goals around\n",
      "which we will be able to use the previous technique! This method is called\n",
      "Di\u000b",
      "erential Dynamic Programming . The main steps are\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "212\n",
      "step 1 come up with a nominal trajectory using a naive controller, that approx-\n",
      "imate the trajectory we want to follow. In other words, our controller\n",
      "is able to approximate the gold trajectory with\n",
      "s\u0003\n",
      "0;a\u0003\n",
      "0!s\u0003\n",
      "1;a\u0003\n",
      "1!:::\n",
      "step 2 linearize the dynamics around each trajectory point s\u0003\n",
      "t, in other words\n",
      "st+1\u0019F(s\u0003\n",
      "t;a\u0003\n",
      "t) +rsF(s\u0003\n",
      "t;a\u0003\n",
      "t)(st\u0000s\u0003\n",
      "t) +raF(s\u0003\n",
      "t;a\u0003\n",
      "t)(at\u0000a\u0003\n",
      "t)\n",
      "wherest;atwould be our current state and action. Now that we have\n",
      "a linear approximation around each of these points, we can use the\n",
      "previous section and rewrite\n",
      "st+1=At\u0001st+Bt\u0001at\n",
      "(notice that in that case, we use the non-stationary dynamics setting\n",
      "that we mentioned at the beginning of these lecture notes)\n",
      "Note We can apply a similar derivation for the reward R(t), with a\n",
      "second-order Taylor expansion.\n",
      "R(st;at)\u0019R(s\u0003\n",
      "t;a\u0003\n",
      "t) +rsR(s\u0003\n",
      "t;a\u0003\n",
      "t)(st\u0000s\u0003\n",
      "t) +raR(s\u0003\n",
      "t;a\u0003\n",
      "t)(at\u0000a\u0003\n",
      "t)\n",
      "+1\n",
      "2(st\u0000s\u0003\n",
      "t)>Hss(st\u0000s\u0003\n",
      "t) + (st\u0000s\u0003\n",
      "t)>Hsa(at\u0000a\u0003\n",
      "t)\n",
      "+1\n",
      "2(at\u0000a\u0003\n",
      "t)>Haa(at\u0000a\u0003\n",
      "t)\n",
      "whereHxyrefers to the entry of the Hessian of Rwith respect to xand\n",
      "yevaluated in ( s\u0003\n",
      "t;a\u0003\n",
      "t) (omitted for readability). This expression can be\n",
      "re-written as\n",
      "Rt(st;at) =\u0000s>\n",
      "tUtst\u0000a>\n",
      "tWtat\n",
      "for some matrices Ut;Wt, with the same trick of adding an extra dimen-\n",
      "sion of ones. To convince yourself, notice that\n",
      "\u0000\n",
      "1x\u0001\n",
      "\u0001\u0012a b\n",
      "b c\u0013\n",
      "\u0001\u00121\n",
      "x\u0013\n",
      "=a+ 2bx+cx2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "213\n",
      "step 3 Now, you can convince yourself that our problem is strictly re-written\n",
      "in the LQR framework. Let's just use LQR to \f",
      "nd the optimal policy\n",
      "\u0019t. As a result, our new controller will (hopefully) be better!\n",
      "Note: Some problems might arise if the LQR trajectory deviates too\n",
      "much from the linearized approximation of the trajectory, but that can\n",
      "be \f",
      "xed with reward-shaping...\n",
      "step 4 Now that we get a new controller (our new policy \u0019t), we use it to\n",
      "produce a new trajectory\n",
      "s\u0003\n",
      "0;\u00190(s\u0003\n",
      "0)!s\u0003\n",
      "1;\u00191(s\u0003\n",
      "1)!:::!s\u0003\n",
      "T\n",
      "note that when we generate this new trajectory, we use the real Fand\n",
      "not its linear approximation to compute transitions, meaning that\n",
      "s\u0003\n",
      "t+1=F(s\u0003\n",
      "t;a\u0003\n",
      "t)\n",
      "then, go back to step 2 and repeat until some stopping criterion.\n",
      "16.4 Linear Quadratic Gaussian (LQG)\n",
      "Often, in the real word, we don't get to observe the full state st. For example,\n",
      "an autonomous car could receive an image from a camera, which is merely\n",
      "anobservation , and not the full state of the world. So far, we assumed\n",
      "that the state was available. As this might not hold true for most of the\n",
      "real-world problems, we need a new tool to model this situation: Partially\n",
      "Observable MDPs .\n",
      "A POMDP is an MDP with an extra observation layer. In other words,\n",
      "we introduce a new variable ot, that follows some conditional distribution\n",
      "given the current state st\n",
      "otjst\u0018O(ojs)\n",
      "Formally, a \f",
      "nite-horizon POMDP is given by a tuple\n",
      "(S;O;A;Psa;T;R )\n",
      "Within this framework, the general strategy is to maintain a belief state\n",
      "(distribution over states) based on the observation o1;:::;ot. Then, a policy\n",
      "in a POMDP maps this belief states to actions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "214\n",
      "In this section, we'll present a extension of LQR to this new setting.\n",
      "Assume that we observe yt2Rnwithm<n such that\n",
      "(\n",
      "yt =C\u0001st+vt\n",
      "st+1=A\u0001st+B\u0001at+wt\n",
      "whereC2Rn\u0002dis a compression matrix and vtis the sensor noise (also\n",
      "gaussian, like wt). Note that the reward function R(t)is left unchanged, as a\n",
      "function of the state (not the observation) and action. Also, as distributions\n",
      "are gaussian, the belief state is also going to be gaussian. In this new frame-\n",
      "work, let's give an overview of the strategy we are going to adopt to \f",
      "nd the\n",
      "optimal policy:\n",
      "step 1 \f",
      "rst, compute the distribution on the possible states (the belief state),\n",
      "based on the observations we have. In other words, we want to compute\n",
      "the meanstjtand the covariance \u0006 tjtof\n",
      "stjy1;:::;yt\u0018N\u0000\n",
      "stjt;\u0006tjt\u0001\n",
      "to perform the computation e\u000eciently over time, we'll use the Kalman\n",
      "Filter algorithm (used on-board Apollo Lunar Module!).\n",
      "step 2 now that we have the distribution, we'll use the mean stjtas the best\n",
      "approximation for st\n",
      "step 3 then set the action at:=LtstjtwhereLtcomes from the regular LQR\n",
      "algorithm.\n",
      "Intuitively, to understand why this works, notice that stjtis a noisy ap-\n",
      "proximation of st(equivalent to adding more noise to LQR) but we proved\n",
      "that LQR is independent of the noise!\n",
      "Step 1 needs to be explicated. We'll cover a simple case where there is\n",
      "no action dependence in our dynamics (but the general case follows the same\n",
      "idea). Suppose that\n",
      "(\n",
      "st+1=A\u0001st+wt; wt\u0018N(0;\u0006s)\n",
      "yt =C\u0001st+vt; vt\u0018N(0;\u0006y)\n",
      "As noises are Gaussians, we can easily prove that the joint distribution is\n",
      "also Gaussian\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "215\n",
      "0\n",
      "BBBBBBB@s1\n",
      "...\n",
      "st\n",
      "y1\n",
      "...\n",
      "yt1\n",
      "CCCCCCCA\u0018N (\u0016;\u0006) for some \u0016;\u0006\n",
      "then, using the marginal formulas of gaussians (see Factor Analysis notes),\n",
      "we would get\n",
      "stjy1;:::;yt\u0018N\u0000\n",
      "stjt;\u0006tjt\u0001\n",
      "However, computing the marginal distribution parameters using these\n",
      "formulas would be computationally expensive! It would require manipulating\n",
      "matrices of shape t\u0002t. Recall that inverting a matrix can be done in O(t3),\n",
      "and it would then have to be repeated over the time steps, yielding a cost in\n",
      "O(t4)!\n",
      "TheKalman \f",
      "lter algorithm provides a much better way of computing\n",
      "the mean and variance, by updating them over time in constant time in\n",
      "t! The kalman \f",
      "lter is based on two basics steps. Assume that we know the\n",
      "distribution of stjy1;:::;yt:\n",
      "predict step computest+1jy1;:::;yt\n",
      "update step computest+1jy1;:::;yt+1\n",
      "and iterate over time steps! The combination of the predict and update\n",
      "steps updates our belief states. In other words, the process looks like\n",
      "(stjy1;:::;yt)predict\u0000\u0000\u0000\u0000! (st+1jy1;:::;yt)update\u0000\u0000\u0000\u0000! (st+1jy1;:::;yt+1)predict\u0000\u0000\u0000\u0000!:::\n",
      "predict step Suppose that we know the distribution of\n",
      "stjy1;:::;yt\u0018N\u0000\n",
      "stjt;\u0006tjt\u0001\n",
      "then, the distribution over the next state is also a gaussian distribution\n",
      "st+1jy1;:::;yt\u0018N\u0000\n",
      "st+1jt;\u0006t+1jt\u0001\n",
      "where\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "216\n",
      "(\n",
      "st+1jt=A\u0001stjt\n",
      "\u0006t+1jt=A\u0001\u0006tjt\u0001A>+ \u0006s\n",
      "update step givenst+1jtand \u0006t+1jtsuch that\n",
      "st+1jy1;:::;yt\u0018N\u0000\n",
      "st+1jt;\u0006t+1jt\u0001\n",
      "we can prove that\n",
      "st+1jy1;:::;yt+1\u0018N\u0000\n",
      "st+1jt+1;\u0006t+1jt+1\u0001\n",
      "where\n",
      "(\n",
      "st+1jt+1 =st+1jt+Kt(yt+1\u0000Cst+1jt)\n",
      "\u0006t+1jt+1= \u0006t+1jt\u0000Kt\u0001C\u0001\u0006t+1jt\n",
      "with\n",
      "Kt:= \u0006t+1jtC>(C\u0006t+1jtC>+ \u0006y)\u00001\n",
      "The matrix Ktis called the Kalman gain .\n",
      "Now, if we have a closer look at the formulas, we notice that we don't\n",
      "need the observations prior to time step t! The update steps only depends\n",
      "on the previous distribution. Putting it all together, the algorithm \f",
      "rst runs\n",
      "a forward pass to compute the Kt, \u0006tjtandstjt(sometimes referred to as\n",
      "^sin the literature). Then, it runs a backward pass (the LQR updates) to\n",
      "compute the quantities \t t;\ttandLt. Finally, we recover the optimal policy\n",
      "witha\u0003\n",
      "t=Ltstjt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chapter 17\n",
      "Policy Gradient\n",
      "(REINFORCE)\n",
      "We will present a model-free algorithm called REINFORCE that does not\n",
      "require the notion of value functions and Qfunctions. It turns out to be more\n",
      "convenient to introduce REINFORCE in the \f",
      "nite horizon case, which will\n",
      "be assumed throughout this note: we use \u001c",
      "= (s0;a0;:::;sT\u00001;aT\u00001;sT) to\n",
      "denote a trajectory, where T <1is the length of the trajectory. Moreover,\n",
      "REINFORCE only applies to learning a randomized policy . We use\u0019\u0012(ajs)\n",
      "to denote the probability of the policy \u0019\u0012outputting the action aat states.\n",
      "The other notations will be the same as in previous lecture notes.\n",
      "The advantage of applying REINFORCE is that we only need to assume\n",
      "that we can sample from the transition probabilities fPsagand can query the\n",
      "reward function R(s;a) at statesand actiona,1but we do not need to know\n",
      "the analytical form of the transition probabilities or the reward function.\n",
      "We do not explicitly learn the transition probabilities or the reward function\n",
      "either.\n",
      "Lets0be sampled from some distribution \u0016. We consider optimizing the\n",
      "expected total payo\u000b",
      " of the policy \u0019\u0012over the parameter \u0012de\f",
      "ned as.\n",
      "\u0011(\u0012),E\"T\u00001X\n",
      "tR(st;at)#\n",
      "(17.1)\n",
      "Recall that st\u0018Pst\u00001at\u00001andat\u0018\u0019\u0012(\u0001jst). Also note that \u0011(\u0012) =\n",
      "Es0\u0018P[V\u0019\u0012(s0)] if we ignore the di\u000b",
      "erence between \f",
      "nite and in\f",
      "nite hori-\n",
      "zon.\n",
      "1In this notes we will work with the general setting where the reward depends on both\n",
      "the state and the action.\n",
      "217\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "218\n",
      "We aim to use gradient ascent to maximize \u0011(\u0012). The main challenge\n",
      "we face here is to compute (or estimate) the gradient of \u0011(\u0012) without the\n",
      "knowledge of the form of the reward function and the transition probabilities.\n",
      "LetP\u0012(\u001c",
      ") denote the distribution of \u001c",
      "(generated by the policy \u0019\u0012), and\n",
      "letf(\u001c",
      ") =PT\u00001\n",
      "tR(st;at). We can rewrite \u0011(\u0012) as\n",
      "\u0011(\u0012) = E\u001c",
      "\u0018P\u0012[f(\u001c",
      ")] (17.2)\n",
      "We face a similar situations in the variational auto-encoder (VAE) setting\n",
      "covered in the previous lectures, where the we need to take the gradient w.r.t\n",
      "to a variable that shows up under the expectation | the distribution P\u0012\n",
      "depends on \u0012. Recall that in VAE, we used the re-parametrization techniques\n",
      "to address this problem. However it does not apply here because we do\n",
      "know not how to compute the gradient of the function f. (We only have\n",
      "an e\u000ecient way to evaluate the function fby taking a weighted sum of the\n",
      "observed rewards, but we do not necessarily know the reward function itself\n",
      "to compute the gradient.)\n",
      "The REINFORCE algorithm uses an another approach to estimate the\n",
      "gradient of \u0011(\u0012). We start with the following derivation:\n",
      "r\u0012E\u001c",
      "\u0018P\u0012[f(\u001c",
      ")] =r\u0012Z\n",
      "P\u0012(\u001c",
      ")f(\u001c",
      ")d\u001c",
      "\n",
      "=Z\n",
      "r\u0012(P\u0012(\u001c",
      ")f(\u001c",
      "))d\u001c",
      " (swap integration with gradient)\n",
      "=Z\n",
      "(r\u0012P\u0012(\u001c",
      "))f(\u001c",
      ")d\u001c",
      " (becauefdoes not depend on \u0012)\n",
      "=Z\n",
      "P\u0012(\u001c",
      ")(r\u0012logP\u0012(\u001c",
      "))f(\u001c",
      ")d\u001c",
      "\n",
      "(becauserlogP\u0012(\u001c",
      ") =rP\u0012(\u001c",
      ")\n",
      "P\u0012(\u001c",
      "))\n",
      "= E\u001c",
      "\u0018P\u0012[(r\u0012logP\u0012(\u001c",
      "))f(\u001c",
      ")] (17.3)\n",
      "Now we have a sample-based estimator for r\u0012E\u001c",
      "\u0018P\u0012[f(\u001c",
      ")]. Let\u001c",
      "(1);:::;\u001c",
      "(n)\n",
      "benempirical samples from P\u0012(which are obtained by running the policy\n",
      "\u0019\u0012forntimes, with Tsteps for each run). We can estimate the gradient of\n",
      "\u0011(\u0012) by\n",
      "r\u0012E\u001c",
      "\u0018P\u0012[f(\u001c",
      ")] = E\u001c",
      "\u0018P\u0012[(r\u0012logP\u0012(\u001c",
      "))f(\u001c",
      ")] (17.4)\n",
      "\u00191\n",
      "nnX\n",
      "i=1(r\u0012logP\u0012(\u001c",
      "(i)))f(\u001c",
      "(i)) (17.5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "219\n",
      "The next question is how to compute log P\u0012(\u001c",
      "). We derive an analyt-\n",
      "ical formula for log P\u0012(\u001c",
      ") and compute its gradient w.r.t \u0012(using auto-\n",
      "di\u000b",
      "erentiation). Using the de\f",
      "nition of \u001c",
      ", we have\n",
      "P\u0012(\u001c",
      ") =\u0016(s0)\u0019\u0012(a0js0)Ps0a0(s1)\u0019\u0012(a1js1)Ps1a1(s2)\u0001\u0001\u0001PsT\u00001aT\u00001(sT) (17.6)\n",
      "Here recall that \u0016to used to denote the density of the distribution of s0. It\n",
      "follows that\n",
      "logP\u0012(\u001c",
      ") = log\u0016(s0) + log\u0019\u0012(a0js0) + logPs0a0(s1) + log\u0019\u0012(a1js1)\n",
      "+ logPs1a1(s2) +\u0001\u0001\u0001+ logPsT\u00001aT\u00001(sT) (17.7)\n",
      "Taking gradient w.r.t to \u0012, we obtain\n",
      "r\u0012logP\u0012(\u001c",
      ") =r\u0012log\u0019\u0012(a0js0) +r\u0012log\u0019\u0012(a1js1) +\u0001\u0001\u0001+r\u0012log\u0019\u0012(aT\u00001jsT\u00001)\n",
      "Note that many of the terms disappear because they don't depend on \u0012and\n",
      "thus have zero gradients. (This is somewhat important | we don't know how\n",
      "to evaluate those terms such as log Ps0a0(s1) because we don't have access to\n",
      "the transition probabilities, but luckily those terms have zero gradients!)\n",
      "Plugging the equation above into equation (17.4), we conclude that\n",
      "r\u0012\u0011(\u0012) =r\u0012E\u001c",
      "\u0018P\u0012[f(\u001c",
      ")] = E\u001c",
      "\u0018P\u0012\" T\u00001X\n",
      "t=0r\u0012log\u0019\u0012(atjst)!\n",
      "\u0001f(\u001c",
      ")#\n",
      "= E\u001c",
      "\u0018P\u0012\" T\u00001X\n",
      "t=0r\u0012log\u0019\u0012(atjst)!\n",
      "\u0001 T\u00001X\n",
      "tR(st;at)!#\n",
      "(17.8)\n",
      "We estimate the RHS of the equation above by empirical sample trajectories,\n",
      "and the estimate is unbiased. The vanilla REINFORCE algorithm iteratively\n",
      "updates the parameter by gradient ascent using the estimated gradients.\n",
      "Interpretation of the policy gradient formula (17.8) .The quantity\n",
      "r\u0012P\u0012(\u001c",
      ") =PT\u00001\n",
      "t=0r\u0012log\u0019\u0012(atjst) is intuitively the direction of the change\n",
      "of\u0012that will make the trajectory \u001c",
      "more likely to occur (or increase the\n",
      "probability of choosing action a0;:::;at\u00001), andf(\u001c",
      ") is the total payo\u000b",
      " of\n",
      "this trajectory. Thus, by taking a gradient step, intuitively we are trying to\n",
      "improve the likelihood of all the trajectories, but with a di\u000b",
      "erent emphasis\n",
      "or weight for each \u001c",
      "(or for each set of actions a0;a1;:::;at\u00001). If\u001c",
      "is very\n",
      "rewarding (that is, f(\u001c",
      ") is large), we try very hard to move in the direction\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "220\n",
      "that can increase the probability of the trajectory \u001c",
      "(or the direction that\n",
      "increases the probability of choosing a0;:::;at\u00001), and if\u001c",
      "has low payo\u000b",
      ",\n",
      "we try less hard with a smaller weight.\n",
      "An interesting fact that follows from formula (17.3) is that\n",
      "E\u001c",
      "\u0018P\u0012\"T\u00001X\n",
      "t=0r\u0012log\u0019\u0012(atjst)#\n",
      "= 0 (17.9)\n",
      "To see this, we take f(\u001c",
      ") = 1 (that is, the reward is always a constant),\n",
      "then the LHS of (17.8) is zero because the payo\u000b",
      " is always a \f",
      "xed constantPT\n",
      "t. Thus the RHS of (17.8) is also zero, which implies (17.9).\n",
      "In fact, one can verify that E at\u0018\u0019\u0012(\u0001jst)r\u0012log\u0019\u0012(atjst) = 0 for any \f",
      "xed t\n",
      "andst.2This fact has two consequences. First, we can simplify formula (17.8)\n",
      "to\n",
      "r\u0012\u0011(\u0012) =T\u00001X\n",
      "t=0E\u001c",
      "\u0018P\u0012\"\n",
      "r\u0012log\u0019\u0012(atjst)\u0001 T\u00001X\n",
      "jR(sj;aj)!#\n",
      "=T\u00001X\n",
      "t=0E\u001c",
      "\u0018P\u0012\"\n",
      "r\u0012log\u0019\u0012(atjst)\u0001 T\u00001X\n",
      "jR(sj;aj)!#\n",
      "(17.10)\n",
      "where the second equality follows from\n",
      "E\u001c",
      "\u0018P\u0012\"\n",
      "r\u0012log\u0019\u0012(atjst)\u0001 X\n",
      "jR(sj;aj)!#\n",
      "= E\"\n",
      "E [r\u0012log\u0019\u0012(atjst)js0;a0;:::;st\u00001;at\u00001;st]\u0001 X\n",
      "jR(sj;aj)!#\n",
      "= 0 (because E [ r\u0012log\u0019\u0012(atjst)js0;a0;:::;st\u00001;at\u00001;st] = 0)\n",
      "Note that here we used the law of total expectation. The outer expecta-\n",
      "tion in the second line above is over the randomness of s0;a0;:::;at\u00001;st,\n",
      "whereas the inner expectation is over the randomness of at(conditioned on\n",
      "s0;a0;:::;at\u00001;st.) We see that we've made the estimator slightly simpler.\n",
      "The second consequence of E at\u0018\u0019\u0012(\u0001jst)r\u0012log\u0019\u0012(atjst) = 0 is the following: for\n",
      "any valueB(st) that only depends on st, it holds that\n",
      "E\u001c",
      "\u0018P\u0012[r\u0012log\u0019\u0012(atjst)\u0001B(st)]\n",
      "= E [E [r\u0012log\u0019\u0012(atjst)js0;a0;:::;st\u00001;at\u00001;st]B(st)]\n",
      "= 0 (because E [ r\u0012log\u0019\u0012(atjst)js0;a0;:::;st\u00001;at\u00001;st] = 0)\n",
      "2In general, it's true that E x\u0018p\u0012[rlogp\u0012(x)] = 0.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "221\n",
      "Again here we used the law of total expectation. The outer expecta-\n",
      "tion in the second line above is over the randomness of s0;a0;:::;at\u00001;st,\n",
      "whereas the inner expectation is over the randomness of at(conditioned on\n",
      "s0;a0;:::;at\u00001;st.) It follows from equation (17.10) and the equation above\n",
      "that\n",
      "r\u0012\u0011(\u0012) =T\u00001X\n",
      "t=0E\u001c",
      "\u0018P\u0012\"\n",
      "r\u0012log\u0019\u0012(atjst)\u0001 T\u00001X\n",
      "tB(st)!#)\u0000\n",
      "=T\u00001X\n",
      "t=0E\u001c",
      "\u0018P\u0012\"\n",
      "t T\u00001X\u0012(atjst)\u0001\n",
      "j\u0000tR(sj;aj)\u0000B(st)!#\n",
      "(17.11)\n",
      "Therefore, we will get a di\u000b",
      "erent estimator for estimating the r\u0011(\u0012) with a\n",
      "di\u000b",
      "erence choice of B(\u0001). The bene\f",
      "t of introducing a proper B(\u0001) | which\n",
      "is often referred to as a baseline | is that it helps reduce the variance of the\n",
      "estimator.3It turns out that a near optimal estimator would be the expected\n",
      "future payo\u000b",
      " EhPT\u00001\n",
      "j\u0000tR(sj;aj)jsti\n",
      ", which is pretty much the same as the\n",
      "value function V\u0019\u0012(st) (if we ignore the di\u000b",
      "erence between \f",
      "nite and in\f",
      "nite\n",
      "horizon.) Here one could estimate the value function V\u0019\u0012(\u0001) in a crude way,\n",
      "uence the mean of the estimator but only\n",
      "the variance. This leads to a policy gradient algorithm with baselines stated\n",
      "in Algorithm 7.4\n",
      "3As a heuristic but illustrating example, suppose for a \f",
      "xed t, the future rewardPT\u00001\n",
      "j\u0000tR(sj;aj) randomly takes two values 1000 + 1 and 1000 \u00002 with equal proba-\n",
      "bility, and the corresponding values for r\u0012log\u0019\u0012(atjst) are vector zand\u0000z. (Note that\n",
      "because E [r\u0012log\u0019\u0012(atjst)] = 0, ifr\u0012log\u0019\u0012(atjst) can only take two values uniformly,\n",
      "then the two values have to two vectors in an opposite direction.) In this case, without\n",
      "subtracting the baseline, the estimators take two values (1000 + 1) zand\u0000(1000\u00002)z,\n",
      "whereas after subtracting a baseline of 1000, the estimator has two values zand 2z. The\n",
      "latter estimator has much lower variance compared to the original estimator.\n",
      "4We note that the estimator of the gradient in the algorithm does not exactly match\n",
      "tin the summand of equation (17.13), then they will\n",
      "exactly match. Removing such discount factors empirically works well because it gives a\n",
      "large update.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "222\n",
      "Algorithm 7 Vanilla policy gradient with baseline\n",
      "fori= 1;\u0001\u0001\u0001do\n",
      "Collect a set of trajectories by executing the current policy. Use R\u0015t\n",
      "as a shorthand forPT\u00001\n",
      "j\u0000tR(sj;aj)\n",
      "Fit the baseline by \f",
      "nding a function Bthat minimizes\n",
      "X\n",
      "\u001c",
      "X\n",
      "t(R\u0015t\u0000B(st))2(17.12)\n",
      "Update the policy parameter \u0012with the gradient estimator\n",
      "X\n",
      "\u001c",
      "X\n",
      "tr\u0012log\u0019\u0012(atjst)\u0001(R\u0015t\u0000B(st)) (17.13)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bibliography\n",
      "Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling\n",
      "modern machine-learning practice and the classical bias{variance trade-\n",
      "o\u000b",
      ".Proceedings of the National Academy of Sciences , 116(32):15849{15854,\n",
      "2019.\n",
      "Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for\n",
      "weak features. SIAM Journal on Mathematics of Data Science , 2(4):1167{\n",
      "1180, 2020.\n",
      "David M Blei, Alp Kucukelbir, and Jon D McAuli\u000b",
      "e. Variational inference:\n",
      "A review for statisticians. Journal of the American Statistical Association ,\n",
      "112(518):859{877, 2017.\n",
      "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran\n",
      "Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\n",
      "Bosselut, Emma Brunskill, et al. On the opportunities and risks of foun-\n",
      "dation models. arXiv preprint arXiv:2108.07258 , 2021.\n",
      "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\n",
      "plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sas-\n",
      "try, Amanda Askell, et al. Language models are few-shot learners. Advances\n",
      "in neural information processing systems , 33:1877{1901, 2020.\n",
      "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geo\u000b",
      "rey Hinton.\n",
      "A simple framework for contrastive learning of visual representations. In\n",
      "International Conference on Machine Learning , pages 1597{1607. PMLR,\n",
      "2020.\n",
      "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:\n",
      "Pre-training of deep bidirectional transformers for language understand-\n",
      "ing. In Proceedings of the 2019 Conference of the North American Chapter\n",
      "of the Association for Computational Linguistics: Human Language Tech-\n",
      "nologies, Volume 1 (Long and Short Papers) , pages 4171{4186, 2019.\n",
      "223\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "224\n",
      "Je\u000b",
      " Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters:\n",
      "Understanding the implicit bias of the noise covariance. arXiv preprint\n",
      "arXiv:2006.08680 , 2020.\n",
      "Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.\n",
      "Surprises in high-dimensional ridgeless least squares interpolation. 2019.\n",
      "Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani.\n",
      "Surprises in high-dimensional ridgeless least squares interpolation. The\n",
      "Annals of Statistics , 50(2):949{986, 2022.\n",
      "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual\n",
      "learning for image recognition. In Proceedings of the IEEE conference on\n",
      "computer vision and pattern recognition , pages 770{778, 2016.\n",
      "Sergey Io\u000b",
      "e and Christian Szegedy. Batch normalization: Accelerating deep\n",
      "network training by reducing internal covariate shift. In Proceedings of the\n",
      "32nd International Conference on Machine Learning, ICML 2015, Lille,\n",
      "France, 6-11 July 2015 , pages 448{456, 2015. URL http://jmlr.org/\n",
      "proceedings/papers/v37/ioffe15.html .\n",
      "Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An\n",
      "introduction to statistical learning, second edition , volume 112. Springer,\n",
      "2021.\n",
      "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic opti-\n",
      "mization. arXiv preprint arXiv:1412.6980 , 2014.\n",
      "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv\n",
      "preprint arXiv:1312.6114 , 2013.\n",
      "Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and\n",
      "Tengyu Ma. Algorithmic framework for model-based deep reinforcement\n",
      "learning with theoretical guarantees. In International Conference on Learn-\n",
      "ing Representations , 2018.\n",
      "Song Mei and Andrea Montanari. The generalization error of random features\n",
      "regression: Precise asymptotics and the double descent curve. Communi-\n",
      "cations on Pure and Applied Mathematics , 75(4):667{766, 2022.\n",
      "Preetum Nakkiran. More data can hurt for linear regression: Sample-wise\n",
      "double descent. 2019.\n",
      "Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal\n",
      "regularization can mitigate double descent. 2020.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "225\n",
      "Manfred Opper. Statistical mechanics of learning: Generalization. The hand-\n",
      "book of brain theory and neural networks , pages 922{925, 1995.\n",
      "Manfred Opper. Learning to generalize. Frontiers of Life , 3(part 2):763{775,\n",
      "2001.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n",
      "Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\n",
      "need. arXiv preprint arXiv:1706.03762 , 2017.\n",
      "Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pe-\n",
      "dro Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro. Kernel and\n",
      "rich regimes in overparametrized models. arXiv preprint arXiv:2002.09277 ,\n",
      "2020.\n",
      "Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the\n",
      "European conference on computer vision (ECCV) , pages 3{19, 2018.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for page in pdf_text:\n",
    "    print(page)\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
